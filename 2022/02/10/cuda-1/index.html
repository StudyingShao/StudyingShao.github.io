<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CUDA学习随记 假如想在 warp 内通过 shared memory 做 reduction, 并且假设数据在 shared memory 已经 ready，大家觉得代码这样写有问题吗？  1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives2. CUDA Binary Utilities3. CUDA C++ Programmi">
<meta property="og:type" content="article">
<meta property="og:title" content="cuda_1">
<meta property="og:url" content="http://example.com/2022/02/10/cuda-1/index.html">
<meta property="og:site_name" content="邵大宝的学习Blog">
<meta property="og:description" content="CUDA学习随记 假如想在 warp 内通过 shared memory 做 reduction, 并且假设数据在 shared memory 已经 ready，大家觉得代码这样写有问题吗？  1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives2. CUDA Binary Utilities3. CUDA C++ Programmi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/cuda-1/reduce_shfl_down.png">
<meta property="og:image" content="http://example.com/cuda-1/NVIDIA%20GPU%20architecture.jpg">
<meta property="article:published_time" content="2022-02-10T05:55:56.000Z">
<meta property="article:modified_time" content="2022-02-10T17:07:14.891Z">
<meta property="article:author" content="Jiang Shao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/cuda-1/reduce_shfl_down.png">

<link rel="canonical" href="http://example.com/2022/02/10/cuda-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>cuda_1 | 邵大宝的学习Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">邵大宝的学习Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">最爱严小跳</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cuda_1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 13:55:56" itemprop="dateCreated datePublished" datetime="2022-02-10T13:55:56+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-11 01:07:14" itemprop="dateModified" datetime="2022-02-11T01:07:14+08:00">2022-02-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="CUDA学习随记"><a href="#CUDA学习随记" class="headerlink" title="CUDA学习随记"></a>CUDA学习随记</h1><blockquote>
<p>假如想在 warp 内通过 shared memory 做 reduction, 并且假设数据在 shared memory 已经 ready，大家觉得代码这样写有问题吗？</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">3. CUDA C++ Programming Guide</a></p>
<p>primitives: 原函数、基元功能</p>
<p><strong>SIMD: Single Instruction, Multiple Data</strong><br>In a SIMD architecture, each instruction applies the same operation in parallel across many data elements.<br>每条指令对众多数据单元并行执行相同的操作。<br>SIMD is typically implemented using processors with vector registers and execution units; a scalar thread issues vector instructions that execute in SIMD fashion.<br>通过具有矢量寄存器和执行单元的处理器实现。一个标量线程发射矢量SIMD指令作用于数据矢量。</p>
<p><strong>SIMT: Single Instruction, Multiple Thread</strong><br>In a SIMT architecture, rather than <font color="red">a single thread issuing vector instructions applied to data vectors</font>, <font color="red">multiple threads issue common instructions to arbitrary data</font>.<br>多个线程发射相同的指令作用于任意数据。矢量线程发射标量指令作用于多个标量数据。</p>
<p>NVIDIA GPUs execute warps of 32 parallel threads using SIMT, which enables each thread to access its own registers, to load and store from divergent addresses, and to follow divergent control flow paths. The CUDA compiler and the GPU work together to ensure the threads of a warp execute the same instruction sequences together as frequently as possible to maximize performance.<br>利用SIMT架构执行以32个并行线程为单位的warps，每个线程访问自己的寄存器，从不同地址存取数据，执行分歧控制流路径。</p>
<p>While the high performance obtained by warp execution happens behind the scene, many CUDA programs can achieve even higher performance by using explicit warp-level programming.<br>由warp执行所带来的高性能往往发生在幕后，而CUDA程序还可以通过显式warp级别编程来达到更好的性能。</p>
<center><img src="/cuda-1/reduce_shfl_down.png" width="100%" height="100%"><font color="#708090" size="2">Part of a warp-level parallel reduction using shfl_down_sync().</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> FULL_MASK 0xffffffff</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">    val += __shfl_down_sync(FULL_MASK, val, offset);</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://xh.5156edu.com/page/z1015m9220j18754.html">颜色表及html代码</a><br>一个使用warp内向下shuffle函数实现reduction操作的示例。</p>
<p>A warp comprises 32 lanes, with each thread occupying one lane. For a thread at lane X in the warp, __shfl_down_sync(FULL_MASK, val, offset) gets the value of the val variable from the thread at lane X+offset of the same warp. The data exchange is performed between registers, and more efficient than going through shared memory, which requires a load, a store and an extra register to hold the address.<br>单个warp中的thread被称为一个lane。warp shuffle函数直接读取同一warp内其他lane所占有的register，相比于使用shared mem速度显而易见的更快。读取shared mem需要一个load操作，一个store操作，还需要一个额外的寄存器装地址（访存需要计算地址偏移）。</p>
<p>CUDA 9 introduced three categories of new or updated warp-level primitives.<br>CUDA 9引入三类新的/升级的warp-level基元函数</p>
<ol>
<li>Synchronized data exchange: exchange data between threads in warp.<br>同步数据交换，原有warp shuffle的升级。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__all_sync, __any_sync, __uni_sync, __ballot_sync</span><br><span class="line">__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</span><br><span class="line">__match_any_sync, __match_all_sync</span><br></pre></td></tr></table></figure></li>
<li>Active mask query: returns a 32-bit mask indicating which threads in a warp are active with the current executing thread.<br>活动掩码查询，返回32 bit的掩码，对应32个lane，表明在当前执行中warp中的哪些线程是active的。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask</span><br></pre></td></tr></table></figure></li>
<li>Thread synchronization: synchronize threads in a warp and provide a memory fence.<br>线程同步指令，同步warp内的所有线程，并提供一个memory fence（这点与block内的线程同步指令<strong>syncthreads()有所区别，</strong>syncthreads()并不提供memory fence，线程间的访存结果并不彼此可见，仅保证了线程内部对自身访存结果的前后一致性）。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp</span><br></pre></td></tr></table></figure>
详细内容参见：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></li>
</ol>
<h2 id="Synchronized-Data-Exchange"><a href="#Synchronized-Data-Exchange" class="headerlink" title="Synchronized Data Exchange"></a>Synchronized Data Exchange</h2><p>Each of the “synchronized data exchange” primitives perform a collective operation among a set of threads in a warp. The set of threads that participates in invoking each primitive is specified using a 32-bit mask, which is the first argument of these primitives. 由基元函数中第一个参数mask，来指定warp中哪些线程参与调用基元函数。<br>All the participating threads must be synchronized for the collective operation to work correctly. Therefore, these primitives first synchronize the threads if they are not already synchronized. <font color="red">为了保证协同操作的正确性，所有参与的线程必须同步。</font>意思就是，在读取其他lane的寄存器之前，必须保证对应得lane已经执行到了这里，准备好了数据，才能保证正确性，否则可能读取到其他线程修改过/未修改的值，造成错误。比如reduction操作的__shfl_down_sync，32个lane中的前16个先读取后16个的寄存器，必须操作完成之后，前8个才能再读取lane ID为8-15的lane的寄存器，以此类推，每次读取之前需要一个warp级别的同步，来保证对应的lane中寄存器的数据已经准备好。因此这些基元函数首先会隐式同步线程。</p>
<blockquote>
<p>what should I use for the mask argument?</p>
</blockquote>
<p>mask指定了哪些线程需要参与到协同操作中，这部份线程通常由程序逻辑决定，在程序中早先的一些条件分支中计算得到。以reduction为例，如果vector的单元数比block中的threads数量少，对应的代码如下。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>ballot_sync()用于得到参与</strong>shfl_down_sync()的线程mask，而__ballot_sync()本身使用FULL_MASK (0xffffffff for 32 threads)，因为我们假定所有线程都要执行这条指令。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> val, <span class="type">int</span> src_line, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __shfl_down_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> var, <span class="type">unsigned</span> detla, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>Each thread that calls __ballot_sync() receives a bit mask representing all the threads in the warp that pass a true value for the predicate argument.</p>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others.<br>Volta以及之后的架构支持Independent Thread Scheduling，允许同一warp内的线程执行分歧分支，不再严格保证warp内的同步。</p>
<center><img src="/cuda-1/NVIDIA%20GPU%20architecture.jpg" width="100%" height="100%"><font color="#708090" size="2">NVIDIA GPU architecture history, Volta之后最新的架构为Turing</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Listing 4 shows an example where all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.<br><strong>shfl_sync()用于获取指定lane ID（代码中为lane 0）中寄存器中的数据。示例中，warp内所有线程获取lane 0中的val变量值，而奇数线程与偶数线程分别执行一条if语句的不同分支。可以看到mask参数这里给定FULL_MASK，这是由于不管是基数线程还是偶数线程，都会执行到一条</strong>shfl_sync(FULL_MASK, val, 0)指令，因此对这32个warp lane同步并不会导致死锁。</p>
<h2 id="Active-Mask-Query"><a href="#Active-Mask-Query" class="headerlink" title="Active Mask Query"></a>Active Mask Query</h2><p><strong>active_mask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same </strong>active_mask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.<br><strong>active_mask()就是返回当前warp中所有active线程的对应mask。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Incorrect use of __active_mask()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    <span class="type">unsigned</span> mask = __active_mask(); </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如上代码是一种错误用法。相比于使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br></pre></td></tr></table></figure><br>来计算mask，代码中在if分支中使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __active_mask(); </span><br></pre></td></tr></table></figure><br>来计算mask。错误的原因在于，从Volta架构开始，不再保证同一warp内所有线程的完全同步执行，而当某一个线程调用</strong>active_mask()，其返回的mask是当前时刻warp中所有active threads所对应的mask，而不是所有将会执行到这里的threads所对应的mask。<br>The CUDA execution model does not guarantee that all threads taking the branch together will execute the __activemask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>
<h2 id="Warp-Synchronization"><a href="#Warp-Synchronization" class="headerlink" title="Warp Synchronization"></a>Warp Synchronization</h2><p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the <strong>syncwarp() primitive to synchronize threads in a warp. It is similar to the </strong>syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>用于同步warp中的线程，同步哪些由mask指定。</p>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence** to allow threads to communicate via memory before and after calling the primitive.</p>
<p>只要mask指定的线程执行到了具有相同mask的<strong>syncwarp()即可继续执行，并非是一定位于同一个控制流分支的</strong>syncwarp()。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/02/09/NVIDIA-intern-paper-md/" rel="prev" title="NVIDIA_intern_paper.md">
      <i class="fa fa-chevron-left"></i> NVIDIA_intern_paper.md
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/02/10/cuda-2/" rel="next" title="cuda-2">
      cuda-2 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">CUDA学习随记</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Synchronized-Data-Exchange"><span class="nav-number">1.1.</span> <span class="nav-text">Synchronized Data Exchange</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Active-Mask-Query"><span class="nav-number">1.2.</span> <span class="nav-text">Active Mask Query</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Warp-Synchronization"><span class="nav-number">1.3.</span> <span class="nav-text">Warp Synchronization</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiang Shao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Shao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
