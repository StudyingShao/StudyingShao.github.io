<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CUDA学习随记: Using CUDA Warp-Level Primitives1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives2. CUDA Binary Utilities3. CUDA C++ Programming Guide primitives: 原函数、基元功能 SIMD: Single Instruction,">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA学习随记 Using CUDA Warp-Level Primitives">
<meta property="og:url" content="http://example.com/2022/02/10/cuda-1/index.html">
<meta property="og:site_name" content="邵大宝的学习Blog">
<meta property="og:description" content="CUDA学习随记: Using CUDA Warp-Level Primitives1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives2. CUDA Binary Utilities3. CUDA C++ Programming Guide primitives: 原函数、基元功能 SIMD: Single Instruction,">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2022/02/10/cuda-1/reduce_shfl_down.png">
<meta property="og:image" content="http://example.com/2022/02/10/cuda-1/NVIDIA%20GPU%20architecture.jpg">
<meta property="og:image" content="http://example.com/2022/02/10/cuda-1/Volta_independent_thread_scheduling.png">
<meta property="article:published_time" content="2022-02-10T05:55:56.000Z">
<meta property="article:modified_time" content="2022-02-17T17:26:52.149Z">
<meta property="article:author" content="Jiang Shao">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/02/10/cuda-1/reduce_shfl_down.png">

<link rel="canonical" href="http://example.com/2022/02/10/cuda-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>CUDA学习随记 Using CUDA Warp-Level Primitives | 邵大宝的学习Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">邵大宝的学习Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">最爱严小跳</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CUDA学习随记 Using CUDA Warp-Level Primitives
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 13:55:56" itemprop="dateCreated datePublished" datetime="2022-02-10T13:55:56+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-18 01:26:52" itemprop="dateModified" datetime="2022-02-18T01:26:52+08:00">2022-02-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="CUDA学习随记-Using-CUDA-Warp-Level-Primitives"><a href="#CUDA学习随记-Using-CUDA-Warp-Level-Primitives" class="headerlink" title="CUDA学习随记: Using CUDA Warp-Level Primitives"></a>CUDA学习随记: Using CUDA Warp-Level Primitives</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">3. CUDA C++ Programming Guide</a></p>
<p>primitives: 原函数、基元功能</p>
<p><strong>SIMD: Single Instruction, Multiple Data</strong><br>In a SIMD architecture, each instruction applies the same operation in parallel across many data elements.<br>每条指令对众多数据单元并行执行相同的操作。<br>SIMD is typically implemented using processors with vector registers and execution units; a scalar thread issues vector instructions that execute in SIMD fashion.<br>通过具有矢量寄存器和执行单元的处理器实现。一个标量线程发射矢量SIMD指令作用于数据矢量。</p>
<p><strong>SIMT: Single Instruction, Multiple Thread</strong><br>In a SIMT architecture, rather than <font color="red">a single thread issuing vector instructions applied to data vectors</font>, <font color="red">multiple threads issue common instructions to arbitrary data</font>.<br>多个线程发射相同的指令作用于任意数据。矢量线程发射标量指令作用于多个标量数据。</p>
<p>NVIDIA GPUs execute warps of 32 parallel threads using SIMT, which enables each thread to access its own registers, to load and store from divergent addresses, and to follow divergent control flow paths. The CUDA compiler and the GPU work together to ensure the threads of a warp execute the same instruction sequences together as frequently as possible to maximize performance.<br>利用SIMT架构执行以32个并行线程为单位的warps，每个线程访问自己的寄存器，从不同地址存取数据，执行分歧控制流路径。</p>
<p>While the high performance obtained by warp execution happens behind the scene, many CUDA programs can achieve even higher performance by using explicit warp-level programming.<br>由warp执行所带来的高性能往往发生在幕后，而CUDA程序还可以通过显式warp级别编程来达到更好的性能。</p>
<center><img src="/2022/02/10/cuda-1/reduce_shfl_down.png" width="100%" height="100%"><font color="#708090" size="2">Part of a warp-level parallel reduction using shfl_down_sync().</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> FULL_MASK 0xffffffff</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">    val += __shfl_down_sync(FULL_MASK, val, offset);</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://xh.5156edu.com/page/z1015m9220j18754.html">颜色表及html代码</a><br>一个使用warp内向下shuffle函数实现reduction操作的示例。</p>
<p>A warp comprises 32 lanes, with each thread occupying one lane. For a thread at lane X in the warp, __shfl_down_sync(FULL_MASK, val, offset) gets the value of the val variable from the thread at lane X+offset of the same warp. The data exchange is performed between registers, and more efficient than going through shared memory, which requires a load, a store and an extra register to hold the address.<br>单个warp中的thread被称为一个lane。warp shuffle函数直接读取同一warp内其他lane所占有的register，相比于使用shared mem速度显而易见的更快。读取shared mem需要一个load操作，一个store操作，还需要一个额外的寄存器装地址（访存需要计算地址偏移）。</p>
<p>CUDA 9 introduced three categories of new or updated warp-level primitives.<br>CUDA 9引入三类新的/升级的warp-level基元函数</p>
<ol>
<li>Synchronized data exchange: exchange data between threads in warp.<br>同步数据交换，原有warp shuffle的升级。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__all_sync, __any_sync, __uni_sync, __ballot_sync</span><br><span class="line">__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</span><br><span class="line">__match_any_sync, __match_all_sync</span><br></pre></td></tr></table></figure></li>
<li>Active mask query: returns a 32-bit mask indicating which threads in a warp are active with the current executing thread.<br>活动掩码查询，返回32 bit的掩码，对应32个lane，表明在当前执行中warp中的哪些线程是active的。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask</span><br></pre></td></tr></table></figure></li>
<li>Thread synchronization: synchronize threads in a warp and provide a memory fence.<br>线程同步指令，同步warp内的所有线程，并提供一个memory fence（不保证线程间的访存结果彼此可见，仅保证了线程内部的访存顺序）。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp</span><br></pre></td></tr></table></figure>
详细内容参见：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></li>
</ol>
<h2 id="Synchronized-Data-Exchange"><a href="#Synchronized-Data-Exchange" class="headerlink" title="Synchronized Data Exchange"></a>Synchronized Data Exchange</h2><p>Each of the “synchronized data exchange” primitives perform a collective operation among a set of threads in a warp. The set of threads that participates in invoking each primitive is specified using a 32-bit mask, which is the first argument of these primitives. 由基元函数中第一个参数mask，来指定warp中哪些线程参与调用基元函数。<br>All the participating threads must be synchronized for the collective operation to work correctly. Therefore, these primitives first synchronize the threads if they are not already synchronized. <font color="red">为了保证协同操作的正确性，所有参与的线程必须同步。</font>意思就是，在读取其他lane的寄存器之前，必须保证对应得lane已经执行到了这里，准备好了数据，才能保证正确性，否则可能读取到其他线程修改过/未修改的值，造成错误。比如reduction操作的__shfl_down_sync，32个lane中的前16个先读取后16个的寄存器，必须操作完成之后，前8个才能再读取lane ID为8-15的lane的寄存器，以此类推，每次读取之前需要一个warp级别的同步，来保证对应的lane中寄存器的数据已经准备好。因此这些基元函数首先会隐式同步线程。</p>
<blockquote>
<p>what should I use for the mask argument?</p>
</blockquote>
<p>mask指定了哪些线程需要参与到协同操作中，这部份线程通常由程序逻辑决定，在程序中早先的一些条件分支中计算得到。以reduction为例，如果vector的单元数比block中的threads数量少，对应的代码如下。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>ballot_sync()用于得到参与</strong>shfl_down_sync()的线程mask，而__ballot_sync()本身使用FULL_MASK (0xffffffff for 32 threads)，因为我们假定所有线程都要执行这条指令。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> val, <span class="type">int</span> src_line, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __shfl_down_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> var, <span class="type">unsigned</span> detla, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>Each thread that calls __ballot_sync() receives a bit mask representing all the threads in the warp that pass a true value for the predicate argument.</p>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others.<br>Volta以及之后的架构支持Independent Thread Scheduling，允许同一warp内的线程执行分歧分支，不再严格保证warp内的同步。</p>
<center><img src="/2022/02/10/cuda-1/NVIDIA%20GPU%20architecture.jpg" width="100%" height="100%"><font color="#708090" size="2">NVIDIA GPU architecture history, Volta之后最新的架构为Turing</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Listing 4 shows an example where all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.<br><strong>shfl_sync()用于获取指定lane ID（代码中为lane 0）中寄存器中的数据。示例中，warp内所有线程获取lane 0中的val变量值，而奇数线程与偶数线程分别执行一条if语句的不同分支。可以看到mask参数这里给定FULL_MASK，这是由于不管是基数线程还是偶数线程，都会执行到一条</strong>shfl_sync(FULL_MASK, val, 0)指令，因此对这32个warp lane同步并不会导致死锁。</p>
<h2 id="Active-Mask-Query"><a href="#Active-Mask-Query" class="headerlink" title="Active Mask Query"></a>Active Mask Query</h2><p><strong>activemask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same </strong>activemask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.<br><strong>activemask()就是返回当前warp中所有active线程的对应mask。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Incorrect use of __activemask()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    <span class="type">unsigned</span> mask = __activemask(); </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如上代码是一种错误用法。相比于使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br></pre></td></tr></table></figure><br>来计算mask，代码中在if分支中使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __activemask(); </span><br></pre></td></tr></table></figure><br>来计算mask。错误的原因在于，从Volta架构开始，不再保证同一warp内所有线程的完全同步执行，而当某一个线程调用</strong>activemask()，其返回的mask是当前时刻warp中所有active threads所对应的mask，而不是所有将会执行到这里的threads所对应的mask。<br>The CUDA execution model does not guarantee that all threads taking the branch together will execute the __activemask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>
<h2 id="Warp-Synchronization"><a href="#Warp-Synchronization" class="headerlink" title="Warp Synchronization"></a>Warp Synchronization</h2><p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the <strong>syncwarp() primitive to synchronize threads in a warp. It is similar to the </strong>syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>用于同步warp中的线程，同步哪些由mask指定。</p>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence** to allow threads to communicate via memory before and after calling the primitive.</p>
<p>只要mask指定的线程执行到了具有相同mask的<strong>syncwarp()即可继续执行，并非是一定位于同一个控制流分支的</strong>syncwarp()。还额外提供一个memory fence，保证了当前线程中内存操作的顺序。<br>Memory fence functions only affect the ordering of memory operations by a thread; they do not ensure that these memory operations are visible to other threads (like <strong>syncthreads() does for threads within a block (see Synchronization Functions)).<br>Memory fence仅影响当前线程内内存操作的顺序，并不保证对其他线程的可见性。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure><br>waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to </strong>syncthreads() are visible to all threads in the block.<br>等待block内所有线程到达，并保证同步指令之前的所有访存对整个block内所有线程可见。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> val = <span class="built_in">get_value</span>(…);</span><br><span class="line">__shared__ <span class="type">float</span> smem[<span class="number">4</span>][<span class="number">8</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// matrix with row-major indexing行主元矩阵</span></span><br><span class="line"><span class="comment">//   0  1  2  3  4  5  6  7 </span></span><br><span class="line"><span class="comment">//   8  9 10 11 12 13 14 15 </span></span><br><span class="line"><span class="comment">//  16 17 18 19 20 21 22 23</span></span><br><span class="line"><span class="comment">//  24 25 26 27 28 29 30 31</span></span><br><span class="line"><span class="type">int</span> x1 = threadIdx.x % <span class="number">8</span>;</span><br><span class="line"><span class="type">int</span> y1 = threadIdx.x / <span class="number">8</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// matrix with column-major indexing列主元矩阵</span></span><br><span class="line"><span class="comment">//   0  4  8 12 16 20 24 28</span></span><br><span class="line"><span class="comment">//   1  5 10 13 17 21 25 29</span></span><br><span class="line"><span class="comment">//   2  6 11 14 18 22 26 30</span></span><br><span class="line"><span class="comment">//   3  7 12 15 19 23 27 31</span></span><br><span class="line"><span class="type">int</span> x2= threadIdx.x / <span class="number">4</span>;</span><br><span class="line"><span class="type">int</span> y2 = threadIdx.x % <span class="number">4</span>;</span><br><span class="line"> </span><br><span class="line">smem[y1][x1] = val;</span><br><span class="line">__syncwarp();</span><br><span class="line">val = smem[y2][x2];</span><br><span class="line"> </span><br><span class="line"><span class="built_in">use</span>(val);</span><br></pre></td></tr></table></figure>
<p>如上是一个矩阵转置代码，可以看出作者launch block size选取4 * 8 = 32，恰好是一个warp大小。因此在将各个thread寄存器中内容存入shared mem后，使用<strong>syncwarp()同步，确保shared mem中的数据已经完全准备好，再进行读取，从而起到一个转置的作用。<br>Make sure that </strong>syncwarp() separates shared memory reads and writes to avoid race conditions.<br>通过同步分离了smem的读取和写入，避免了内存竞争。</p>
<blockquote>
<p>A tree sum reduction in shared memory</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Incorrect use of __syncwarp()</span></span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>假如想在 warp 内通过 shared memory 做 reduction, 并且假设数据在 shared memory 已经 ready，大家觉得代码这样写有问题吗？</p>
</blockquote>
<p>首先假设block中只有一个warp，size为32，且shared memory开辟了足够大，不存在越界问题，就是说shmem有48个元素。<br>对于Pascal架构及以下来说，我个人认为没有问题，因为warp中所有线程的指令是完全同步的，所有的smem读会发生在所有smem写之前进行。<br>而对于从Volta架构开始，支持warp内线程独立调度，不再保证这种完全的同步。那么由于并没有添加tid &lt; 16的条件判断，因此对于shmem[16-31]这部分内存来说，每个地址会被一个线程读取，被另一个线程写入，二者发生的顺序没有保证（race condition），因此是不正确的。改正的办法很简单，就是在拆分读写，在中间加一个同步指令，最终的结果与Pascal架构的做法一致。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line"><span class="type">int</span> v = shmem[tid];</span><br><span class="line"></span><br><span class="line">v += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;</span><br></pre></td></tr></table></figure>
<p>The CUDA compiler may elide some of these synchronization instructions in the final generated code depending on the target architecture (e.g. on pre-Volta architectures).<br>编译器会针对目标架构在最终生成的代码中省略这样的一些同步指令，比如在Volta之前的架构上，这些架构warp完全同步执行，没有独立调度功能不需要同步指令。</p>
<p>On the latest Volta (and future) GPUs, you can also use <strong>syncwarp() in thread-divergent branches to synchronize threads from both branches. But once they return from the primitive, the threads will become divergent again. See Listing 13 for such an example.<br>可以在分支中使用</strong>syncwarp()同步warp，一旦函数返回，这些线程重新进入不同分支。</p>
<h2 id="Opportunistic-Warp-level-Programming"><a href="#Opportunistic-Warp-level-Programming" class="headerlink" title="Opportunistic Warp-level Programming"></a>Opportunistic Warp-level Programming</h2><p>在进入分支之前，一般需要先计算mask，然后一直向下传递这个mask。在一些library functions，你不能改变函数的interface，就无法在进行warp-level programming，因为不能向函数中传入所需要的mask。</p>
<p>Some computations can use whatever threads happen to be executing together. We can use a technique called opportunistic warp-level programming, as the following example illustrates. </p>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/">CUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics</a><br>On warp-aggregated atomics for more information on the algorithm<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cooperative-groups/">Cooperative Groups: Flexible CUDA Thread Programming</a><br>For discussion of how Cooperative Groups makes the implementation much simpler.</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__INT.html">CUDA Math API</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   <span class="comment">// increment the value at ptr by 1 and return the old value</span></span><br><span class="line"><span class="number">2</span>   <span class="function">__device__ <span class="type">int</span> <span class="title">atomicAggInc</span><span class="params">(<span class="type">int</span> *ptr)</span> </span>&#123;</span><br><span class="line"><span class="number">3</span>       <span class="type">int</span> mask = __match_any_sync(__activemask(), (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)ptr);</span><br><span class="line"><span class="number">4</span>       <span class="type">int</span> leader = __ffs(mask) – <span class="number">1</span>;    <span class="comment">// select a leader</span></span><br><span class="line"><span class="number">5</span>       <span class="type">int</span> res;</span><br><span class="line"><span class="number">6</span>       <span class="keyword">if</span>(<span class="built_in">lane_id</span>() == leader)                  <span class="comment">// leader does the update</span></span><br><span class="line"><span class="number">7</span>           res = <span class="built_in">atomicAdd</span>(ptr, __popc(mask));</span><br><span class="line"><span class="number">8</span>       res = __shfl_sync(mask, res, leader);    <span class="comment">// get leader’s old value</span></span><br><span class="line"><span class="number">9</span>       <span class="keyword">return</span> res + __popc(mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>)); <span class="comment">//compute old value</span></span><br><span class="line"><span class="number">10</span>  &#125;</span><br></pre></td></tr></table></figure>
<p>atomicAggInc() atomically increments the value pointed to by ptr by 1 and returns the old value. It uses the atomicAdd() function, which may incur contention. To reduce contention, atomicAggInc replaces the per-thread atomicAdd() operation with a per-warp atomicAdd().<br>函数atomicAggInc给输入指针ptr指向的变量+1，然后返回原始值。<br>直接使用atomicAdd()会引发线程间内存竞争导致顺序执行延迟（这里只有一个地址，由ptr指向）。为了减少这种内存竞争，atomicAggInc使用一个warp级别的atomicAdd()替换掉了原有的thread级别的atomicAdd()。</p>
<p>The <strong>activemask() in line 3 finds the set of threads in the warp that are about to perform the atomic operation.<br>返回warp中将要进行原子操作的线程集合。</strong>activemask(): 返回当前warp中active threads的mask。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_any_sync(<span class="type">unsigned</span> mask, T value);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_all_sync(<span class="type">unsigned</span> mask, T value, <span class="type">int</span> *pred);</span><br></pre></td></tr></table></figure>
<p>__match_any_sync()返回输入的掩码mask中指定的threads中，value值相同的线程的线程的掩码。对mask进一步筛选，条件是value值相同。<br>Returns mask of threads that have same value of value in mask.</p>
<p>__match_all_sync()，若输入的掩码mask中指定的所有threads都具有相同的value，则返回掩码、pred指向true，否则返回0、pred指向false。</p>
<p><strong>match_any_sync() returns the bit mask of the threads that have the same value ptr, partitioning the incoming threads into groups whose members have the same ptr value.<br>返回</strong>activemask()返回的mask指定的threads中，ptr值相同的threads所构成的掩码。也就是对此时active的线程根据ptr的值进行进一步分类。<br>Returns mask if all threads in mask have the same value for value; otherwise 0 is returned. Predicate pred is set to true if all threads in mask have the same value of value; otherwise the predicate is set to false.</p>
<p>Each group elects a leader thread (line 4), which performs the atomicAdd() (line 7) for the whole group.<br>分类得到的每组线程选取一个leader（首个mask中bit位为1的线程），并使用这个线程执行atomicAdd()，加的数量为mask中bit为1的数量，正好对应此时经过两次筛选剩下的线程数量。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__​ <span class="type">int</span> __ffs ( <span class="type">int</span>  x )</span><br></pre></td></tr></table></figure><br>Find the position of the least significant bit set to 1 in a 32-bit integer.<br>Bit Operations: 在一个32bit整型中找到置为1的最低有效位。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__​ <span class="type">int</span> __popc ( <span class="type">unsigned</span> <span class="type">int</span>  x )</span><br></pre></td></tr></table></figure><br>Count the number of bits that are set to 1 in a 32-bit integer.<br>Bit Operations: 返回一个32bit整型中为1的bit位数量。</p>
<p>Every thread gets the old value from the leader (line 8) returned by the atomicAdd().<br>warp中其他线程通过__shfl_sync()读取leader线程中的res变量。</p>
<p>Line 9 computes and returns the old value the current thread would get from atomicInc() if it were to call the function instead of atomicAggInc.<br>最终函数返回若线程调用atomicInc()而不是atomicAggInc()而应该得到的返回值。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>())                        <span class="comment">//得到当前线程对应bit置1的mask</span></span><br><span class="line">((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>)                  <span class="comment">//得到当前线程的所有低位置1的mask</span></span><br><span class="line">mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>)           <span class="comment">//保留当前线程所有低位线程所对应的mask</span></span><br><span class="line">__popc(mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>))   <span class="comment">//计数</span></span><br></pre></td></tr></table></figure><br>也就是对当前warp中的线程由低到高做一个单调递增的累加。如果低位一个线程为1，那么高位所有返回值+1。</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/54055195/activemask-vs-ballot-sync"><strong>activemask() vs </strong>ballot_sync()</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__ballot_sync(<span class="type">unsigned</span> mask, predicate):</span><br></pre></td></tr></table></figure>
<p>Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if and only if <strong>predicate evaluates to non-zero for the Nth thread of the warp and the Nth thread is active</strong>.<br>是一个shuffle operation，隐含一个对mask指定的线程的同步作用，没指定的不约束，行为未定义（可能执行了shuffle也可能没执行shuffle）。返回一个mask，如果输入mask中指定的线程对应predicate为true，那么对应bit置1，否则为0。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask():</span><br></pre></td></tr></table></figure>
<p>Returns a 32-bit integer mask of <strong>all currently active threads in the calling warp</strong>. The Nth bit is set if <strong>the Nth lane in the warp is active when __activemask() is called</strong>. Inactive threads are represented by 0 bits in the returned mask. Threads which have exited the program are always marked as inactive. Note that threads that are convergent at an __activemask() call are not guaranteed to be convergent at subsequent instructions unless those instructions are synchronizing warp-builtin functions.<br>仅是一个查询功能，没有任何操作，不对线程进行任何限制，仅返回当前warp中active（convergent）的线程对应的mask。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-shfl-sync">PTX ISA: 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync</a></p>
<p>As an additional clarification on the usage of the mask parameter, note the usage statements in the PTX guide. In particular, the mask parameter is not intended to be an exclusion method. If you wish threads to be excluded from the shuffle operation, you must use conditional code to do that. This is important in light of the following statement from the PTX guide:<br>mask仅指出哪些线程必须要进行同步、参加shuffle operation，并不会将没指定的线程排除在外。如果需要将某些线程排除在外，需要认为使用conditional code，制造divergence。</p>
<blockquote>
<p>The behavior of shfl.sync is undefined if the executing thread is not in the membermask.</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> mask = __match_any_sync(__activemask(), (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)ptr);</span><br></pre></td></tr></table></figure>
<p>回看这条指令，<strong>activemask()指出当前同步的线程有哪些，以这些线程的mask为参数传给</strong>match_any_sync()，此函数进一步指出这些线程中，ptr指向的值相等的有哪些，并以这些线程为mask继续向下执行。</p>
<p>this statement is saying “tell me which threads are converged” (i.e. the <strong>activemask() request), and then “use (at least) those threads to perform the </strong>match_all operation. This is perfectly legal and will use whatever threads happen to be converged at that point. As that listing 9 example continues, the mask computed in the above step is used in the only other warp-cooperative primitive:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = __shfl_sync(mask, res, leader); </span><br></pre></td></tr></table></figure><br><strong>shfl_sync()函数又带了一个对mask中指定线程的同步，就是说mask所指定的线程在</strong>match_any_sync()与__shfl_sync()之间的操作可以基本认为是同步的。</p>
<h2 id="Implicit-Warp-Synchronous-Programming-is-Unsafe"><a href="#Implicit-Warp-Synchronous-Programming-is-Unsafe" class="headerlink" title="Implicit Warp-Synchronous Programming is Unsafe"></a>Implicit Warp-Synchronous Programming is Unsafe</h2><p>CUDA toolkits prior to version 9.0 provided a (now legacy) version of warp-level primitives. Compared with the CUDA 9 primitives, the legacy primitives do not accept a mask argument.<br>早于CUDA 9.0的版本的warp-level primitives版本没有mask参数（不需要warp同步）。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __any(<span class="type">int</span> predicate)</span><br><span class="line"><span class="type">int</span> __any_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate)</span><br></pre></td></tr></table></figure><br>The mask argument, as explained previously, specifies the set of threads in a warp that must participate in the primitives. The new primitives perform intra-warp thread-level synchronization if the threads specified by the mask are not already synchronized during execution.<br>CUDA 9.0引入的新版本提供了warp内级别的线程同步，由mask指定的线程必须同步，其余线程不做要求。</p>
<p>The legacy warp-level primitives do not allow programmers to specify the required threads and do not perform synchronization. Therefore, the threads that must participate in the warp-level operation are not explicitly expressed by the CUDA program. The correctness of such a program depends on implicit warp-synchronous behavior, which may change from one hardware architecture to another, from one CUDA toolkit release to another (due to changes in <strong>compiler optimizations</strong>, for example), or even from one run-time execution to another. Such implicit warp-synchronous programming is unsafe and may not work correctly.<br>原先的warp-level基元函数在使用时不要求指定必须参与的线程，也不进行线程同步。因此，CUDA程序没有显式指定哪些线程必须参与warp-level操作，这种程序的正确性有赖于隐式warp同步。这种程序随着架构、CUDA版本、运行时的不同可能具有不同行为。正确性无法得到保证。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming all 32 threads in a warp execute line 1 together.</span></span><br><span class="line"><span class="built_in">assert</span>(__ballot(<span class="number">1</span>) == FULL_MASK);</span><br><span class="line"><span class="type">int</span> result;</span><br><span class="line"><span class="keyword">if</span> (thread_id % <span class="number">2</span>) &#123;</span><br><span class="line">    result = <span class="built_in">foo</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    result = <span class="built_in">bar</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">unsigned</span> ballot_result = __ballot(result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">assert</span><span class="params">( <span class="type">int</span> expression )</span></span>;</span><br></pre></td></tr></table></figure>
<p>Evaluates an expression and, when the result is false, prints a diagnostic message and aborts the program.<br>若表达式为false，输出诊断消息并终止程序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> __ballot_sync(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p><strong>ballot()是</strong>ballot_sync()的早期版本。接受一个int predicate参数。<br>return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp.<br>返回一个整形，若第N个线程predicate为真，则整型对应bit置为1，否则为0。<br><strong>ballot(1) == FULL_MASK，毫无作用，输入参数为1，每个线程都为1。<br>foo, bar这里理解为随意两个函数，张三李四之类的<br>假设warp中的32个线程同时执行了第一次</strong>ballot()，而随后的if else导致了线程的divergence，而不同分支中函数执行时间未必一致，因此在执行到第二次<strong>ballot()时，warp的re-convergence不能保证。因此ballot_result中可能不会包含warp中所有线程的结果，而仅包含当前线程调用</strong>ballot()时convergent threads的结果。<br>在第二次<strong>ballot()之前使用</strong>syncwarp()同步整个warp也不保证正确性，这也是隐式同步implicit warp-synchronous programming，其隐含一个条件：一旦同步之后，warp中所有线程保持同步直到遇到下一次thread-divergent branch。这个虽然看似正确，但是并没有得到CUDA的官方认可。最安全的办法就是将<strong>ballot()替换为</strong>ballot_sync()，并把所有线程都标记在mask参数中。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp();</span><br><span class="line"><span class="type">unsigned</span> ballot_result = __ballot(result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> ballot_result = __ballot_sync(FULL_MASK, result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   v = <span class="built_in">foo</span>();</span><br><span class="line"><span class="number">2</span>   <span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line"><span class="number">3</span>       __syncwarp();</span><br><span class="line"><span class="number">4</span>       v = __shfl(<span class="number">0</span>);       <span class="comment">// L3 will get undefined result because lane 0 </span></span><br><span class="line"><span class="number">5</span>       __syncwarp();        <span class="comment">// is not active when L3 is executed. L3 and L6</span></span><br><span class="line"><span class="number">6</span>   &#125; <span class="keyword">else</span> &#123;                 <span class="comment">// will execute divergently.</span></span><br><span class="line"><span class="number">7</span>       __syncwarp();</span><br><span class="line"><span class="number">8</span>       v = __shfl(<span class="number">0</span>);</span><br><span class="line"><span class="number">9</span>       __syncwarp();</span><br><span class="line"><span class="number">10</span>  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp(); </span><br><span class="line">v = __shfl(<span class="number">0</span>); </span><br><span class="line">__syncwarp(); </span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shfl_sync(FULL_MASK, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>二者并不等同。</p>
<p><strong>Reason 1:</strong><br>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence<strong> to allow threads to communicate via memory before and after calling the primitive.<br>线程首先会同步(convergent)在line 3 &amp; 7。接下来由于if else分支的关系，threads in a warp become divergent，因此line 4执行的时候，lane 0由于不在此分支上，因此有可能是inactive的（被屏蔽了），对lane 0使用<strong>shfl()结果undefined。改用</strong>shfl_sync(FULL_MASK, 0)就会解决这个问题，此函数会强制mask中指定的所有threads converge，然后执行对应shuffle operation。
</strong>Reason 2:<em>*<br>__syncwarp()仅能保证在执行此函数时，warp中所有threads converge，并不能保证在执行完这条代码之后，threads还能保持convergent，虽然大多数情况是convergent的，但没有得到官方认可is not guaranteed。属于implicit warp-synchronous programming。Thread convergence is guaranteed only within explicitly synchronous warp-level primitives like </em>_sync functions. 隐式同步is unsafe program，因此CUDA 9.0之后舍弃了原有的shuffle函数。</p>
<h2 id="Update-Legacy-Warp-Level-Programming"><a href="#Update-Legacy-Warp-Level-Programming" class="headerlink" title="Update Legacy Warp-Level Programming"></a>Update Legacy Warp-Level Programming</h2><p>将原来的程序更新为使用新shuffle函数（the sync version of the primitives）的程序。<br>Any form of implicit warp-synchronous programming, such as communicating between threads of a warp without synchronization.<br>也可以选择use Cooperative Groups restructure your code。其提供了一个更高级别的编组抽象，可以任意组织线程并同步组内线程，such as multi-block synchronization。</p>
<p>使用warp-level primitives最困难的部份就是确定函数所使用的mask（参与shuffle operations的threads）。Here are some suggestions:</p>
<blockquote>
<ol>
<li><p>Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.<br>不要简单的使用FULL_MASK当mask参数传入primitives，如果warp中有threads执行不到会造成死锁hang。</p>
</li>
<li><p>Don’t just use <strong>activemask() as the mask value. </strong>activemask() tells you what threads happen to be convergent when the function is called, which can be different from what you want to be in the collective operation.<br>也不要简单的使用<strong>activemask()的返回值当作mask，</strong>activemask()仅仅返回当前碰巧convergent的threads，与你希望的threads可能会有出入。</p>
</li>
<li><p>Do analyze the program logic and understand the membership requirements. Compute the mask ahead based on your program logic.<br>分析程序逻辑，理解需求，在程序逻辑分支之前先行计算mask。</p>
</li>
<li><p>If your program does opportunistic warp-synchronous programming, use “detective” functions such as <strong>activemask() and </strong>match_any_sync() to find the right mask.<br>如果要在程序中使用opportunistic warp-synchronous programming（不做同步，直接查询当前active threads，再筛选出满足条件的threads，有哪些用哪些），使用一些具有查询检测功能的函数，例如<strong>activemask() and </strong>match_any_sync()，来返回满足要求的mask。</p>
</li>
<li><p>Use <strong>syncwarp() to separate operations with intra-warp dependences. Do not assume lock-step execution.<br>不要假定线程同步implicit synchronous，使用</strong>syncwarp()分离具有intra-warp dependences的操作。warp内前后依赖，指后续操作依赖前面操作的结果。</p>
</li>
</ol>
</blockquote>
<p>If your existing CUDA program gives a different result on Volta architecture GPUs, and you suspect the difference is caused by Volta’s new independent thread scheduling which can change warp synchronous behavior, you may want to recompile your program with <strong>nvcc options -arch=compute_60 -code=sm_70</strong>. Such compiled programs opt-in to Pascal’s thread scheduling.<br>Caused by implicit warp-synchronous programming.</p>
<p>Volta’s new independent thread scheduling:<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/inside-volta/">Inside Volta: The World’s Most Advanced Data Center GPU</a></p>
<p>Volta independent thread scheduling enables interleaved execution of statements from divergent branches. This enables execution of fine-grain parallel algorithms where threads within a warp may synchronize and communicate.</p>
<center><img src="/2022/02/10/cuda-1/Volta_independent_thread_scheduling.png" width="100%" height="100%"><font color="#708090" size="2">Volta_independent_thread_scheduling.</font></center>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/02/09/NVIDIA-intern-paper-md/" rel="prev" title="NVIDIA Intern Paper 1 Smoke Simulation Method">
      <i class="fa fa-chevron-left"></i> NVIDIA Intern Paper 1 Smoke Simulation Method
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/02/10/cuda-2/" rel="next" title="CUDA学习随记 关于Warp Shuffle Functions *_sync 首个参数mask的作用">
      CUDA学习随记 关于Warp Shuffle Functions *_sync 首个参数mask的作用 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0-Using-CUDA-Warp-Level-Primitives"><span class="nav-number">1.</span> <span class="nav-text">CUDA学习随记: Using CUDA Warp-Level Primitives</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Synchronized-Data-Exchange"><span class="nav-number">1.1.</span> <span class="nav-text">Synchronized Data Exchange</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Active-Mask-Query"><span class="nav-number">1.2.</span> <span class="nav-text">Active Mask Query</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Warp-Synchronization"><span class="nav-number">1.3.</span> <span class="nav-text">Warp Synchronization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Opportunistic-Warp-level-Programming"><span class="nav-number">1.4.</span> <span class="nav-text">Opportunistic Warp-level Programming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implicit-Warp-Synchronous-Programming-is-Unsafe"><span class="nav-number">1.5.</span> <span class="nav-text">Implicit Warp-Synchronous Programming is Unsafe</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Update-Legacy-Warp-Level-Programming"><span class="nav-number">1.6.</span> <span class="nav-text">Update Legacy Warp-Level Programming</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiang Shao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Shao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
