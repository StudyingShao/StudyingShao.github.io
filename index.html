<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="邵大宝的学习Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="邵大宝的学习Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jiang Shao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>邵大宝的学习Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">邵大宝的学习Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">最爱严小跳</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/10/cuda-2/" class="post-title-link" itemprop="url">cuda-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 22:22:15" itemprop="dateCreated datePublished" datetime="2022-02-10T22:22:15+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-11 01:06:04" itemprop="dateModified" datetime="2022-02-11T01:06:04+08:00">2022-02-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关于Warp-Shuffle-Functions-sync-首个参数mask的作用"><a href="#关于Warp-Shuffle-Functions-sync-首个参数mask的作用" class="headerlink" title="关于Warp Shuffle Functions *_sync 首个参数mask的作用"></a>关于Warp Shuffle Functions *_sync 首个参数mask的作用</h1><p>首先来看CUDA 9.0之前的Warp Shuffle Functions形式。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl(T var,<span class="type">int</span> srcLane,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_up(T var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_down(T var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_xor(T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize)</span><br></pre></td></tr></table></figure><br>再来看从CUDA 9.0开始，引入的改进的Warp Shuffle Functions形式。<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a><br>Deprecation Notice: <strong>shfl, </strong>shfl_up, <strong>shfl_down, and </strong>shfl_xor have been deprecated as of CUDA 9.0.<br>C++官方手册中说，从CUDA 9.0开始，原有Warp Shuffle Functions被废弃，全部改用以下新形式。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> srcLane, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_up_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_down_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_xor_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize);</span><br></pre></td></tr></table></figure><br>新形式与原有Warp Shuffle Functions功能完全一致。可以看到，与原有Warp Shuffle Functions最大的区别是新形式引入了一个变量mask。mask为一个32 bit变量，对应一个warp中的32个thread (lane)，指明参与调用的线程，也就是参与到Warp Shuffle Functions的线程集合。除此之外，可以看到每个函数后面都额外增加了一个表明同步的后缀_sync。<strong>在Pascal以及之前的架构中，我们知道一个warp中的32个线程是完全同步执行的。</strong> warp scheduler每次分别发射一条相同的指令给一个warp中的32个线程，除非遇到warp divergence情况，不在当前控制流path上的线程会被屏蔽。<br>然而，<strong>从Pascal的下一代架构——Volta架构开始，warp scheduler支持Independent Thread Scheduling，也就是说，同一个warp中的32个threads不再保证完全同步。</strong> 因此，为了保证Warp Shuffle Functions协同操作的正确性，所有参与的线程必须首先进行同步，从而保证结果的正确性。比如reduction操作的__shfl_down_sync，32个lane中的前16个先读取后16个的寄存器，必须等待操作完成之后，前8个才能再读取lane ID为8-15的lane的寄存器，以此类推。每次读取之前需要一个warp级别的同步，来保证对应的lane中寄存器的数据已经准备好。<strong>因此这些带有_sync后缀的函数首先会隐式同步所有参与到函数调用的线程，而哪些线程将参与到函数调用中，由mask的对应bit位来指定，0代表不参与，1代表参与。</strong><br>The new *_sync shfl intrinsics take in a mask indicating the threads participating in the call. A bit, representing the thread’s lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. All non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.<br>那么，未被mask指定的线程呢？这里简单做一个测试。写一个kernel，并以&lt;&lt;<1,32>&gt;&gt;启动。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">16</span>);</span><br></pre></td></tr></table></figure><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0x0000000f</span>, val, <span class="number">16</span>);</span><br></pre></td></tr></table></figure><br>观察执行后每个线程中变量shfl_val的值，可以看到传入两个不同的mask，得到的结果相同，如下所示。</1,32></p>
<font size="1">

|Thread ID|0|1|...|15|16|...|30|31|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**shfl_val**|16|17|...|31|16|...|30|31|
</font>
将参数delta由16改为8，即为每个thread读当前lane ID + 8的thread中的寄存器。
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">8</span>);</span><br></pre></td></tr></table></figure>
<font size="1">

|Thread ID|0|1|...|7|8|...|14|15|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**shfl_val**|8|9|...|15|16|...|22|23|

|Thread ID|16|17|...|23|24|...|30|31|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**shfl_val**|24|25|...|31|24|...|30|31|
</font>

<p>这也就证明了:</p>
<ol>
<li>__shfl_down_sync()，为lane ID小的thread读取大的thread的寄存器。越界的线程，即lane ID + 8 &gt; 31的线程返回自身寄存器中的值。</li>
<li>mask未指定的thread也会调用warp shuffle functions，而不是不执行，但这部分线程不会被函数隐式同步，所得到的结果是undefined。</li>
</ol>
<p>也就是说，对于Pascal及之前的架构来说，带有_sync后缀的Warp Shuffle Functions中的参数mask没有作用。而对于从Volta开始的架构来说，mask将指定参与Warp Shuffle Functions的线程，从而告知编译器，来对这部分线程在执行时进行必要的同步，来保证结果的正确性。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/what-does-mask-mean-in-warp-shuffle-functions-shfl-sync/67697">What does mask mean in warp shuffle functions (__shfl_sync)</a></p>
<p>引用其中一段回答：</p>
<blockquote>
<p>For correctness, you must specify a mask parameter which includes the warp lanes you expect to participate. The behavior of lanes outside the mask is undefined (because, in fact, Volta provides no guarantees of warp convergence, except those the programmer specifically asks for, and therefore a warp lane with a zero bit in the mask implies that the specified lane may or may not participate.)</p>
<p>The mask parameter says the following:</p>
<p>“These are the warp lanes that must participate for correctness.”</p>
<p>The compiler will generate the necessary instructions to reconverge those threads if they are not already converged.</p>
<p>Thereafter the warp shuffle proceeds for the current state of the warp.</p>
<p>There is no other implied behavior. Regardless of the mask, after the reconvergence step, the result of the warp shuffle operation will be the result you would get for whichever threads happen to be participating. A zero bit in the mask argument does not prevent a warp lane from participating, it merely does not guarantee that such a lane will participate if the warp is in a diverged state.</p>
</blockquote>
<p>mask中的0 bit位不会阻止一个warp lane参与到shuffle function的执行中，而仅仅是不保证这样一个lane在warp内具有分歧时会参与到shuffle function的执行中。换句话说，就是不会在执行前同步这个lane，如果它恰好执行到这里，那么就一起执行，如果不在不管它。</p>
<blockquote>
<p>There is an important caveat here. The mask parameter will create a reconvergence of the indicated threads. However it cannot cause reconvergence of threads that your code has made impossible.</p>
<p>For example, this is illegal (will result in undefined behavior for warp 0):</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">if</span> (threadIdx.x &gt; <span class="number">3</span>)</span><br><span class="line">   __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, v, offset, <span class="number">8</span>);</span><br></pre></td></tr></table></figure>
<p>有一个重要的警告。mask参数会在shuffle function中同步指定threads，然而，并不包括代码中规定的并不能同步的线程（条件语句等造成的condition branches，控制流分叉warp divergence）。如上所示代码，对于warp 0来说，mask指定32个warp lane都要执行<strong>shfl_down_sync，因此要对warp 0中的32个线程进行同步。但if语句规定只有后28个线程会执行到这里，而前四个线程不会执行，二者冲突，会导致undefined behavior。<br>正确写法：使用</strong>ballot_sync预先得到所有参与__shfl_down_sync()的线程所对应的mask。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(<span class="number">0xFFFFFFFF</span>, threadIdx.x &gt; <span class="number">3</span>);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &gt; <span class="number">3</span>)</span><br><span class="line">    __shfl_down_sync(mask, v, offset, <span class="number">8</span>);</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>额外这里记录一个我自己看到的好玩儿的问题。<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">developer.nvidia.com/blog/: Using CUDA Warp-Level Primitives</a>中，在Listing 4，作者写了这样一段代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>配文：</p>
<blockquote>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others. Listing 4 shows an example where <strong>all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.</strong><br>On the latest Volta (and future) GPUs, you can run library functions that use warp synchronous primitives without worrying whether the function is called in a thread-divergent branch.</p>
</blockquote>
<p>毫无疑问，这与我上面所说的警告有所冲突，原因在于这里FULL_MASK为0xFFFFFFFF，但对于其中一个if分支，仅有odd/even线程会访问到这里。也就是说会这会导致上文中提到的undefined behavior问题。具体这个undefined behavior是什么，在<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/using-cuda-warp-level-primitives/148673/16">forums.developer.nvidia.com: Using CUDA Warp-Level Primitives</a>中，有人提到：</p>
<blockquote>
<p>In “Update Legacy Warp-Level Programming”, it says “Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.”</p>
<p>In listing 4, regardless the thread id is even or odd, the thread in the warp will always execute one of the two __shfl_sync() statements. Therefore, FULL_MASK should be used.</p>
<p>The following code may cause a stall.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">  val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">  …</span><br><span class="line">&gt;&#125;</span><br><span class="line">&gt;<span class="keyword">else</span> &#123;</span><br><span class="line">  …</span><br><span class="line">&gt;&#125;</span><br></pre></td></tr></table></figure><br>因此我猜这里所说的undefined behavior应该就是死锁，由于FULL_MASK要求同步warp中的32个lane，而控制流导致有的lane永远不会执行到这里，因此造成死锁。这与在if条件语句中使用block内同步函数<strong>syncthreads()所导致的死锁原理相似。<br>Listing 4中的代码合法的原因是，不管是odd还是even线程，都会执行到一条</strong>shfl_sync(FULL_MASK, val, 0)指令，这两条指令的mask都是FULL_MASK，因此对这32个warp lane同步并不会导致死锁。这个道理类似__syncwarp()。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>用于同步warp中的线程，同步哪些由mask指定。</p>
</blockquote>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)** before resuming execution. It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</p>
<p>只要mask指定的线程执行到了具有相同mask的<strong>syncwarp()即可继续执行，并非是一定位于同一个控制流分支的</strong>syncwarp()。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/10/cuda-1/" class="post-title-link" itemprop="url">cuda_1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 13:55:56" itemprop="dateCreated datePublished" datetime="2022-02-10T13:55:56+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-11 01:07:14" itemprop="dateModified" datetime="2022-02-11T01:07:14+08:00">2022-02-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDA学习随记"><a href="#CUDA学习随记" class="headerlink" title="CUDA学习随记"></a>CUDA学习随记</h1><blockquote>
<p>假如想在 warp 内通过 shared memory 做 reduction, 并且假设数据在 shared memory 已经 ready，大家觉得代码这样写有问题吗？</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">3. CUDA C++ Programming Guide</a></p>
<p>primitives: 原函数、基元功能</p>
<p><strong>SIMD: Single Instruction, Multiple Data</strong><br>In a SIMD architecture, each instruction applies the same operation in parallel across many data elements.<br>每条指令对众多数据单元并行执行相同的操作。<br>SIMD is typically implemented using processors with vector registers and execution units; a scalar thread issues vector instructions that execute in SIMD fashion.<br>通过具有矢量寄存器和执行单元的处理器实现。一个标量线程发射矢量SIMD指令作用于数据矢量。</p>
<p><strong>SIMT: Single Instruction, Multiple Thread</strong><br>In a SIMT architecture, rather than <font color="red">a single thread issuing vector instructions applied to data vectors</font>, <font color="red">multiple threads issue common instructions to arbitrary data</font>.<br>多个线程发射相同的指令作用于任意数据。矢量线程发射标量指令作用于多个标量数据。</p>
<p>NVIDIA GPUs execute warps of 32 parallel threads using SIMT, which enables each thread to access its own registers, to load and store from divergent addresses, and to follow divergent control flow paths. The CUDA compiler and the GPU work together to ensure the threads of a warp execute the same instruction sequences together as frequently as possible to maximize performance.<br>利用SIMT架构执行以32个并行线程为单位的warps，每个线程访问自己的寄存器，从不同地址存取数据，执行分歧控制流路径。</p>
<p>While the high performance obtained by warp execution happens behind the scene, many CUDA programs can achieve even higher performance by using explicit warp-level programming.<br>由warp执行所带来的高性能往往发生在幕后，而CUDA程序还可以通过显式warp级别编程来达到更好的性能。</p>
<center><img src="/cuda-1/reduce_shfl_down.png" width="100%" height="100%"><font color="#708090" size="2">Part of a warp-level parallel reduction using shfl_down_sync().</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> FULL_MASK 0xffffffff</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">    val += __shfl_down_sync(FULL_MASK, val, offset);</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://xh.5156edu.com/page/z1015m9220j18754.html">颜色表及html代码</a><br>一个使用warp内向下shuffle函数实现reduction操作的示例。</p>
<p>A warp comprises 32 lanes, with each thread occupying one lane. For a thread at lane X in the warp, __shfl_down_sync(FULL_MASK, val, offset) gets the value of the val variable from the thread at lane X+offset of the same warp. The data exchange is performed between registers, and more efficient than going through shared memory, which requires a load, a store and an extra register to hold the address.<br>单个warp中的thread被称为一个lane。warp shuffle函数直接读取同一warp内其他lane所占有的register，相比于使用shared mem速度显而易见的更快。读取shared mem需要一个load操作，一个store操作，还需要一个额外的寄存器装地址（访存需要计算地址偏移）。</p>
<p>CUDA 9 introduced three categories of new or updated warp-level primitives.<br>CUDA 9引入三类新的/升级的warp-level基元函数</p>
<ol>
<li>Synchronized data exchange: exchange data between threads in warp.<br>同步数据交换，原有warp shuffle的升级。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__all_sync, __any_sync, __uni_sync, __ballot_sync</span><br><span class="line">__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</span><br><span class="line">__match_any_sync, __match_all_sync</span><br></pre></td></tr></table></figure></li>
<li>Active mask query: returns a 32-bit mask indicating which threads in a warp are active with the current executing thread.<br>活动掩码查询，返回32 bit的掩码，对应32个lane，表明在当前执行中warp中的哪些线程是active的。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask</span><br></pre></td></tr></table></figure></li>
<li>Thread synchronization: synchronize threads in a warp and provide a memory fence.<br>线程同步指令，同步warp内的所有线程，并提供一个memory fence（这点与block内的线程同步指令<strong>syncthreads()有所区别，</strong>syncthreads()并不提供memory fence，线程间的访存结果并不彼此可见，仅保证了线程内部对自身访存结果的前后一致性）。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp</span><br></pre></td></tr></table></figure>
详细内容参见：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></li>
</ol>
<h2 id="Synchronized-Data-Exchange"><a href="#Synchronized-Data-Exchange" class="headerlink" title="Synchronized Data Exchange"></a>Synchronized Data Exchange</h2><p>Each of the “synchronized data exchange” primitives perform a collective operation among a set of threads in a warp. The set of threads that participates in invoking each primitive is specified using a 32-bit mask, which is the first argument of these primitives. 由基元函数中第一个参数mask，来指定warp中哪些线程参与调用基元函数。<br>All the participating threads must be synchronized for the collective operation to work correctly. Therefore, these primitives first synchronize the threads if they are not already synchronized. <font color="red">为了保证协同操作的正确性，所有参与的线程必须同步。</font>意思就是，在读取其他lane的寄存器之前，必须保证对应得lane已经执行到了这里，准备好了数据，才能保证正确性，否则可能读取到其他线程修改过/未修改的值，造成错误。比如reduction操作的__shfl_down_sync，32个lane中的前16个先读取后16个的寄存器，必须操作完成之后，前8个才能再读取lane ID为8-15的lane的寄存器，以此类推，每次读取之前需要一个warp级别的同步，来保证对应的lane中寄存器的数据已经准备好。因此这些基元函数首先会隐式同步线程。</p>
<blockquote>
<p>what should I use for the mask argument?</p>
</blockquote>
<p>mask指定了哪些线程需要参与到协同操作中，这部份线程通常由程序逻辑决定，在程序中早先的一些条件分支中计算得到。以reduction为例，如果vector的单元数比block中的threads数量少，对应的代码如下。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>ballot_sync()用于得到参与</strong>shfl_down_sync()的线程mask，而__ballot_sync()本身使用FULL_MASK (0xffffffff for 32 threads)，因为我们假定所有线程都要执行这条指令。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> val, <span class="type">int</span> src_line, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __shfl_down_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> var, <span class="type">unsigned</span> detla, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>Each thread that calls __ballot_sync() receives a bit mask representing all the threads in the warp that pass a true value for the predicate argument.</p>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others.<br>Volta以及之后的架构支持Independent Thread Scheduling，允许同一warp内的线程执行分歧分支，不再严格保证warp内的同步。</p>
<center><img src="/cuda-1/NVIDIA%20GPU%20architecture.jpg" width="100%" height="100%"><font color="#708090" size="2">NVIDIA GPU architecture history, Volta之后最新的架构为Turing</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Listing 4 shows an example where all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.<br><strong>shfl_sync()用于获取指定lane ID（代码中为lane 0）中寄存器中的数据。示例中，warp内所有线程获取lane 0中的val变量值，而奇数线程与偶数线程分别执行一条if语句的不同分支。可以看到mask参数这里给定FULL_MASK，这是由于不管是基数线程还是偶数线程，都会执行到一条</strong>shfl_sync(FULL_MASK, val, 0)指令，因此对这32个warp lane同步并不会导致死锁。</p>
<h2 id="Active-Mask-Query"><a href="#Active-Mask-Query" class="headerlink" title="Active Mask Query"></a>Active Mask Query</h2><p><strong>active_mask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same </strong>active_mask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.<br><strong>active_mask()就是返回当前warp中所有active线程的对应mask。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Incorrect use of __active_mask()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    <span class="type">unsigned</span> mask = __active_mask(); </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如上代码是一种错误用法。相比于使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br></pre></td></tr></table></figure><br>来计算mask，代码中在if分支中使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __active_mask(); </span><br></pre></td></tr></table></figure><br>来计算mask。错误的原因在于，从Volta架构开始，不再保证同一warp内所有线程的完全同步执行，而当某一个线程调用</strong>active_mask()，其返回的mask是当前时刻warp中所有active threads所对应的mask，而不是所有将会执行到这里的threads所对应的mask。<br>The CUDA execution model does not guarantee that all threads taking the branch together will execute the __activemask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>
<h2 id="Warp-Synchronization"><a href="#Warp-Synchronization" class="headerlink" title="Warp Synchronization"></a>Warp Synchronization</h2><p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the <strong>syncwarp() primitive to synchronize threads in a warp. It is similar to the </strong>syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>用于同步warp中的线程，同步哪些由mask指定。</p>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence** to allow threads to communicate via memory before and after calling the primitive.</p>
<p>只要mask指定的线程执行到了具有相同mask的<strong>syncwarp()即可继续执行，并非是一定位于同一个控制流分支的</strong>syncwarp()。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/09/NVIDIA-intern-paper-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/09/NVIDIA-intern-paper-md/" class="post-title-link" itemprop="url">NVIDIA_intern_paper.md</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-09 23:02:28" itemprop="dateCreated datePublished" datetime="2022-02-09T23:02:28+08:00">2022-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-10 02:16:39" itemprop="dateModified" datetime="2022-02-10T02:16:39+08:00">2022-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation"><a href="#Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation" class="headerlink" title="Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation"></a>Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation</h1><h2 id="1-Smoke-Simulation-Method"><a href="#1-Smoke-Simulation-Method" class="headerlink" title="1. Smoke Simulation Method"></a>1. Smoke Simulation Method</h2><h3 id="1-1-Operator-Splitting-Method"><a href="#1-1-Operator-Splitting-Method" class="headerlink" title="1.1. Operator-Splitting Method"></a>1.1. Operator-Splitting Method</h3><p><strong>Reference:</strong> Harris M J. Fast fluid dynamics simulation on the GPU[J]. SIGGRAPH Courses, 2005, 220(10.1145): 1198555-1198790.<br>&ensp;&ensp;The smoke simulation solves an incompressible NS equation expressed as</p>
<script type="math/tex; mode=display">\frac{\partial \mathbf{u} }{\partial t}=-\left( \mathbf{u}\cdot \nabla  \right)\mathbf{u}-\frac{1}{\rho }\nabla p+\nu { {\nabla }^{2} }\mathbf{u}+\mathbf{F} \tag {1.1.1}</script><p>, where <strong>u</strong> is the velocity field and for an incompressible fluid, it satisfies</p>
<script type="math/tex; mode=display">\nabla \cdot \mathbf{u}=0. \tag {1.1.2}</script><p>With the Operator-Splitting Method, the solution of the NS equation is calculated via composition of transformations on the state. In other words, each component of the NS equation is a step that takes a field as input, and produces a new field as output. $\color{red}{[Fast Fluid Dynamics Simulation on the GPU.]}$ Define an operator <strong>S</strong> that is equivalent to the solution of NS equation over a single time step. Then, the operator S can be decomposed into the operators for advection <strong>A</strong>, diffusion <strong>D</strong>, force application <strong>F</strong>, and projection <strong>P</strong>.</p>
<center><img src="/2022/02/09/NVIDIA-intern-paper-md/1.1.1_1.png" width="50%" height="50%"></center>

<p>&ensp;&ensp;After <strong>F</strong>, <strong>D</strong>, and <strong>A</strong> operations, we can get a divergent velocity field <strong>w</strong>. Then we can get the velocity field <strong>u</strong> with zero divergence by adopting the Helmholtz-Hodge Decomposition Theorem.</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{u}+\nabla p \tag {1.1.3}</script><p>This process is called the projection corresponding to <strong>P</strong>. A projection operation is always needed to guarantee the incompressibility of the fluid.<br>&ensp;&ensp;If we apply the divergence operator to both sides of the above equation, we obtain</p>
<script type="math/tex; mode=display">\nabla \cdot \mathbf{u}+{ {\nabla }^{2} }p={ {\nabla }^{2} }p=\nabla \cdot \mathbf{w}. \tag {1.1.4}</script><p>Note that for an incompressible fluid, we have $\nabla \cdot \mathbf{u}=0$. Then, it becomes a Poisson equation for the pressure of the fluid, sometimes called the Poisson-pressure equation. With this equation, we can get the pressure p, and then use <strong>w</strong> and p to compute the new divergence-free velocity field <strong>u</strong> by</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{u}+\nabla p. \tag {1.1.5}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/09/GAMES101/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/09/GAMES101/" class="post-title-link" itemprop="url">hexo_blog.md</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-09 22:03:18" itemprop="dateCreated datePublished" datetime="2022-02-09T22:03:18+08:00">2022-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-10 02:21:24" itemprop="dateModified" datetime="2022-02-10T02:21:24+08:00">2022-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Geometry-part3"><a href="#Geometry-part3" class="headerlink" title="Geometry part3"></a>Geometry part3</h1><ol>
<li>mesh subdivision 网格细化</li>
<li>mesh simplification 网格粗化</li>
<li>mesh regularization 网格正则化，使三角形趋于一致</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/1970/01/01/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/1970/01/01/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 1970-01-01 08:00:00" itemprop="dateCreated datePublished" datetime="1970-01-01T08:00:00+08:00">1970-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-09 19:43:51" itemprop="dateModified" datetime="2022-02-09T19:43:51+08:00">2022-02-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiang Shao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Shao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
