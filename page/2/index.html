<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jiang Shao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>é‚µå¤§å®çš„å­¦ä¹ Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">é‚µå¤§å®çš„å­¦ä¹ Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">æœ€çˆ±ä¸¥å°è·³</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/22/cuda-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/22/cuda-5/" class="post-title-link" itemprop="url">CUDAå­¦ä¹ éšè®°5 Asynchronous copy cp.async</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-22 21:51:12" itemprop="dateCreated datePublished" datetime="2022-02-22T21:51:12+08:00">2022-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-14 23:17:25" itemprop="dateModified" datetime="2022-06-14T23:17:25+08:00">2022-06-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDAå­¦ä¹ éšè®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDAå­¦ä¹ éšè®°5-Asynchronous-copy-cp-async"><a href="#CUDAå­¦ä¹ éšè®°5-Asynchronous-copy-cp-async" class="headerlink" title="CUDAå­¦ä¹ éšè®°5 Asynchronous copy cp.async"></a>CUDAå­¦ä¹ éšè®°5 Asynchronous copy cp.async</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-asynchronous-copy">1. PTX (Parallel Thread Execution) ISA (Instruction Set Architecture)</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities (ç”¨äºæŸ¥è¯¢SASS code)</a></p>
<p>An asynchronous copy operation copies data from one state space to another asynchronously <strong>without blocking the executing thread</strong>.<br>Initiation of an asynchronous copy operation simply dispatches the copy operation from the source memory location to the destination memory location.<br>å¼‚æ­¥æ‹·è´ï¼Œä¸é˜»å¡çº¿ç¨‹ï¼Œä»…å‘èµ·ä¸€ä¸ªå¼‚æ­¥æ‹·è´è¯·æ±‚ã€‚</p>
<p>There are two ways to wait for the completion of an asynchronous copy operation:<br>ä¸¤ç§æ–¹å¼ç­‰å¾…å¼‚æ­¥æ‹·è´æ“ä½œå®Œæˆï¼š</p>
<ol>
<li>ä½¿ç”¨<strong>cp.async-groups</strong><br>a. Initiate asynchronous copy operations.<br>b. Commit copy operations into a cp.async-group. <strong>Using cp.async.commit_group</strong><br>c. Wait for cp.async-group to complete the copy. <strong>Using cp.async.wait_group</strong><br>d. Once the cp.async-group completes, the writes performed by the copy operation in that cp.async-group are made <strong>visible to the thread that initiated the copy operations</strong>.</li>
<li>ä½¿ç”¨<strong>mbarrier</strong>å¯¹è±¡<br>a. Initiate asynchronous copy operations.<br>b. Make an mbarrier object track the asynchronous copy operations.<br>c. Wait for the mbarrier object to complete the phaseé˜¶æ®µ using mbarrier.test_wait.<br>d. Once mbarrier.test_wait returns True, the writes performed by the copy operation are made <strong>visible to all the threads which waited on the mbarrier object</strong>.</li>
</ol>
<p>A sequence of asynchronous copy operations initiated by a thread can be batched into a <strong>per-thread group</strong> referred to as cp.async-group.<br>ç”±å•ä¸ªçº¿ç¨‹å‘å‡ºçš„ä¸€ç³»åˆ—å¼‚æ­¥æ‹·è´æ“ä½œå¯ä»¥æ‰“åŒ…ä¸ºa per-thread groupï¼Œç§°ä¸ºcp.async-groupã€‚</p>
<p>A commit operation creates <strong>a cp.async-group containing all prior asynchronous copy operations initiated by the executing thread</strong> but none of the asynchronous copy operations following the commit operation. A committed asynchronous copy operation belongs to a single cp.async-group.<br>åˆ›å»ºä¸€ä¸ªcp.async-groupå°†åŒ…å«æ‰€æœ‰ç”±æœ¬çº¿ç¨‹å‘å‡ºçš„ä¹‹å‰çš„å¼‚æ­¥æ‹·è´æ“ä½œï¼Œä¸åŒ…æ‹¬ä¹‹åçš„ã€‚æ¯ä¸ªå¼‚æ­¥æ‹·è´æ“ä½œä»…å±äºä¸€ä¸ªcp.async-groupã€‚<br>ç”±äºæ‰§è¡Œåˆ°cp.async-groupæ—¶ï¼Œå…¶ä¹‹å‰çš„æ‰€æœ‰å¼‚æ­¥æ‹·è´æ“ä½œå‡å·²è¢«å¯¹åº”cp.async-groupæ”¶å½•ï¼Œå› æ­¤ä¸‹ä¸€ä¸ªcp.async-groupä»…åŒ…å«ä¸¤æ¡cp.async-groupä¹‹é—´çš„æ–°å‘å‡ºçš„å¼‚æ­¥æ‹·è´æ“ä½œï¼Œæ‰€ä»¥è¯´æ¯ä¸ªå¼‚æ­¥æ‹·è´æ“ä½œä»…å±äºä¸€ä¸ªcp.async-groupã€‚</p>
<p>When a cp.async-group completes, all the asynchronous copy operations belonging in that group are complete and the executing thread that initiated copy operations can read copied results. All cp.async-groups committed by an executing thread always complete in the order in which they were committed. There is no ordering between asynchronous copy operations within a cp.async-group.<br>å½“cp.async-groupå®Œæˆæ—¶ï¼Œå…¶æ‰€åŒ…å«çš„æ‰€æœ‰å¼‚æ­¥æ‹·è´æ“ä½œå‡å·²å®Œæˆï¼Œå‘å‡ºå¼‚æ­¥æ‹·è´è¯·æ±‚çš„çº¿ç¨‹ï¼ˆä¹Ÿå°±æ˜¯å½“å‰çº¿ç¨‹ï¼‰å¯ä»¥å®‰å…¨è¯»å–æ‹·è´ç»“æœã€‚çº¿ç¨‹å‘å°„çš„cp.async-groupsæ°¸è¿œæŒ‰ç…§é¡ºåºå®Œæˆï¼Œè€Œcp.async-groupä¸­åŒ…å«çš„å¼‚æ­¥æ‹·è´æ“ä½œçš„æ‰§è¡Œé¡ºåºæ˜¯éšæœºçš„ã€‚</p>
<p>Writes performed by an asynchronous copy operation are visible to the thread that initiated the asynchronous copy operation only after the cp.async-group completes or mbarrier object tracking the asynchronous copy has completed the phase.<br>ä»…å½“cp.async-groupå®Œæˆ or mbarrierè¿½è¸ªåˆ°å¼‚æ­¥æ‹·è´å®Œæˆï¼Œå½“å‰çº¿ç¨‹æ‰èƒ½å®‰å…¨è¯»å–ç”±å½“å‰çº¿ç¨‹å‘å‡ºçš„å¼‚æ­¥æ‹·è´æ“ä½œå†™å…¥çš„æ•°æ®ï¼ˆæ•°æ®å¯è§ï¼‰ã€‚</p>
<p>Once an asynchronous copy operation is initaited, modifiying the source memory location or reading from the destination memory location before the asynchronous copy operation completes, will cause unpredictable results.<br>ä¸€æ—¦ä¸€ä¸ªå¼‚æ­¥æ‹·è´æ“ä½œè¢«å‘èµ·ï¼Œä¸ç­‰å¾…å¼‚æ­¥æ‹·è´æ“ä½œå®Œæˆå°± ä¿®æ”¹æºå†…å­˜åœ°å€ä¸­çš„æ•°æ® or å°±è¯»å–ç›®æ ‡å†…å­˜åœ°å€ä¸­çš„æ•°æ®ï¼Œå°†ä¼šäº§ç”Ÿunpredictable resultsã€‚</p>
<h4 id="cp-async"><a href="#cp-async" class="headerlink" title="cp.async"></a>cp.async</h4><p>Initiates an asynchronous copy operation from one state space to another.<br>å‘èµ·ä¸€ä¸ªå¼‚æ­¥æ‹·è´æ“ä½œã€‚<br>Operand src specifies a location in the global state space and dst specifies a location in the shared state space.<br>å¼‚æ­¥æ‹·è´åªèƒ½æ˜¯ä»global memæ‹·è´åˆ°shared memã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cp.async.ca.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], cp-size&#123;, src-size&#125;&#123;, cache-policy&#125; ;</span><br><span class="line">cp.async.cg.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], <span class="number">16</span>&#123;, src-size&#125;&#123;, cache-policy&#125; ;</span><br><span class="line">cp.async.ca.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], cp-size&#123;, ignore-src&#125;&#123;, cache-policy&#125; ;</span><br><span class="line">cp.async.cg.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], <span class="number">16</span>&#123;, ignore-src&#125;&#123;, cache-policy&#125; ;</span><br></pre></td></tr></table></figure><br>The <strong>.cg</strong> qualifier indicates caching of data only at global level cache L2 and not at L1 whereas <strong>.ca</strong> qualifier indicates caching of data at all levels including L1 cache. Cache operator are treated as performance hints only.<br><strong>.cg</strong>è¡¨ç¤ºä»…ç”¨L2 cacheï¼Œæ•°æ®ä¼ è¾“ç»•è¿‡L1 cacheä¸registerã€‚å¯¹åº”LDGSTS.BYPASSã€‚<br><strong>.ca</strong>è¡¨ç¤ºåŒæ—¶ä½¿ç”¨L1 cacheä¸L2 cacheï¼Œæ•°æ®ä¼ è¾“ä»…ç»•è¿‡registerã€‚å¯¹åº”LDGSTS.ACCESSã€‚<br>è¿™ä¿©æ ‡è¯†ç¬¦åªæ˜¯å¯¹ç¼–è¯‘å™¨çš„å»ºè®®ï¼Œå…·ä½“ç”¨ä¸ç”¨è¦çœ‹ç¼–è¯‘å™¨çš„æ„æ€ã€‚å®é™…ä¸Šå…·ä½“ä½¿ç”¨å“ªç§å–å†³äºä¼ è¾“æ•°æ®çš„å¤§å°ä¸å¯¹é½æ–¹å¼ã€‚4 Bytes-&gt;accessï¼Œ16 Bytes-&gt;bypassã€‚</p>
<p>Operand cp-size is an integer constant which specifies the size of data in bytes to be copied to the destination dst. cp-size can only be 4, 8 and 16.<br>cp-sizeæŒ‡å®šäº†ä¼ è¾“æ•°æ®çš„å¤§å°ï¼ˆin bytesï¼‰ï¼Œå¯ä»¥æ˜¯4ã€8ã€16ï¼Œå¯ä»¥çœ‹åˆ°cp-sizeæœ€å°æ˜¯4ï¼Œè€Œ.cgæŒ‡ä»¤cp-sizeç›´æ¥æŒ‡å®šä¸º16ï¼Œæ­£å¥½ç¬¦åˆä¸Šé¢çš„è¯´æ³•ã€‚</p>
<center><img src="/2022/02/22/cuda-5/LDGSTS%20access%20and%20bypass.png" width="100%" height="100%"><font color="#708090" size="2">Two types of LDGSTS: access and bypass.</font></center>

<p>Instruction cp.async allows optionally specifying a 32-bit integer operand <strong>src-size</strong>. Operand src-size represents the size of the data in bytes to be copied from src to dst and must be less than <strong>cp-size</strong>. In such case, remaining bytes in destination dst are filled with zeros. Specifying src-size larger than cp-size results in undefined behavior.<br>src-sizeï¼ŒæŒ‡å®šæ‹·è´æºæ•°æ®å¤§å°ï¼Œå¿…é¡»æ¯”cp-sizeå°ï¼Œå¦åˆ™undefined behaviorã€‚å¾ˆç®€ç­”ï¼Œå¦‚æœæ¯”cp-sizeå¤§ï¼Œåˆ™æ‹·è´æ•°æ®ä¸å®Œæ•´ï¼Œå†è¯»å–è‚¯å®šä¸å¯¹ã€‚cp-sizeæ¯”src-sizeå¤šå‡ºæ¥çš„éƒ¨ä»½åœ¨dstä¸­ä¼šè¢«å¡«å……0ã€‚</p>
<p>The optional and non-immediate predicate argument <strong>ignore-src</strong> specifies whether the data from the source location src should be ignored completely. If the source data is ignored then zeros will be copied to destination dst. If the argument ignore-src is not specified then it defaults to False.<br>æŒ‡å®šæ˜¯å¦å¿½ç•¥scrä¸­çš„æ•°æ®ï¼Œå¦‚æœç½®ä¸ºtrueï¼Œåˆ™å‘dstä¸­æ‹·è´0ã€‚é»˜è®¤ä¸ºfalseã€‚</p>
<p>The mandatory(å¼ºåˆ¶çš„) <strong>.async</strong> qualifier indicates that the cp instruction will <strong>initiate the memory copy operation asynchronously</strong> and <strong>control will return to the executing thread before the copy operation is complete</strong>. The executing thread can then use <strong>cp.async.wait_all</strong> or <strong>cp.async.wait_group</strong> or <strong>mbarrier instructions</strong> to wait for completion of the asynchronous copy operation. No other synchronization mechanisms described in Memory Consistency Model can be used to guarantee the completion of the asynchronous copy operations.<br>ä¸‰ç§ç”¨æ¥åŒæ­¥å¼‚æ­¥æ•°æ®ä¼ è¾“æ“ä½œçš„æ–¹æ³•ï¼š<br><strong>cp.async.wait_all</strong> or <strong>cp.async.wait_group</strong> or <strong>mbarrier instructions</strong></p>
<p>There is no ordering guarantee between two cp.async operations if they are not explicitly synchronized using cp.async.wait_all or cp.async.wait_group or mbarrier instructions.<br>å¦‚æœæ²¡æœ‰æ˜¾å¼åŒæ­¥ï¼Œä¸¤ä¸ªcp.asyncä¹‹é—´æ²¡æœ‰é¡ºåºä¿è¯ã€‚</p>
<p>The <strong>.level::prefetch_size</strong> qualifier is a hint to <strong>fetch additional data of the specified size into the respective cache level</strong>.The sub-qualifier prefetch_size can be set to either of 64B, 128B, 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes respectively.</p>
<p>The qualifier .level::prefetch_size may only be used with .global state space and with generic addressing where the address points to .global state space. If the generic address does not fall within the address window of the global memory, then the prefetching behavior is undefined.</p>
<p>The .level::prefetch_size qualifier is treated as a performance hint only.</p>
<p>When the optional argument <strong>cache-policy</strong> is specified, the qualifier <strong>.level::cache_hint</strong> is required. The 64-bit operand cache-policy specifies <strong>the cache eviction policy</strong> that may be used during the memory access.</p>
<p>The qualifier .level::cache_hint is only supported for .global state space and for generic addressing where the address points to the .global state space.</p>
<p>cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as a performance hint only, and does not change the memory consistency behavior of the program.</p>
<p>.level::prefetch_sizeä¸.level::cache_hintéƒ½æ˜¯ä¸ç¼“å­˜ç›¸å…³çš„qualifierã€‚éƒ½æ˜¯hints to the compilerï¼Œå¹¶ä¸ä¸€å®šè¢«ç¼–è¯‘å™¨æ‰€é‡‡çº³ã€‚ç”±äºæ˜¯ç¼“å­˜ç›¸å…³çš„ï¼Œæ‰€ä»¥æ“ä½œå¯¹è±¡åœ°å€å¿…é¡»åœ¨global memoryä¸­ï¼Œå¦åˆ™ä¼šå¯¼è‡´undefined behavior.<br>.level::prefetch_sizeç”¨äºåœ¨ç¼“å­˜ä¸­é™¤äº†è¦æ‹·è´çš„æ•°æ®ä¹‹å¤–ï¼Œé¢å¤–fetchä¸€å®šå¤§å°çš„æ•°æ®åˆ°å¯¹åº”cacheä¸­ï¼Œç”¨äºå¢åŠ ç¼“å­˜å‘½ä¸­ç‡ï¼Œå‡å°‘ä»device memoryä¸­è¯»å–æ•°æ®çš„æ¬¡æ•°ã€‚<br>.level::cache_hintç”¨äºcache-policyæŒ‡å®šæ—¶ï¼Œç”¨äºæŒ‡å®šç¼“å­˜æ¸…é™¤ç­–ç•¥ï¼Œå¦‚ä½•ä½¿ç”¨ç¼“å­˜ã€‚</p>
<h4 id="cp-async-commit-group"><a href="#cp-async-commit-group" class="headerlink" title="cp.async.commit_group"></a>cp.async.commit_group</h4><p>Commits all prior initiated but uncommitted cp.async instructions into a cp.async-group.<br>å°†ä¹‹å‰çš„æœªè¢«commitçš„cp.async commitåˆ°ä¸€ä¸ªcp.async-groupã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp.async.commit_group;</span><br></pre></td></tr></table></figure>
<p>è¿™æ¡PTX codeæ‰€äº§ç”Ÿçš„SASS codeå°±æ˜¯<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LDGDEPBAR <span class="comment">//Global Load Dependency Barrier</span></span><br></pre></td></tr></table></figure><br>ä¼šå¯¼è‡´ä¸€ä¸ªå¾ˆå¤§çš„stall long scoreboardã€‚</p>
<p><strong>cp.async.commit_group</strong> instruction creates a new cp.async-group <strong>per thread</strong> and batches all prior cp.async instructions <strong>initiated by the executing thread but not committed to any cp.async-group</strong> into the new cp.async-group. If there are no uncommitted cp.async instructions then cp.async.commit_group results in an empty cp.async-group.</p>
<p>An executing thread can wait for the completion of all cp.async operations in a cp.async-group using cp.async.wait_group.</p>
<p>There is no memory ordering guarantee provided between any two cp.async operations within the same cp.async-group. So two or more cp.async operations within a cp.async-group copying data to the same location results in undefined behavior.<br>åœ¨åŒä¸€ä¸ªcp.async-groupä¸­çš„cp.asyncæ²¡æœ‰é¡ºåºä¿è¯ï¼Œå› æ­¤åœ¨åŒä¸€cp.async-groupä¸­å¯¹åŒä¸€åœ°å€çš„cp.asyncä¼šå¯¼è‡´undefined behaviorã€‚</p>
<h4 id="cp-async-wait-group-cp-async-wait-all"><a href="#cp-async-wait-group-cp-async-wait-all" class="headerlink" title="cp.async.wait_group/cp.async.wait_all"></a>cp.async.wait_group/cp.async.wait_all</h4><p>Wait for completion of prior asynchronous copy operations.<br>ç­‰å¾…å…ˆå‰çš„å¼‚æ­¥æ‹·è´æ“ä½œå®Œæˆã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp.async.wait_group N;</span><br><span class="line">cp.async.wait_all;</span><br></pre></td></tr></table></figure>
<p><strong>cp.async.wait_group</strong> instruction will cause executing thread to wait till <strong>only N or fewer of the most recent cp.async-groups are pending</strong> and <strong>all the prior cp.async-groups committed by the executing threads are complete</strong>. For example, when N is 0, the executing thread waits on all the prior cp.async-groups to complete. Operand N is an integer constant.</p>
<p>cp.async.wait_group N; ç­‰å¾…execution threadç›´åˆ°è¿˜å‰© N ä¸ªæˆ–æ›´å°‘ä¸ª<strong>æœ€è¿‘çš„</strong>cp.async-groupè¿˜åœ¨è¿›è¡Œä¸­ã€æœªå®Œæˆã€‚ç”±äºcp.async-groupæŒ‰ç…§è¢«commitçš„é¡ºåºä¾æ¬¡å®Œæˆï¼Œå› æ­¤å…ˆå‘å‡ºçš„å¿…å…ˆå®Œæˆï¼Œç­‰å¾…æŒ‡ä»¤ç­‰å¾…åå‘å‡ºçš„cp.async-groupã€‚</p>
<p>cp.async.wait_all; ç­‰å¾…execution threadå…ˆå‰çš„æ‰€æœ‰cp.asyncéƒ½å®Œæˆã€‚ä¹Ÿå°±æ˜¯ç­‰ä»·äºcommitä¸€ä¸ªcp.async-groupç„¶åé©¬ä¸Šç­‰å¾…æ‰€æœ‰å…ˆå‰çš„cp.async-groupå®Œæˆã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp.async.commit_group; <span class="comment">//å°†ä»æœªè¢«commitçš„cp.async commitåˆ°ä¸€ä¸ªcp.async-groupä¸­</span></span><br><span class="line">cp.async.wait_group <span class="number">0</span>; <span class="comment">//ç­‰å¾…æ‰€æœ‰å…ˆå‰çš„cp.async-groupå®Œæˆ</span></span><br></pre></td></tr></table></figure></p>
<p>å¦‚æœNä¸º0ï¼Œé‚£ä¹ˆè¿™æ¡PTX codeæ‰€äº§ç”Ÿçš„SASS codeå°±æ˜¯<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEPBAR.LE SB0, <span class="number">0x0</span> ;<span class="comment">//ç­‰å¾…ç›´åˆ°SB0çš„è®¡æ•°é™è‡³0</span></span><br></pre></td></tr></table></figure><br>DEPBARçš„ä½œç”¨æ˜¯ç­‰å¾…ï¼Œç›´åˆ°æŸä¸€Scoreboardçš„è®¡æ•°å°äºç‰¹å®šå€¼ã€‚ï¼ˆæ¯”å¦‚DEPBAR.LE SB5, 0x6 ;è¡¨ç¤ºstallç›´è‡³ç¬¬5ä¸ªScoreboardçš„å€¼é™åˆ°6æˆ–ä»¥ä¸‹ï¼‰</p>
<p>SBæŒ‡çš„å°±æ˜¯scoreboardã€‚è®°å½•cp.async-groupçš„æ•°é‡ï¼ˆä¸€ä¸ªcounterï¼‰ä¸æ¯ä¸ªcp.async-groupæ‰€å¯¹åº”çš„cp.async (LDGSTS)çš„æ•°é‡ã€‚æ¯æ¬¡ä¸€ä¸ªcp.async-groupå®Œæˆï¼Œcounter - 1ã€‚counterçš„æ•°é‡åˆšå¥½å¯¹åº”Nçš„å€¼ã€‚<br>N æŒ‡æ˜çº¿ç¨‹ç­‰å¾…ç›´åˆ°åªæœ‰ N æˆ–æ›´å°‘ä¸ªcp.async-groupæœªå®Œæˆæ‰ç»§ç»­ã€‚å¦‚æœæ˜¯0ï¼Œé‚£ä¹ˆå°±æ˜¯ç­‰å¾…åªæœ‰0ä¸ªæœªå®Œæˆï¼Œä»£è¡¨ç­‰å¾…æ‰€æœ‰çš„cp.async-groupå®Œæˆã€‚ç”±äºcp.async-groupæ˜¯ä¿æŒå‘å°„é¡ºåºçš„ï¼Œå› æ­¤Nç­‰å¾…çš„æ°¸è¿œæ˜¯å€’æ•°çš„Nä¸ªcp.async-groupï¼Œå°±æ˜¯æœ€è¿‘å‘å°„çš„ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example of .wait_all:</span></span><br><span class="line">cp.async.ca.shared.global [shrd1], [gbl1], <span class="number">4</span>;</span><br><span class="line">cp.async.cg.shared.global [shrd2], [gbl2], <span class="number">16</span>;</span><br><span class="line">cp.async.wait_all;  <span class="comment">// waits for all prior cp.async to complete</span></span><br></pre></td></tr></table></figure>
<p>cp.async.wait_allç­‰å¾…å‰é¢æ‰€æœ‰çš„cp.asyncå®Œæˆã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example of .wait_group :</span></span><br><span class="line">cp.async.ca.shared.global [shrd3], [gbl3], <span class="number">8</span>;</span><br><span class="line">cp.async.commit_group;  <span class="comment">// End of group 1</span></span><br><span class="line"></span><br><span class="line">cp.async.cg.shared.global [shrd4], [gbl4], <span class="number">16</span>;</span><br><span class="line">cp.async.commit_group;  <span class="comment">// End of group 2</span></span><br><span class="line"></span><br><span class="line">cp.async.cg.shared.global [shrd5], [gbl5], <span class="number">16</span>;</span><br><span class="line">cp.async.commit_group;  <span class="comment">// End of group 3</span></span><br><span class="line"></span><br><span class="line">cp.async.wait_group <span class="number">1</span>;  <span class="comment">// waits for group 1 and group 2 to complete</span></span><br></pre></td></tr></table></figure>
<p>cp.async.wait_group 1 å…è®¸æœ‰ä¸€ä¸ªcp.async-groupæœªå®Œæˆï¼ŒæŒ‰ç…§é¡ºåºï¼Œæ­¤æ¡æŒ‡ä»¤ç­‰å¾…group 1 and 2å®Œæˆã€‚</p>
<p>Writes performed by cp.async operations are made <strong>visible to the executing thread</strong> only after :</p>
<ol>
<li>The completion of cp.async.wait_all or</li>
<li>The completion of cp.async.wait_group on the cp.async-group in which the cp.async belongs to or</li>
<li>mbarrier.test_wait returns True on an mbarrier object which is tracking the completion of the cp.async operation.<br>ä¸‰ç§æƒ…å†µä¸‹ï¼Œç”±cp.asyncå†™å…¥çš„æ•°æ®å¯¹å½“å‰çº¿ç¨‹å¯è§ã€‚cp.async.wait_allå®Œæˆäº†ï¼Œä»£è¡¨æ­¤å¤„ä¹‹å‰æ‰€æœ‰çš„cp.asyncéƒ½å·²å®Œæˆã€‚cp.asyncæ‰€åœ¨çš„cp.async-groupå®Œæˆäº†ï¼Œä½¿ç”¨cp.async.wait_groupå®ç°ã€‚è¿½è¸ªæ­¤cp.asyncçš„mbarrierçš„mbarrier.test_waitè¿”å›trueã€‚</li>
</ol>
<p>There is no ordering between two cp.async operations that are not synchronized with cp.async.wait_all or cp.async.wait_group or mbarrier objects.</p>
<p><strong>cp.async.wait_group and cp.async.wait_all does not provide any ordering and visibility guarantees for any other memory operation apart from cp.async.</strong><br>cp.async.wait_group and cp.async.wait_allä»…å½±å“cp.asyncçš„é¡ºåºï¼Œå…¶ä»–å†…å­˜æ“ä½œä¸æ­¤æ— å…³ã€‚å¾ˆå®¹æ˜“çœ‹å‡ºå˜›ã€‚å‰ç¼€éƒ½æ˜¯cp.async.*ï¼Œæ˜æ˜¾å°±æ˜¯è¿™ä¸ªä¸“ç”¨çš„ã€‚è¿™é‡Œæ²¡è¯´mbarrierï¼Œè¯´æ˜è¿™ç©åº”æ˜¯å¯¹æ‰€æœ‰å†…å­˜æ“ä½œéƒ½æœ‰æ•ˆçš„ï¼Œæ˜¯åŒæ­¥éšœç¢ã€‚</p>
<h3 id="ä¸Julien-Demouth-x3c-x6a-100-x65-109-x6f-x75-x74-x68-64-110-x76-105-x64-105-x61-46-x63-x6f-109-gt-çš„é‚®ä»¶å¾€æ¥"><a href="#ä¸Julien-Demouth-x3c-x6a-100-x65-109-x6f-x75-x74-x68-64-110-x76-105-x64-105-x61-46-x63-x6f-109-gt-çš„é‚®ä»¶å¾€æ¥" class="headerlink" title="ä¸Julien Demouth &#x3c;&#x6a;&#100;&#x65;&#109;&#x6f;&#x75;&#x74;&#x68;&#64;&#110;&#x76;&#105;&#x64;&#105;&#x61;&#46;&#x63;&#x6f;&#109;&gt;çš„é‚®ä»¶å¾€æ¥"></a>ä¸Julien Demouth <a href="&#x6d;&#97;&#x69;&#108;&#x74;&#111;&#58;&#x3c;&#x6a;&#100;&#x65;&#109;&#x6f;&#x75;&#x74;&#x68;&#64;&#110;&#x76;&#105;&#x64;&#105;&#x61;&#46;&#x63;&#x6f;&#109;">&#x3c;&#x6a;&#100;&#x65;&#109;&#x6f;&#x75;&#x74;&#x68;&#64;&#110;&#x76;&#105;&#x64;&#105;&#x61;&#46;&#x63;&#x6f;&#109;</a>&gt;çš„é‚®ä»¶å¾€æ¥</h3><h4 id="æˆ‘æ„šè ¢çš„é—®é¢˜"><a href="#æˆ‘æ„šè ¢çš„é—®é¢˜" class="headerlink" title="æˆ‘æ„šè ¢çš„é—®é¢˜"></a>æˆ‘æ„šè ¢çš„é—®é¢˜</h4><p>Hi Julien<br>Sorry to bother you. I am an intern in devtech team. I have seen your email about LDGDEPBAR with LDGSTS.  I have a question about it.<br>Can LDGSTS bands to a certain gmem barrierï¼ŸFor example, I want to do a prefetch on smem. In a for cycle, can i issue a set of LDGSTS for variable A and LDS variable B.  Then i sync LDGSTS for A. next loop i issue a set of LDGSTS for B and then LDS A. THEN I sync LDGSTS for B. then run this two loop many times.<br>is this possible by using inlinePTXï¼Ÿ<br>I will really appreciate if you can answer my question.<br>thanks a lot. </p>
<blockquote>
<p>è¿™é‡Œæˆ‘æƒ³åšä¸€ä¸ªSMEMçš„prefetchï¼Œåœ¨å½“å‰å¾ªç¯æ­¥ä½¿ç”¨LDGSTSä»GMEMä¸­é¢„å…ˆå¼‚æ­¥è¯»å–ä¸‹ä¸€ä¸ªå¾ªç¯æ­¥è¦ä½¿ç”¨åˆ°çš„æ•°æ®åˆ°SMEMã€‚è€Œå½“å‰å¾ªç¯æ­¥ä½¿ç”¨LDSä»SMEMä¸­è¯»å–ä¸Šä¸€å¾ªç¯é¢„è¯»çš„æ•°æ®ã€‚</p>
</blockquote>
<h4 id="å¤§ä½¬çš„å›ç­”"><a href="#å¤§ä½¬çš„å›ç­”" class="headerlink" title="å¤§ä½¬çš„å›ç­”"></a>å¤§ä½¬çš„å›ç­”</h4><p>Hi Jiang,</p>
<p>You are not bothering me ğŸ˜Š. I hope you enjoy your internship with NVIDIA and the Devtech team.</p>
<p>Say you have two LDGSTS for A and two LDGSTS for B and you want to alternate between both:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>  LDGSTS A0 <span class="comment">// write to shared memory</span></span><br><span class="line"><span class="number">2</span>  LDGSTS A1 <span class="comment">// write to shared memory â€“ you have to make sure thatâ€™s a different region from A0</span></span><br><span class="line"><span class="number">3</span>  LDGDEPBAR </span><br><span class="line"><span class="number">4</span> </span><br><span class="line"><span class="number">5</span>  LDS B0 <span class="comment">// load from shared memory â€“ that buffer must have been filled earlier</span></span><br><span class="line"><span class="number">6</span>  LDS B1</span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span>  DEPBAR <span class="number">0</span> <span class="comment">// make sure the instructions before the LAST LDGDEPBAR have completed =&gt; A0/A1 are in shared memory</span></span><br><span class="line"><span class="number">9</span>  BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">11</span> LDGSTS B2 <span class="comment">// trigger the next LDGSTS for B â€“ you can reuse the same memory buffer than the one for B0</span></span><br><span class="line"><span class="number">12</span> LDGSTS B3</span><br><span class="line"><span class="number">13</span> LDGDEPBAR </span><br><span class="line"><span class="number">14</span>  </span><br><span class="line"><span class="number">15</span> LDS A0 </span><br><span class="line"><span class="number">16</span> LDS A1</span><br><span class="line"><span class="number">17</span> </span><br><span class="line"><span class="number">18</span> DEPBAR <span class="number">0</span> </span><br><span class="line"><span class="number">19</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>&emsp;&emsp;è§£è¯»ä¸€ä¸‹è¿™æ®µä»£ç ï¼ŒLine 2 ç‰¹æ„å‘ŠçŸ¥æˆ‘ â€œyou have to make sure thatâ€™s a different region from A0â€ æ˜¯ç”±äºåŒä¸€ä¸ª cp.async_group ä¸­çš„ cp.saync æ“ä½œçš„å…ˆåé¡ºåºæ˜¯æ²¡æœ‰ä¿è¯çš„ï¼Œå› æ­¤å¯¹åŒä¸€åœ°å€çš„æ‹·è´ä¼šå¯¼è‡´æœªå®šä¹‰çš„ç»“æœã€‚<br>&emsp;&emsp;Line 3 æ˜¯cp.async.commit_groupï¼Œåˆ›å»ºä¸€ä¸ªcp.async_groupå°†å‰ä¸¤ä¸ªLDGSTSåŒ…å«åœ¨å†…ã€‚åˆ°æ­¤ä¸ºæ­¢çš„ä¸‰è¡Œå‘èµ·äº†ä»GMEMåˆ°SMEMçš„å¼‚æ­¥æ‹·è´æ“ä½œï¼Œå¹¶å°†å…¶å½’çº³åˆ°äº†åŒä¸€ä¸ªcp.async_groupã€‚<br>&emsp;&emsp;Line 5 &amp; 6 ä½¿ç”¨LDSä»SMEMä¸­è¯»å–ä¸Šä¸€å¾ªç¯ä¸­å‡†å¤‡å¥½çš„æ•°æ®ã€‚<br>&emsp;&emsp;Line 8 ä¸ºcp.async.wait_group 0; ç­‰å¾…ç›´åˆ°è¿˜å‰©ä¸‹0ä¸ªcp.async_groupæœªå®Œæˆï¼Œä¹Ÿå°±æ˜¯ç­‰å¾…æ‰€æœ‰cp.async_groupå®Œæˆã€‚è¿™é‡Œä¸ºç¡®ä¿A0ã€A1æ•°æ®å‡†å¤‡å°±ç»ªã€‚<br>&emsp;&emsp;BAR.SYNC ä¸º__syncthreads()ï¼ŒåŒæ­¥blockå†…æ‰€æœ‰çº¿ç¨‹ã€‚ï¼ˆä»SMEMä¸­è¯»å–æ•°æ®å¿…é¡»ä¿è¯åŒæ­¥ï¼Œä¿è¯æ‰€æœ‰çº¿ç¨‹éƒ½å·²ç»è¯»å–å®Œæ¯•ï¼‰<br>&emsp;&emsp;åç»­æ“ä½œé‡å¤è¿™ä¸€è¿‡ç¨‹ï¼ŒåŒºåˆ«æ˜¯äº¤æ¢äº†é¢„è¯»ä½¿ç”¨çš„ä¸å½“å‰è¯»å–çš„SMEM bufferã€‚å³ä¸ºä¸Šä¸€å¾ªç¯é¢„è¯»æ•°æ®å­˜å…¥A0ã€A1ï¼Œä½¿ç”¨B0ã€B1å·²ç»è¯»å¥½çš„æ•°æ®ã€‚ä¸‹ä¸€å¾ªç¯é¢„è¯»æ•°æ®å­˜å…¥B0ã€B1ï¼Œä½¿ç”¨A0ã€A1å·²ç»è¯»å¥½çš„æ•°æ®ã€‚ä»¥æ­¤è¾¾åˆ°éšè—è®¿å­˜å»¶è¿Ÿçš„æ•ˆæœã€‚</p>
</blockquote>
<p>This code does the trick. </p>
<p>That said if you have enough shared memory you can do smarter things:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>  LDGSTS A0</span><br><span class="line"><span class="number">2</span>  LDGSTS A1</span><br><span class="line"><span class="number">3</span>  LDGDEPBAR </span><br><span class="line"><span class="number">4</span> </span><br><span class="line"><span class="number">5</span>  LDGSTS B2 <span class="comment">// It must write to a different buffer in shared memory than the one used for B0/B1</span></span><br><span class="line"><span class="number">6</span>  LDGSTS B3</span><br><span class="line"><span class="number">7</span>  LDGDEPBAR </span><br><span class="line"><span class="number">8</span> </span><br><span class="line"><span class="number">9</span>  LDS B0</span><br><span class="line"><span class="number">10</span> LDS B1</span><br><span class="line"><span class="number">11</span> </span><br><span class="line"><span class="number">12</span> DEPBAR <span class="number">1</span> <span class="comment">// The LDGSTS B2/B3 can still be in flight</span></span><br><span class="line"><span class="number">13</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">14</span> </span><br><span class="line"><span class="number">15</span> LDGSTS A2 <span class="comment">// It must write to a different buffer in shared memory than the one used for A0/A1 â€“ shared memory for B0/B1?</span></span><br><span class="line"><span class="number">16</span> LDGSTS A3</span><br><span class="line"><span class="number">17</span> LDGDEPBAR </span><br><span class="line"><span class="number">18</span> </span><br><span class="line"><span class="number">19</span> LDS A0</span><br><span class="line"><span class="number">20</span> LDS A1</span><br><span class="line"><span class="number">21</span>  </span><br><span class="line"><span class="number">22</span> DEPBAR <span class="number">1</span> <span class="comment">// A2/A3 can still be in flight </span></span><br><span class="line"><span class="number">23</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br></pre></td></tr></table></figure>
<p>I hope it makes sense. If not, do not hesitate to ask questions.</p>
<blockquote>
<p>&emsp;&emsp;å†æ¥çœ‹ä¸€ä¸‹è¿™æ®µä»£ç ï¼Œå¤§ä½“ä¸ŠåŸºæœ¬ç›¸ä¼¼ã€‚Line 3ã€7åˆ†åˆ«åˆ›å»ºäº†ä¸¤ä¸ªcp.async-groupï¼ŒLine 3åŒ…å«äº†Line 1ã€2çš„å¼‚æ­¥æ‹·è´æ“ä½œï¼Œè€ŒLine 7åˆ™åŒ…å«Line 5ã€6çš„å¼‚æ­¥æ‹·è´ã€‚éšåLDSä»SMEMä¸­B0ã€B1åœ°å€è¯»å–åœ¨ä¹‹å‰å·²ç»å‡†å¤‡å¥½çš„æ•°æ®ã€‚<br>&emsp;&emsp;Line 12 DEPBAR 1;æŒ‡å®šçº¿ç¨‹ç­‰å¾…ç›´åˆ°ä»…å‰©ä¸€ä¸ªcp.async-groupï¼Œè¿™é‡Œåªæœ‰ä¸¤ä¸ªï¼Œä¸ºç­‰å¾…A0ã€A1è®¿å­˜ç»“æŸã€‚æ­¤æ—¶B2ã€B3è¿˜åœ¨ä¼ è¾“ä¸­ã€‚<br>&emsp;&emsp;Line 17åˆåˆ›å»ºäº†ä¸€ä¸ªcp.async-groupï¼ŒåŒ…å«äº†Line 15ã€16çš„å¼‚æ­¥æ‹·è´æ“ä½œã€‚<br>&emsp;&emsp;ç”±äºA0ã€A1æ•°æ®å·²ç»å‡†å¤‡å°±ç»ªï¼Œå› æ­¤å¯ç›´æ¥LDSè®¿é—®A0ã€A1ã€‚<br>&emsp;&emsp;æ‰§è¡Œåˆ°Line 22æ—¶ï¼ŒåŒæ ·æŒ‡å®šçº¿ç¨‹ç­‰å¾…ç›´åˆ°ä»…å‰©ä¸€ä¸ªcp.async-groupï¼Œæ­¤æ—¶ä»…å‰©2ä¸ªcp.async-groupä»åœ¨æ‰§è¡Œï¼Œå³ä¸ºç­‰å¾…B2ã€B3æ•°æ®æ‹·è´ç»“æŸã€‚</p>
</blockquote>
<p>è¡¥å……ä¸€ä¸‹åç»­å…¶ä»–æ“ä½œï¼Œè¿™ç§æ”¹åŠ¨ç›¸å½“äºæ˜¯ä½¿ç”¨æ›´å¤šçš„SMEMï¼Œå°†æ¯æ¬¡LDGSTSä¸­é—´ç©¿æ’ä¸¤æ¬¡LDSï¼Œå¢å¤§å‘èµ·ä»GMEMä¸­å–å›æ•°æ®çš„å¼‚æ­¥æ‹·è´æ“ä½œä¸LDSä¹‹é—´çš„æ—¶é—´é—´éš”(é—´éš”ä¸¤ä¸ªå¾ªç¯æ­¥çš„prefetch)ï¼Œæ›´å¥½åœ°éšè—è®¿å­˜å»¶è¿Ÿã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>  LDGSTS A0</span><br><span class="line"><span class="number">2</span>  LDGSTS A1</span><br><span class="line"><span class="number">3</span>  LDGDEPBAR </span><br><span class="line"><span class="number">4</span> </span><br><span class="line"><span class="number">5</span>  LDGSTS B2 <span class="comment">// It must write to a different buffer in shared memory than the one used for B0/B1</span></span><br><span class="line"><span class="number">6</span>  LDGSTS B3</span><br><span class="line"><span class="number">7</span>  LDGDEPBAR </span><br><span class="line"><span class="number">8</span>  <span class="comment">// A0 A1 B2 B3 PENDING B0 B1 READY</span></span><br><span class="line"><span class="number">9</span>  LDS B0</span><br><span class="line"><span class="number">10</span> LDS B1</span><br><span class="line"><span class="number">11</span> </span><br><span class="line"><span class="number">12</span> DEPBAR <span class="number">1</span> <span class="comment">// The LDGSTS B2/B3 can still be in flight</span></span><br><span class="line"><span class="number">13</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">14</span> <span class="comment">// B2 B3 PENDING B0 B1 A0 A1 READY</span></span><br><span class="line"><span class="number">15</span> LDGSTS A2 <span class="comment">// It must write to a different buffer in shared memory than the one used for A0/A1 â€“ shared memory for B0/B1?</span></span><br><span class="line"><span class="number">16</span> LDGSTS A3</span><br><span class="line"><span class="number">17</span> LDGDEPBAR </span><br><span class="line"><span class="number">18</span> <span class="comment">// B2 B3 A2 A3 PENDING B0 B1 A0 A1 READY</span></span><br><span class="line"><span class="number">19</span> LDS A0</span><br><span class="line"><span class="number">20</span> LDS A1</span><br><span class="line"><span class="number">21</span>  </span><br><span class="line"><span class="number">22</span> DEPBAR <span class="number">1</span> <span class="comment">// A2/A3 can still be in flight </span></span><br><span class="line"><span class="number">23</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">24</span> <span class="comment">// A2 A3 PENDING B0 B1 A0 A1 B2 B3 READY</span></span><br><span class="line"><span class="number">25</span> LDGSTS B0</span><br><span class="line"><span class="number">26</span> LDGSTS B1</span><br><span class="line"><span class="number">27</span> LDGDEPBAR</span><br><span class="line"><span class="number">28</span> <span class="comment">// A2 A3 B0 B1 PENDING A0 A1 B2 B3 READY</span></span><br><span class="line"><span class="number">29</span> LDS B2</span><br><span class="line"><span class="number">30</span> LDS B3</span><br><span class="line"><span class="number">31</span> </span><br><span class="line"><span class="number">32</span> DEPBAR <span class="number">1</span> </span><br><span class="line"><span class="number">33</span> BAR.SYNC</span><br><span class="line"><span class="number">34</span> <span class="comment">// B0 B1 PENDING A0 A1 B2 B3 A2 A3 READY</span></span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/20/cuda-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/20/cuda-4/" class="post-title-link" itemprop="url">CUDAå­¦ä¹ éšè®°4 Voltaâ€™s Independent Thread Scheduling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-20 20:09:48" itemprop="dateCreated datePublished" datetime="2022-02-20T20:09:48+08:00">2022-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-28 16:55:12" itemprop="dateModified" datetime="2022-06-28T16:55:12+08:00">2022-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDAå­¦ä¹ éšè®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Voltaâ€™s-Independent-Thread-Scheduling"><a href="#Voltaâ€™s-Independent-Thread-Scheduling" class="headerlink" title="Voltaâ€™s Independent Thread Scheduling"></a>Voltaâ€™s Independent Thread Scheduling</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/inside-volta/">Inside Volta: The Worldâ€™s Most Advanced Data Center GPU</a></p>
<p>High Performance Computing (HPC) is a fundamental pillar of modern science. From predicting weather, to discovering drugs, to finding new energy sources, researchers use large computing systems to simulate and predict our world. AI extends traditional HPC by allowing researchers to analyze large volumes of data for rapid insights where simulation alone cannot fully predict the real world.<br>é«˜æ€§èƒ½è®¡ç®—HPCæ˜¯ç°ä»£ç§‘å­¦çš„åŸºæœ¬æ”¯æŸ±ã€‚ä»å¤©æ°”é¢„æµ‹åˆ°è¯ç‰©ç ”ç©¶ï¼Œå†åˆ°è·å–æ–°èƒ½æºï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨å¤§å‹çš„è®¡ç®—ç³»ç»Ÿæ¥æ¨¡æ‹Ÿä¸é¢„æµ‹æˆ‘ä»¬çš„ä¸–ç•Œã€‚äººå·¥æ™ºèƒ½AIé€šè¿‡å…è®¸ç ”ç©¶äººå‘˜åˆ†æå¤§é‡çš„æ•°æ®ä»¥ä¾¿å¿«é€Ÿè·å¾—æ´å¯Ÿï¼Œæ‹“å±•äº†ä¼ ç»Ÿçš„é«˜æ€§èƒ½è®¡ç®—ï¼Œç”¨äºä»…é æ¨¡æ‹Ÿä¸èƒ½å®Œå…¨é¢„æµ‹ç°å®ä¸–ç•Œçš„æƒ…å†µã€‚<br>Computational Science for scientific simulation.<br>Data Science for finding insights in data.</p>
<center><img src="/2022/02/20/cuda-4/Volta%20GV100%20architecture.png" width="100%" height="100%"><font color="#708090" size="2">Volta GV100 Full GPU with 84 SM Units.</font></center>

<p><strong>æ¯å— GV100 åŒ…å«ï¼š</strong><br>6 x Graphics Processing Clusters (GPCs)<br>æ¯ä¸ªGPCåŒ…å« 7 x Texture Processing Clusters (TPCs)ï¼Œå…± 42<br>æ¯ä¸ªTPCåŒ…å« 2 x Streaming Multiprocessors (SMs)ï¼Œå…± 84<br>8 x 512-bit Memory Controllers(4096 bits total) ç”¨äºä¸æ˜¾å­˜è¿›è¡Œæ•°æ®äº¤æ¢</p>
<p><strong>æ¯ä¸ª SM åŒ…å«ï¼š</strong><br>64 FP32 Cores<br>64 INT32 Cores<br>32 FP64 Cores<br><em>8 new Tensor Cores</em><br>4 texture units</p>
<center><img src="/2022/02/20/cuda-4/Volta%20GV100%20SM.png" width="100%" height="100%"><font color="#708090" size="2">Volta GV100 Full GPU with 84 SM Units.</font></center>

<p><strong>æ¯ä¸ªSMè¢«åˆ’åˆ†ä¸ºå››ä¸ªéƒ¨ä»½partitionsï¼Œæ¯ä¸ªéƒ¨ä»½åŒ…å«ï¼š</strong><br>8 FP64 Cores<br>16 INT32 Cores<br>16 FP32 Cores<br>2 new mixed-precision Tensor Cores for deep learning matrix arithmetic<br>a new L0 instruction cache<br>a warp scheduler<br>a dispatch unit<br>a 64 KB Register File</p>
<p>Unlike Pascal GPUs, which could not execute FP32 and INT32 instructions simultaneously, the Volta GV100 SM includes separate FP32 and INT32 cores, allowing simultaneous execution of FP32 and INT32 operations at full throughput, while also increasing instruction issue throughput. </p>
<p>Pascal GPUæ‰€æœ‰çš„CUDAæ ¸å¿ƒéƒ½æ˜¯FP32+INT32æ··åˆæ ¸å¿ƒï¼Œè¿™ç§æ ¸å¿ƒæ¯ä¸ªå‘¨æœŸåªèƒ½è¿›è¡Œæµ®ç‚¹è¿ç®—æˆ–æ•´å‹è¿ç®—çš„ä¸€ç§ã€‚æ•´å—SMåˆ†åŒºï¼ˆå…·æœ‰ä¸€ä¸ªwarp schedulerï¼‰åŒæ—¶åˆ‡æ¢ï¼Œè¿™å—åˆ†åŒºæŸä¸€æ—¶åˆ»åªèƒ½è¿›è¡ŒFPæˆ–INTè¿ç®—çš„ä¸€ç§ã€‚</p>
<p>Volta/Turing GPUå•ç‹¬å¼€è¾Ÿäº†ä¸€æ¡INTå‹é€šé“ï¼Œå…¶å…·æœ‰ç‹¬ç«‹çš„FP32ä¸INT32æ ¸å¿ƒï¼Œå¯åŒæ—¶æ‰§è¡ŒFP32ä¸INT32æ“ä½œã€‚</p>
<blockquote>
<p>å…³äºNVIDIAä¸‰ä»£GPUæ¶æ„CUDAæ ¸å¿ƒçš„å˜åŒ–ï¼ˆPascal, Turing, Ampereï¼‰<br><a target="_blank" rel="noopener" href="https://weibo.com/tv/show/1034:4548031207637036?from=old_pc_videoshow">å¾®åš æ—äº¦LYi: ä»€ä¹ˆæ˜¯ã€Œæ˜¾å¡æµ‹ä¸å‡†åŸç†ã€</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1a64y1F7Gp?from=search&amp;seid=665276990359275781&amp;spm_id_from=333.337.0.0">Bç«™ æ—äº¦LYi: ä»€ä¹ˆæ˜¯ã€Œæ˜¾å¡æµ‹ä¸å‡†åŸç†ã€</a></p>
</blockquote>
<center><img src="/2022/02/20/cuda-4/FP_INT.png" width="100%" height="100%"><font color="#708090" size="2">Floating Point vs Integer.</font></center>
<center><img src="/2022/02/20/cuda-4/CUDA_cores.png" width="100%" height="100%"><font color="#708090" size="2">SM of Volta & Turing</font></center>

<blockquote>
<p>RTX20ç³»æ˜¾å¡ä½¿ç”¨Turingæ¶æ„ï¼Œè€ŒRTX30ç³»æ˜¾å¡ä½¿ç”¨Ampereæ¶æ„ã€‚<br>å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼ŒFP32æ ¸å¿ƒè¦æ¯”INT32æ ¸å¿ƒæ›´å¤§ä¸€äº›ï¼Œè¿™æ˜¯ç”±äºç”¨äºæµ®ç‚¹å‹è¿ç®—çš„æ ¸å¿ƒè¦æ›´å¤æ‚ä¸€äº›ï¼Œä¹Ÿåªæœ‰FP32æ ¸å¿ƒæ‰è¢«ç§°ä¸ºCUDAæ ¸å¿ƒã€‚<br>Turingæ¶æ„ä¸­ï¼ŒINT32æ ¸å¿ƒå’ŒFP32æ ¸å¿ƒæ˜¯ç‹¬ç«‹çš„ï¼Œæ¯ä¸ªSMä¸­å„æœ‰64ä¸ªã€‚<br>Ampereæ¶æ„ä¸­ï¼Œé™¤äº†64ä¸ªFP32æ ¸å¿ƒï¼ŒåŸæœ‰çš„64ä¸ªINT32æ ¸å¿ƒè½¬å˜ä¸ºäº†64ä¸ªFP32+INT32æ ¸å¿ƒï¼Œè€Œè¿™éƒ¨åˆ†æ ¸å¿ƒä¹Ÿç®—ä½œCUDAæ ¸å¿ƒï¼Œå› æ­¤æ¯ä¸ªSMä¸Šå…±æœ‰64+64=128ä¸ªCUDAæ ¸å¿ƒã€‚<br>FP32+INT32æ ¸å¿ƒåœ¨ä¸€ä¸ªå‘¨æœŸå†…ï¼Œåªèƒ½è¿›è¡Œæµ®ç‚¹å‹æˆ–æ•´æ•°å‹è¿ç®—çš„ä¸€ç§ã€‚äºŒé€‰ä¸€ã€‚æ··åˆæ ¸å¿ƒå¹¶ä¸èƒ½æä¾›å®Œå…¨çš„æµ®ç‚¹è¿ç®—èƒ½åŠ›ã€‚ç¨‹åºçš„æ•´å‹è¿ç®—è¶Šå¤šï¼Œæµ®ç‚¹è¿ç®—èƒ½åŠ›å°±ä¼šä¸‹é™è¶Šæ˜æ˜¾ã€‚è¿™æ˜¯ç”±äºæ··åˆæ ¸å¿ƒåœ¨FP32ä¸INT32æ ¸å¿ƒé—´çš„åˆ‡æ¢å¯¼è‡´çš„ã€‚æ¯æ¬¡åˆ‡æ¢æ—¶ï¼Œä»¥SMä¸­çš„ä¸€å—partitionä¸ºå•ä½åˆ‡æ¢ï¼Œpartitionä¸­çš„æ‰€æœ‰æ··åˆæ ¸å¿ƒåŒæ—¶åˆ‡æ¢ã€‚</p>
</blockquote>
<center><img src="/2022/02/20/cuda-4/CUDA_cores_1.png" width="100%" height="100%"><font color="#708090" size="2">SM of Volta & Turing</font></center>

<blockquote>
<p>Pascalæ¶æ„ä¸­æ‰€æœ‰CUDAæ ¸å¿ƒéƒ½æ˜¯æ··åˆæ ¸å¿ƒï¼ŒSMæ¯ä¸ªåˆ†åŒºä¸€ä¸ªæ—¶åˆ»åªèƒ½è¿›è¡Œä¸€ç§è¿ç®—ã€‚<br>Turingæ¶æ„ä¸­ä¸ºINTå‹å•ç‹¬å¼€è¾Ÿä¸€æ¡é€šé“ï¼Œå¯åŒæ—¶æ‰§è¡ŒINTå‹è¿ç®—ä¸FPè¿ç®—ï¼Œä½†è¿™æ ·INTå‹è¿ç®—æ€§èƒ½è¿‡å‰©ï¼Œå¤šæ•°æ—¶é—´éƒ½åœ¨æ‘¸é±¼ã€‚<br>Ampereæ¶æ„ä¸­æŠŠINTå‹æ ¸å¿ƒåˆæ¢å›äº†æ··åˆæ ¸å¿ƒï¼Œè¿™æ ·å°±é¿å…äº†INTå‹æ ¸å¿ƒç©ºé—²çš„é—®é¢˜ã€‚</p>
</blockquote>
<h2 id="Tensor-Cores"><a href="#Tensor-Cores" class="headerlink" title="Tensor Cores"></a>Tensor Cores</h2><p>Matrix-Matrix multiplication (BLAS GEMM) operations are at the core of neural network training and inferencing, and are used to multiply large matrices of input data and weights in the connected layers of the network.<br>çŸ©é˜µä¹˜æ“ä½œæ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒå’Œæ¨ç†çš„æ ¸å¿ƒï¼Œè¢«ç”¨äºåšè¾“å…¥æ•°æ®å’Œå¯¹åº”æƒé‡çŸ©é˜µçš„ä¹˜æ³•ã€‚<br>BLAS: Basic Linear Algebra Subprograms åŸºæœ¬çº¿æ€§ä»£æ•°å­ç¨‹åº<br>GEMM: General Matrix Multiplication é€šç”¨çŸ©é˜µä¹˜æ³•</p>
<p>Each Tensor Core provides a 4x4x4 matrix processing array which performs the operation $\textbf{D} = \textbf{A} \times \textbf{B} + \textbf{C}$, where $\textbf{A}$, $\textbf{B}$, $\textbf{C}$, and $\textbf{D}$ are 4Ã—4 matrices as shown below. The matrix multiply inputs $\textbf{A}$ and $\textbf{B}$ are FP16 matrices, while the accumulation matrices $\textbf{C}$ and $\textbf{D}$ may be FP16 or FP32 matrices.<br>æ¯ä¸ªTensor Coreæä¾›ä¸€ä¸ª4x4x4çš„çŸ©é˜µå¤„ç†é˜µåˆ—ï¼Œåˆ†åˆ«åŒ…æ‹¬ABCDå››ä¸ª4x4çš„çŸ©é˜µï¼Œä¹˜æ³•çŸ©é˜µABæ˜¯FP16ï¼ˆåŠç²¾åº¦halfç±»å‹ï¼‰çš„ï¼Œè€ŒåŠ æ³•çŸ©é˜µCDå¯ä»¥æ˜¯FP16ä¹Ÿå¯ä»¥æ˜¯FP32ã€‚</p>
<center><img src="/2022/02/20/cuda-4/Tensor%20Core%204x4x4%20matrix%20multiply%20and%20accumulate.png" width="100%" height="100%"><font color="#708090" size="2">Tensor Core 4x4x4 matrix multiply and accumulate.</font></center>

<p>Each Tensor Core performs 64 floating point FMA mixed-precision operations per clock.<br>FMA: Fused Multiply Add/Accumulate èåˆä¹˜åŠ <br>4x4 çŸ©é˜µä¹˜$\textbf{A} \times \textbf{B}$ï¼Œä¸€å…±åš 4x4x4 = 64æ¬¡ä¹˜æ³•ï¼Œ4x4x3 = 48æ¬¡åŠ æ³•ã€‚ç»“æœçŸ©é˜µåŠ ä¸€ä¸ª 4x4 çŸ©é˜µ$\textbf{C}$ã€‚æ•´ä¸ªè®¡ç®—ä¸€å…±64æ¬¡ä¹˜ï¼Œ64æ¬¡åŠ ï¼Œå› æ­¤ä¸º64 FMA operationsã€‚<br>D00 = A00*B00 + C00 FMA<br>D00 = A01*B10 + D00 FMA<br>D00 = A02*B20 + D00 FMA<br>D00 = A03*B30 + D00 FMA<br>ç»“æœçŸ©é˜µ$\textbf{D}$ä¸­ä¸€ä¸ªæ•°å¯¹åº”4æ¬¡FMAã€‚</p>
<p>During program execution, multiple Tensor Cores are used concurrently by a full warp of execution. The threads within a warp provide a larger 16x16x16 matrix operation to be processed by the Tensor Cores. CUDA exposes these operations as Warp-Level Matrix Operations in the CUDA C++ API. These C++ interfaces provide specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently utilize Tensor Cores in CUDA C++ programs.<br>å¤šä¸ªTensor coresè¢«ä¸€ä¸ªå®Œæ•´çš„warpåŒæ—¶ä½¿ç”¨ï¼Œä¸ºWarp-Level Matrix Operationsï¼ŒåŒ…æ‹¬matrix load, matrix multiply and accumulate, and matrix store operationsã€‚<br>ä¸€ä¸ªTensor Coreå¯ä»¥æä¾›ä¸€ä¸ª<strong>4x4x4</strong>çš„çŸ©é˜µæ“ä½œï¼Œä¸€ä¸ªwarpå¯ä½¿ç”¨å¤šä¸ªTensor Coresï¼Œä»è€Œæä¾›ä¸€ä¸ª<strong>16x16x16</strong>çš„çŸ©é˜µæ“ä½œï¼Œä¸ºwarpçº§åˆ«çš„æ“ä½œã€‚</p>
<p>In addition to CUDA C++ interfaces to program Tensor Cores directly, CUDA 9 cuBLAS and cuDNN libraries include new library interfaces to make use of Tensor Cores for deep learning applications and frameworks.<br>é™¤äº†CUDA C++ä¹‹å¤–ï¼ŒCUDA 9.0ä¸­çš„cuBLASä¸cuDNNåº“ä¹Ÿæä¾›äº†è°ƒç”¨Tensor coreçš„æ¥å£ã€‚</p>
<p>ä»Voltaå¼€å§‹ï¼ŒL1 data cacheä¸shared memåˆå¹¶ä¸ºä¸€å—å†…å­˜ã€‚å…±äº«128 KB/SMçš„å†…å­˜ç©ºé—´ã€‚<br>The new combined L1 data cache and shared memory subsystem of the Volta SM significantly improves performance while also simplifying programming and reducing the tuning required to attain at or near-peak application performance.<br>è¿™æ ·åšåœ¨æå‡äº†ç¨‹åºæ€§èƒ½çš„åŒæ—¶ï¼Œç®€åŒ–äº†ç¼–ç¨‹ï¼Œä¹Ÿå‡å°‘äº†è¾¾åˆ°å³°å€¼æ€§èƒ½æ‰€éœ€çš„è°ƒä¼˜ã€‚<br>narrows the gap between applications that are manually tuned to keep data in shared memory and those that access data in device memory directly.<br>ç”±äºç°åœ¨L1ä¸shared memå®é™…ä¸ºä¸€å—å†…å­˜ï¼Œå…·æœ‰ç›¸åŒçš„ç‰¹æ€§ï¼Œå› æ­¤ä¹Ÿç¼©å°äº†äººä¸ºä½¿ç”¨shared memä¼˜åŒ–ç¨‹åºä¸ç›´æ¥è®¿é—®device memçš„ç¨‹åºæ€§èƒ½å·®è·ã€‚<br>allow L1 cache operations to attain the benefits of shared memory performance. Shared memory provides high bandwidth and low latency, but the CUDA programmer needs to explicitly manage this memory. Volta narrows the gap between applications that explicitly manage shared memory and those that access data in device memory directly.<br>Shared memå¸¦å®½é«˜å»¶è¿Ÿä½ï¼Œä½†éœ€è¦äººä¸ºæ§åˆ¶ã€‚å› æ­¤åˆå¹¶L1ä¸shared memå¯ä»¥ä½¿å¾—L1 cacheè·å¾—shared memçš„ç‰¹æ€§ï¼Œä»è€Œç¼©å°ç›´æ¥è®¿é—®device memä¸äººä¸ºä½¿ç”¨shared memçš„æ€§èƒ½å·®è·ï¼Œç®€åŒ–ç¼–ç¨‹ã€‚</p>
<h2 id="Independent-Thread-Scheduling"><a href="#Independent-Thread-Scheduling" class="headerlink" title="Independent Thread Scheduling"></a>Independent Thread Scheduling</h2><p>Volta GV100 is the first GPU to support independent thread scheduling, which enables finer-grain synchronization and cooperation between parallel threads in a program.<br>Volta GV100æ˜¯é¦–ä¸ªæ”¯æŒç‹¬ç«‹çº¿ç¨‹è°ƒåº¦çš„GPUã€‚</p>
<h4 id="Prior-NVIDIA-GPU-SIMT-Models"><a href="#Prior-NVIDIA-GPU-SIMT-Models" class="headerlink" title="Prior NVIDIA GPU SIMT Models"></a>Prior NVIDIA GPU SIMT Models</h4><p>Pascal and earlier NVIDIA GPUs execute <strong>groups of 32 threads, known as warps</strong>, in SIMT (Single Instruction, Multiple Thread) fashion. The Pascal warp uses <strong>a single program counter shared amongst all 32 threads</strong>, combined with an â€œ<strong>active mask</strong>â€ that specifies which threads of the warp are active at any given time.  This means that divergent execution paths leave some threads inactive, <strong>serializing execution</strong> for different portions of the warp as shown below. The original mask is stored until the warp reconverges at the end of the divergent section, at which point the mask is restored and the threads run together once again.<br>warpä¸­çš„32ä¸ªçº¿ç¨‹å…±äº«åŒä¸€ä¸ªç¨‹åºè®¡æ•°å™¨ï¼ˆä¹Ÿç§°ä¸ºæŒ‡ä»¤æŒ‡é’ˆï¼Œæˆ–æŒ‡ä»¤åœ°å€å¯„å­˜å™¨ï¼‰ï¼ŒåŒæ­¥æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼ŒåŒæ—¶ä½¿ç”¨ä¸€ä¸ªmaskæ ‡è®°å½“å‰å“ªäº›çº¿ç¨‹æ˜¯activeçš„ã€‚å½“é‡åˆ°åˆ†æ”¯è·¯å¾„æ—¶ï¼Œé¡ºåºæ‰§è¡Œä¸åŒçš„åˆ†æ”¯å¹¶å±è”½æ‰ä¸åœ¨æ­¤è·¯å¾„ä¸Šçš„çº¿ç¨‹ã€‚åœ¨åˆ†æ”¯éƒ¨ä»½å¼€å§‹æ—¶åˆ‡æ¢é¡ºåºæ‰§è¡Œï¼Œåœ¨åˆ†æ”¯ç»“æŸæ—¶é‡æ–°åŒæ­¥reconvergeæ‰€æœ‰çº¿ç¨‹ã€‚</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%201.png" width="100%" height="100%"><font color="#708090" size="2">Thread scheduling under the SIMT warp execution model of Pascal and earlier NVIDIA GPUs.</font></center>

<p>å¦‚å›¾æ‰€ç¤ºï¼Œä¼šå…ˆæ‰§è¡Œå…¶ä¸­ä¸€ä¸ªåˆ†æ”¯ç›´åˆ°ç»“æŸï¼Œå±è”½æ‰ä¸åœ¨è¿™æ¡åˆ†æ”¯ä¸Šçš„çº¿ç¨‹ï¼Œå†å¼€å§‹æ‰§è¡Œå¦ä¸€æ¡åˆ†æ”¯ï¼Œåœ¨åˆ†æ”¯ç»“æŸæ—¶æ‰€æœ‰çº¿ç¨‹é‡æ–°åŒæ­¥ã€‚</p>
<p>The Pascal SIMT execution model maximizes efficiency by reducing the quantity of resources required to track thread state and by aggressively reconverging threads to maximize parallelism. Tracking thread state in aggregate for the whole warp, however, means that when the execution pathway diverges, the threads which take different branches lose concurrency until they reconverge. This loss of concurrency means that threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data. This presents an inconsistency in which threads from different warps continue to run concurrently, but diverged threads from the same warp run sequentially until they reconverge. This means, for example, that algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from. Therefore, on Pascal and earlier GPUs, programmers have to avoid fine-grained synchronization or rely on lock-free or warp-aware algorithms.<br>The Pascal SIMT execution modelé€šè¿‡å‡å°‘è®°å½•çº¿ç¨‹çŠ¶æ€æ‰€éœ€èµ„æºï¼ˆåŒä¸€warpåŒæ­¥æ‰§è¡Œï¼Œåªè®°å½•ä¸€ä¸ªactive maskï¼‰ã€ç§¯æåŒæ­¥çº¿ç¨‹æœ€å¤§åŒ–å¹¶è¡Œåº¦ï¼ˆåˆ†æ”¯ç»“æŸå³ç«‹åˆ»åŒæ­¥warpä¸­çš„çº¿ç¨‹ï¼‰æ¥æœ€å¤§åŒ–æ•ˆç‡ã€‚<br>ä¸²è¡ŒåŒ–é€ä¸ªæ‰§è¡Œåˆ†æ”¯ï¼Œä¹Ÿå°±æ„å‘³ç€åŒä¸€ä¸ªwarpä¸­æ‰§è¡Œä¸åŒåˆ†æ”¯çš„çº¿ç¨‹ä¹‹é—´å¤±å»äº†å¹¶è¡Œæ€§ï¼Œå½¼æ­¤ä¸èƒ½é€šä¿¡ã€äº¤æ¢æ•°æ®ã€‚ä½†ä¸åŒwarpä¸­çš„çº¿ç¨‹ä»ç„¶æ˜¯åŒæ­¥çš„ï¼Œåˆ†æ”¯ä»…å±è”½æ‰æœ¬warpä¸­çš„ä¸€éƒ¨åˆ†çº¿ç¨‹è€Œæ‰§è¡Œå¦ä¸€éƒ¨åˆ†ï¼Œè¿™å°±æ˜¾ç¤ºå‡ºä¸€ç§çŸ›ç›¾ã€‚è¿™ä¹Ÿå°±é˜»æ­¢äº†æ›´ç»†ç²’åº¦ï¼ˆwarp-levelï¼‰çš„ç¼–ç¨‹ï¼ŒåŒä¸€ä¸ªwarpä¸­ä¸åŒçº¿ç¨‹äº¤æ¢æ•°æ®ä½¿ç”¨é”æˆ–äº’æ–¥ä½“å¾ˆå®¹æ˜“å¯¼è‡´æ­»é”é—®é¢˜ï¼ˆä¸€ä¸ªçº¿ç¨‹ç­‰å¾…å¦ä¸€ä¸ªçº¿ç¨‹ï¼Œä½†å¦ä¸€ä¸ªçº¿ç¨‹éœ€è¦ç­‰å¾…æœ¬çº¿ç¨‹æ‰§è¡Œå®Œæ¯•æ‰èƒ½æ‰§è¡Œï¼Œäº’ç›¸ç­‰å¾…å¯¼è‡´æ­»é”ï¼‰ã€‚</p>
<h4 id="Volta-SIMT-Model"><a href="#Volta-SIMT-Model" class="headerlink" title="Volta SIMT Model"></a>Volta SIMT Model</h4><p>enabling equal concurrency between all threads, regardless of warp.<br>maintaining execution state per thread, including the program counter and call stack.<br>Voltaé€šè¿‡ç»´æŠ¤æ¯ä¸€ä¸ªçº¿ç¨‹çš„æ‰§è¡ŒçŠ¶æ€ï¼ŒåŒ…æ‹¬ç¨‹åºè®¡æ•°å™¨ï¼ˆæŒ‡ä»¤åœ°å€å¯„å­˜å™¨ï¼‰å’Œè°ƒç”¨æ ˆï¼Œåœ¨æ‰€æœ‰çº¿ç¨‹é—´å®ç°äº†ç›¸åŒçš„å¹¶å‘æ€§ï¼Œå³åŒä¸€ä¸ªwarpä¸­çš„çº¿ç¨‹ä¹Ÿæ˜¯å¹¶å‘çš„ã€‚</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%202.png" width="100%" height="100%"><font color="#708090" size="2">Volta (bottom) independent thread scheduling architecture block diagram compared to Pascal and earlier architectures (top).</font></center>

<p>Volta maintains per-thread scheduling resources such as program counter (PC) and call stack (S), while earlier architectures maintained these resources per warp.<br>Voltaæ¯ä¸ªçº¿ç¨‹ç»´æŠ¤ä¸€ä¸ªprogram counter (PC) and call stack (S)ï¼Œè€Œä¹‹å‰çš„æ¶æ„æ¯ä¸ªwarpç»´æŠ¤ä¸€ä¸ªã€‚</p>
<p>To maximize parallel efficiency, Volta includes a <strong>schedule optimizer</strong> which determines how to group <strong>active threads from the same warp</strong> together into SIMT units. Threads can now diverge and reconverge at sub-warp granularity, and Volta will still <strong>group together threads which are executing the same code and run them in parallel</strong>.</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%203.png" width="100%" height="100%"><font color="#708090" size="2">Volta independent thread scheduling enables interleaved execution of statements from divergent branches.</font></center>

<p>Statements from the if and else branches in the program can now be interleaved in time.<br>if elseåˆ†æ”¯ä¸­çš„è¯­å¥ç°åœ¨å¯ä»¥äº¤é”™æ‰§è¡Œã€‚<br>Note that execution is still SIMT: at any given clock cycle CUDA cores execute the same instruction for all active threads in a warp just as before, retaining the execution efficiency of previous architectures.<br>ä»ç„¶æ˜¯SIMTæ‰§è¡Œæ¶æ„ï¼Œå•æŒ‡ä»¤æµå¤šçº¿ç¨‹ã€‚æ¯ä¸ªclock cycleï¼Œæ‰€æœ‰active threadsåœ¨CUDA coresä¸Šæ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ã€‚<br>Importantly, Voltaâ€™s ability to independently schedule threads within a warp makes it possible to implement complex, <strong>fine-grained algorithms (sub-warp)</strong> and data structures in a more natural way. While the scheduler supports independent execution of threads, it optimizes non-synchronizing code to maintain <strong>as much convergence as possible</strong> for maximum SIMT efficiency.<br>Independently schedule threads within a warpï¼Œä½¿å¾—ä½äºä¸åŒåˆ†æ”¯çš„è¯­å¥ï¼ˆç”±ä¸åŒactive threadsæ‰§è¡Œï¼‰å¯ä»¥äº¤é”™æ‰§è¡Œï¼Œä»è€Œwarpä¸­çš„çº¿ç¨‹å…·æœ‰concurrencyï¼Œå¯ä»¥äº’ç›¸åŒæ­¥ã€äº¤æ¢æ•°æ®ï¼Œä½¿å¾—sub-warpç®—æ³•æˆä¸ºå¯èƒ½ã€‚Schedulerå°½å¯èƒ½åœ°ä¼˜åŒ–éåŒæ­¥ä»£ç ï¼Œä»¥å®ç°æœ€å¤§åŒ–çš„convergenceï¼ˆå¹¶è¡Œæ€§è¶Šå¥½ï¼ŒSIMT efficiencyè¶Šé«˜ï¼‰ã€‚</p>
<p>It is interesting to note that the Figure does not show execution of statement Z by all threads in the warp at the same time. This is because the scheduler must conservatively assume that Z may produce data required by other divergent branches of execution in which case it would be unsafe to automatically enforce reconvergence. In the common case where A, B, X, and Y do not consist of synchronizing operations, the scheduler can identify that it is safe for the warp to naturally reconverge on Z, as on prior architectures.<br>ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œä½äºif elseåˆ†æ”¯è¯­å¥ä¹‹å¤–çš„Zè¯­å¥ï¼Œå¹¶æ²¡æœ‰è¢«æ‰€æœ‰çº¿ç¨‹åŒæ­¥æ‰§è¡Œã€‚ä¹Ÿå°±æ˜¯è¯´åœ¨divergenceç»“æŸçš„éƒ¨ä»½ï¼Œwarpä¸­çš„çº¿ç¨‹å¹¶ä¸ä¸€å®šä¼šreconvergeã€‚<br>è¿™æ˜¯ç”±äºç¼–è¯‘å™¨ä¿å®ˆåœ°å‡å®šæœ¬çº¿ç¨‹ä¸­çš„Zè¯­å¥ä¼šäº§ç”Ÿå…¶ä»–divergentçº¿ç¨‹æ‰§è¡Œæ‰€éœ€è¦çš„æ•°æ®ï¼Œå¦‚æœåŒæ­¥ä¼šå¯¼è‡´å…¶ä»–çº¿ç¨‹ä¸€ç›´å¤„äºç­‰å¾…è¿™éƒ¨åˆ†æ•°æ®çš„æƒ…å†µunsafeã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¦‚æœif elseä¸­çš„æŒ‡ä»¤æœªåŒ…å«åŒæ­¥æ“ä½œï¼Œschedulerèƒ½è¯†åˆ«å‡ºåœ¨æ‰§è¡ŒZä¹‹å‰reconvergeæ˜¯safeçš„ï¼Œé‚£ä¹ˆå°±ä¼šreconvergeï¼Œä»è€Œæé«˜å¹¶è¡Œæ€§æé«˜SIMT efficiencyã€‚<br>ä½†divergenceä¹‹åæ˜¯å¦reconverge warpä¸­çš„çº¿ç¨‹æ˜¯ç¼–è¯‘å™¨æ“…è‡ªåšä¸»ï¼Œæ²¡ä¿è¯ï¼Œæœ€å®‰å…¨çš„è¿˜æ˜¯äººä¸ºä½¿ç”¨CUDA 9 æ–°å¼•å…¥çš„__syncwarp()å‡½æ•°to force reconvergenceåŒæ­¥all threads in a warpã€‚</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%204.png" width="100%" height="100%"><font color="#708090" size="2">Programs can use explicit synchronization __syncwarp() to reconverge threads in a warp.</font></center>

<p>In this case, the divergent portions of the warp might not execute Z together, but all execution pathways from threads within a warp will complete before any thread reaches the statement after the <strong>syncwarp(). Similarly, placing the call to </strong>syncwarp() before the execution of Z would force reconvergence before executing Z, potentially enabling greater SIMT efficiency if the developer knows that this is safe for their application.<br>åˆ°è¾¾__syncwarp()æ—¶ï¼Œæ‰€æœ‰warpä¸­çš„divergent portionsæ‰§è¡Œå®Œæ¯•ï¼Œåœ¨æ­¤å¤„reconvergeã€‚</p>
<h4 id="Starvation-Free-Algorithms"><a href="#Starvation-Free-Algorithms" class="headerlink" title="Starvation-Free Algorithms"></a>Starvation-Free Algorithms</h4><p>Starvation-free algorithms are a key pattern enabled by independent thread scheduling.</p>
<p>Starvation: å¹¶å‘è¿›ç¨‹æ°¸ä¹…åœ°æ— æ³•è·å¾—æ‰§è¡Œå·¥ä½œæ‰€éœ€çš„æŸä¸ªèµ„æºçš„æƒ…å†µã€‚é¥¥é¥¿å¯èƒ½ä¼šå¯¼è‡´ç¨‹åºæ— æ•ˆæˆ–ä¸æ­£ç¡®ï¼Œå› ä¸ºæ— æ³•è·å¾—èµ„æºçš„çº¿ç¨‹æ²¡æœ‰æ­£ç¡®åœ°å®Œæˆå·¥ä½œã€‚<br>Starvation-Free: æ— é¥¥é¥¿ã€‚<strong>æŸä¸ªçº¿ç¨‹ï¼Œæ€»æ˜¯å¯ä»¥è·å–åˆ°æŸä¸ªèµ„æºï¼Œè·å–èµ„æºçš„æ—¶é—´ä¸ä½œé™åˆ¶ï¼Œå¯é•¿å¯çŸ­ï¼Œåªè¦èƒ½è·å–åˆ°å³å¯</strong>ã€‚å–å†³äºçº¿ç¨‹ä¹‹é—´æ˜¯å¦æœ‰ä¼˜å…ˆçº§çš„å­˜åœ¨ï¼Œå¦‚æœç³»ç»Ÿå…è®¸é«˜ä¼˜å…ˆçº§çš„çº¿ç¨‹æ’é˜Ÿï¼Œè¿™æ ·æœ‰å¯èƒ½å¯¼è‡´ä½ä¼˜å…ˆçº§çº¿ç¨‹äº§ç”Ÿé¥¥é¥¿ï¼Œèµ„æºè¢«é«˜ä¼˜å…ˆçº§çº¿ç¨‹ä¸€ç›´é”ä½ï¼Œå¯¼è‡´ä½ä¼˜å…ˆçº§çº¿ç¨‹ä¸€ç›´æ— æ³•è®¿é—®ã€‚Starvation-Freeçš„çº¿ç¨‹å¯ä»¥æ˜¯é˜»å¡çš„ï¼Œå¯ä»¥spinning on a lockç­‰å¾…æ•°æ®å¯ä¾›è®¿é—®ã€‚</p>
<p>Starvation-Free Algorithms are concurrent computing algorithms that are guaranteed to execute correctly so long as the system ensures that <strong>all threads have adequate access to a contended resource</strong>.<br>å¹¶å‘ç®—æ³•ï¼Œéœ€è¦ä¿è¯æ‰€æœ‰çº¿ç¨‹å¯¹ç«äº‰èµ„æºéƒ½å…·æœ‰å……åˆ†çš„è®¿é—®æƒé™ã€‚</p>
<p>Inserting nodes into a doubly-linked list in a multithreaded application.<br>åœ¨ä¸€ä¸ªå¤šçº¿ç¨‹åº”ç”¨ä¸­ï¼Œå‘ä¸€ä¸ªåŒå‘é“¾è¡¨ä¸­æ’å…¥èŠ‚ç‚¹ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">insert_after</span><span class="params">(Node *a, Node *b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node *c;</span><br><span class="line">    <span class="built_in">lock</span>(a); <span class="built_in">lock</span>(a-&gt;next);</span><br><span class="line">    c = a-&gt;next;</span><br><span class="line"></span><br><span class="line">    a-&gt;next = b;</span><br><span class="line">    b-&gt;prev = a;</span><br><span class="line"></span><br><span class="line">    b-&gt;next = c;</span><br><span class="line">    c-&gt;prev = b;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">unlock</span>(c); <span class="built_in">unlock</span>(a);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>åŒå‘é“¾è¡¨ï¼Œå‰åèŠ‚ç‚¹éƒ½è¦è®°å½•ã€‚<br>a -&gt; a_next(c)<br>a -&gt; b -&gt; a_next(c)<br>ç”±äºè¦ä¿®æ”¹aä¸a_nextä¸­çš„æ•°æ®ï¼Œå› æ­¤é¦–å…ˆé”å®šè¿™ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œä¿®æ”¹ä¹‹åè§£é”ã€‚</p>
<p>each element of a doubly linked list has at least three components: a next pointer, a previous pointer, and a lock providing the owner <strong>exclusive access</strong> to update the node.<br>ç‹¬å è®¿é—®æƒ</p>
<center><img src="/2022/02/20/cuda-4/a%20double-linked%20list%20with%20locks.png" width="100%" height="100%"><font color="#708090" size="2">A doubly-linked list with fine-grained locks. Per-node locks are acquired (left) before inserting node B into the list (right).</font></center>

<p>Independent thread scheduling in Volta ensures that even if a thread T0 currently holds the lock for node A, another thread T1 in the same warp can successfully wait for the lock to become available without impeding the progress of thread T0. Note, however, that because active threads in a warp execute together, threads spinning on a lock may degrade the performance of the thread holding the lock.<br>Voltaçš„ç‹¬ç«‹çº¿ç¨‹è°ƒåº¦æœºåˆ¶ä½¿å¾—ï¼Œå³ä¾¿ä¸€ä¸ªthread T0æ­£æŒæœ‰node Açš„lockï¼ˆæ­£åœ¨ä½¿ç”¨node Açš„æ•°æ®ï¼Œå…¶ä»–çº¿ç¨‹ä¸å¯è®¿é—®ï¼Œéœ€ç­‰å¾…T0ä½¿ç”¨å®Œæ¯•ä¹‹åunlockï¼‰ï¼Œç›¸åŒwarpä¸­çš„å¦ä¸€ä¸ªçº¿ç¨‹T1ä¹Ÿå¯ä»¥åœ¨ä¸å¹²æ‰°çº¿ç¨‹T0çš„æƒ…å†µä¸‹ç­‰åˆ°lockè§£é”ã€‚<br>è§£é‡Šä¸€ä¸‹ï¼Œè¿™æ˜¯ç”±äºç‹¬ç«‹çº¿ç¨‹è°ƒåº¦å…è®¸åœ¨divergent branchesä¹‹é—´äº¤æ›¿æ‰§è¡Œï¼Œä½†ä¹Ÿæ˜¯å±è”½æ‰ä¸åœ¨è¿™æ¡pathä¸Šçš„çº¿ç¨‹ã€‚å› æ­¤æ­¤æ—¶T0æ­£åœ¨æŒæœ‰lockï¼Œè€ŒT1æ­£åœ¨spinning on the lockï¼Œç­‰å¾…ä½¿ç”¨çŠ¶æ€ã€‚æ­£æ˜¯ç”±äºè¿™æ ·çš„åŸå› ï¼Œå½“T1 spinningçš„æ—¶å€™ï¼ŒT0æ˜¯è¢«å±è”½äº†çš„ï¼Œè€ŒT0æ‰§è¡Œçš„æ—¶å€™ï¼ŒT1æ˜¯è¢«å±è”½çš„ã€‚å› æ­¤è¿™é‡Œè¯´spinning on a lockçš„çº¿ç¨‹ä¼šå½±å“æ­¤æ—¶holding the lockçš„çº¿ç¨‹çš„æ€§èƒ½ã€‚<br>è‹¥æ²¡æœ‰ç‹¬ç«‹çº¿ç¨‹è°ƒåº¦æœºåˆ¶ï¼Œè¿™ç§ç®—æ³•æ— æ³•å®ç°ï¼ŒåŸå› æ˜¯åŒä¸€warpä¸­çš„çº¿ç¨‹åªèƒ½æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œè¦ä¹ˆå°±æ˜¯è¢«å±è”½æ‰äº†ã€‚å½“ä¸¤çº¿ç¨‹åŒæ—¶access node Aï¼Œä¸€ä¸ªæˆåŠŸhold the lockï¼Œå¦ä¸€ä¸ªåªèƒ½spin on the lockã€‚è€Œç”±äºå‘ç”Ÿdivergentæ—¶ï¼Œä¸åŒåˆ†æ”¯æ˜¯äº¤æ›¿æ‰§è¡Œçš„ï¼Œæ— æ³•ç¡®ä¿å“ªä¸ªä¼˜å…ˆæ‰§è¡Œï¼Œæœ‰å¯èƒ½æ‰§è¡Œåˆ°spinningçŠ¶æ€ï¼Œè¿™æ—¶å¦ä¸€ä¸ªçº¿ç¨‹æ˜¯å¤„äºä¸€ç›´è¢«å±è”½çŠ¶æ€çš„ï¼Œæ²¡æœ‰æœºä¼šunlockï¼Œå¯¼è‡´ç¨‹åºæŒ‚èµ·ã€‚</p>
<p>It is also important to note that the use of a per-node lock in the above example is critical for performance on the GPU. Traditional doubly-linked list implementations may use a coarse-grained lock that provides exclusive access to the entire structure, rather than separately protecting individual nodes. This approach typically leads to poor performance in applications with many threadsâ€”Volta may have up to 163,840 concurrent threadsâ€”caused by extremely high contention for the lock. By using a fine-grained lock on each node, the average per-node contention in large lists will usually be low except under certain pathological node insertion patterns.</p>
<p>ä½¿ç”¨è¿™ç§per-node lockå¯¹GPUæ‰§è¡Œæ€§èƒ½æ”¶ç›Šå¾ˆå¤§ï¼ˆé™¤éæ˜¯é‚£ç§ç—…æ€è®¿é—®æƒ…å†µï¼šçº¿ç¨‹é—´è®¿é—®çš„èŠ‚ç‚¹éå¸¸é›†ä¸­ï¼‰ã€‚ä¼ ç»ŸåŒå‘é“¾è¡¨åªèƒ½ç”¨æ›´ç²—ç²’åº¦çš„lockï¼Œæä¾›å¯¹æ•´ä¸ªstructureçš„ç‹¬å è®¿é—®æƒï¼Œè€Œä¸æ˜¯å•ç‹¬æ£€æµ‹æ¯ä¸ªnodeã€‚å½“æœ‰æ•°é‡éå¸¸åºå¤§çš„çº¿ç¨‹æ—¶ï¼Œçº¿ç¨‹é—´ç«äº‰éå¸¸æ¿€çƒˆï¼Œå¹¶è¡Œå°±å®Œå…¨é€€åŒ–ä¸ºäº†ä¸²è¡Œã€‚</p>
<p><a target="_blank" rel="noopener" href="https://images.nvidia.cn/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Tesla V100 architecture white paper</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/16/ComSys-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/16/ComSys-1/" class="post-title-link" itemprop="url">è®¡ç®—æœºç³»ç»Ÿå­¦ä¹ éšè®° åŸå­æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-02-16 01:46:36 / Modified: 01:53:28" itemprop="dateCreated datePublished" datetime="2022-02-16T01:46:36+08:00">2022-02-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">è®¡ç®—æœºç³»ç»Ÿå­¦ä¹ éšè®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="åŸå­æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„"><a href="#åŸå­æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„" class="headerlink" title="åŸå­æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„"></a>åŸå­æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33445834">çŸ¥ä¹: åŸå­æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ</a></p>
<blockquote>
<p>X86æ¶æ„(The X86 architecture)</p>
</blockquote>
<p>X86æ¶æ„(The X86 architecture)æ˜¯å¾®å¤„ç†å™¨æ‰§è¡Œçš„è®¡ç®—æœºè¯­è¨€æŒ‡ä»¤é›†ï¼Œå¯¹åº”32ä½ã€‚è€Œå¹³æ—¶å¸¸è¯´çš„x64ï¼Œå…¨ç§°æ˜¯x86-64ï¼Œå¯¹åº”64ä½ï¼Œæ˜¯x86æŒ‡ä»¤é›†çš„64ä½æ‰©å±•è¶…é›†ï¼Œå…·å¤‡å‘ä¸‹å…¼å®¹çš„ç‰¹ç‚¹ã€‚</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/11/cuda-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/11/cuda-3/" class="post-title-link" itemprop="url">CUDAå­¦ä¹ éšè®°3 Memory Fenceä¸Synchronization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-11 17:13:37" itemprop="dateCreated datePublished" datetime="2022-02-11T17:13:37+08:00">2022-02-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-28 20:39:41" itemprop="dateModified" datetime="2022-06-28T20:39:41+08:00">2022-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDAå­¦ä¹ éšè®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Memory-Fenceä¸Synchronization"><a href="#Memory-Fenceä¸Synchronization" class="headerlink" title="Memory Fenceä¸Synchronization"></a>Memory Fenceä¸Synchronization</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></p>
<h2 id="Synchronization-Functions"><a href="#Synchronization-Functions" class="headerlink" title="Synchronization Functions"></a>Synchronization Functions</h2><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">B.6. Synchronization Functions</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure>
<p>waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to <strong>syncthreads() are visible to all threads in the block.<br>ç­‰å¾…blockä¸­æ‰€æœ‰çº¿ç¨‹åˆ°è¾¾ï¼ˆ<strong>æŒ‡ä»¤åŒæ­¥</strong>ï¼‰ï¼Œå¹¶ä½¿å¾—</strong>syncthreads()å‰æ‰€æœ‰çº¿ç¨‹çš„gmemå’Œsmemè®¿å­˜å¯¹blockå†…æ‰€æœ‰çº¿ç¨‹å¯è§ï¼ˆ<strong>å†…å­˜é¡ºåºï¼Œè®¿å­˜å¯è§æ€§</strong>ï¼‰ã€‚è¿™ä¸ªå¯è§åŒ…å«äº†å¦ä¸€å±‚å«ä¹‰ï¼Œå°±æ˜¯æ‰€æœ‰å†…å­˜å†™å…¥ï¼Œä¿è¯çœŸå®çš„å†™åˆ°äº†å¯¹åº”çš„ç‰©ç†å†…å­˜ä¸Šï¼Œè€Œä¸ä»…ä»…æ˜¯é©»ç•™åœ¨Cacheä¸­ã€‚</p>
<p>__syncthreads() is used to coordinate communication between the threads of the same block. When some threads within a block access the same addresses in shared or global memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses. These data hazards can be avoided by synchronizing threads in-between these accesses.</p>
<p><strong>syncthreads() is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects.<br>å¿…é¡»blockå†…çš„æ‰€æœ‰çº¿ç¨‹éƒ½èƒ½åŒæ—¶æ‰§è¡Œåˆ°</strong>syncthreads()å¦åˆ™ä¼šé€ æˆæ­»é”ã€‚</p>
<p>Devices of compute capability 2.x and higher support three variations of <strong>syncthreads() described below.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __syncthreads_count(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure><br>is identical to </strong>syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero.<br>é™¤äº†åŒæ­¥ä¹‹å¤–ï¼Œè¿˜ä¼šè¿”å›blockä¸­predicateä¸ºé0ï¼ˆtrueï¼‰çš„threadsçš„æ•°é‡ã€‚ï¼ˆä½œä¸ºå‡½æ•°çš„è¿”å›å€¼ï¼‰ç”¨äºåˆ¤æ–­blockä¸­æ»¡è¶³æ¡ä»¶çº¿ç¨‹çš„ä¸ªæ•°ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __syncthreads_and(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them.<br>é™¤äº†åŒæ­¥ï¼Œå½“ä¸”ä»…å½“blockä¸­æ‰€æœ‰threadsçš„predicateéƒ½ä¸ºé0ï¼Œå‡½æ•°è¿”å›ä¸€ä¸ªé0å€¼ã€‚ç”¨äºåˆ¤æ–­blockä¸­æ‰€æœ‰çº¿ç¨‹æ˜¯å¦éƒ½æ»¡è¶³æ¡ä»¶ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __syncthreads_or(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them.<br>é™¤äº†åŒæ­¥ï¼Œblockä¸­ä»»ä¸€threadçš„predicateé0ï¼Œé‚£ä¹ˆå‡½æ•°å°±è¿”å›ä¸€ä¸ªé0å€¼ã€‚ç”¨äºåˆ¤æ–­blockä¸­æ˜¯å¦å…·æœ‰æ»¡è¶³æ¡ä»¶çš„çº¿ç¨‹ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=<span class="number">0xffffffff</span>);</span><br></pre></td></tr></table></figure>
<p>will cause the executing thread to wait until all warp lanes named in mask have executed a <strong>syncwarp() (with the same mask) before resuming execution. All non-exited threads named in mask must execute a corresponding </strong>syncwarp() with the same mask, or the result is undefined.<br>ç”¨äºåŒæ­¥warpå†…éƒ¨æ‰€æœ‰è¢«maskæ‰€æŒ‡å®šçš„çº¿ç¨‹ï¼ŒPascalåŠä»¥ä¸‹æ¶æ„æ²¡ç”¨ï¼Œå› ä¸ºå…¶warpæ˜¯å®Œå…¨åŒæ­¥æ‰§è¡Œçš„ã€‚<br>æ‰€æœ‰maskæ ‡è®°çš„çº¿ç¨‹å¿…é¡»æ‰§è¡Œåˆ°ä¸€ä¸ªå…·æœ‰ç›¸åŒmaskçš„__syncwarp() ï¼Œå¦åˆ™ä¼šäº§ç”Ÿæœªå®šä¹‰ç»“æœã€‚</p>
<p>Executing <strong>syncwarp() guarantees memory ordering among threads participating in the barrier. Thus, threads within a warp that wish to communicate via memory can store to memory, execute </strong>syncwarp(), and then safely read values stored by other threads in the warp.<br>æ‰§è¡Œ__syncwarp()ä¼šä¿è¯æ‰€æœ‰è¢«maskæ ‡è®°çš„warpå†…çº¿ç¨‹çš„memory orderï¼Œçº¿ç¨‹é—´å¯å®‰å…¨çš„äº’ç›¸è®¿å­˜ã€‚ï¼ˆ<strong>å†…å­˜é¡ºåºï¼Œè®¿å­˜å¯è§æ€§</strong>ï¼‰</p>
<p>Note: For .target sm_6x or below, all threads in mask must execute the same <strong>syncwarp() in convergence, and the union of all values in mask must be equal to the active mask. Otherwise, the behavior is undefined.<br>sm_6xåŠä»¥ä¸‹çš„è®¾å¤‡ï¼ˆPascalæ¶æ„åŠä»¥ä¸‹ï¼‰ï¼Œmaskä¸­æŒ‡å®šçš„æ‰€æœ‰çº¿ç¨‹å¿…é¡»åŒæ­¥æ‰§è¡ŒåŒä¸€ä¸ª</strong>syncwarp()ï¼Œä¸”maskå¿…é¡»ä¸__active_mask()å‡½æ•°è¿”å›çš„å½“å‰active maskä¸€è‡´ã€‚å¦åˆ™ä¼šå¯¼è‡´undefined behaviorã€‚</p>
<h2 id="Memory-Fence-Functions"><a href="#Memory-Fence-Functions" class="headerlink" title="Memory Fence Functions"></a>Memory Fence Functions</h2><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-fence-functions">B.5. Memory Fence Functions</a></p>
<p>The CUDA programming model assumes a device with a weakly-ordered memory model, that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another CUDA or host thread. It is undefined behaviour for two threads read from or write to the same memory location without synchronization.<br>CUDAæœ¬èº«å…·æœ‰å¼±å†…å­˜é¡ºåºæ¨¡å‹weakly-ordered memory modelï¼Œå°±æ˜¯è¯´ä¸€ä¸ªCUDAçº¿ç¨‹çš„è®¿å­˜é¡ºåºï¼Œä¸å¦ä¸€ä¸ªçº¿ç¨‹æˆ–è€…ä¸»æœºçº¿ç¨‹çœ‹åˆ°çš„è®¿å­˜é¡ºåºæœªå¿…ä¸€è‡´ã€‚å¦‚æœåœ¨ä¸è¿›è¡ŒåŒæ­¥çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸¤ä¸ªçº¿ç¨‹å¯¹åŒä¸€ä¸ªå†…å­˜åœ°å€è¿›è¡Œæ“ä½œï¼Œç»“æœå°†æ˜¯æœªå®šä¹‰çš„ã€‚</p>
<p>In the following example, thread 1 executes writeXY(), while thread 2 executes readXY().</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> X = <span class="number">1</span>, Y = <span class="number">2</span>;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">writeXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X = <span class="number">10</span>;</span><br><span class="line">    Y = <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">readXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> B = Y;</span><br><span class="line">    <span class="type">int</span> A = X;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The two threads read and write from the same memory locations X and Y simultaneously. Any data-race is undefined behaviour, and has no defined semantics. The resulting values for A and B can be anything.<br>ä¸¤ä¸ªçº¿ç¨‹ä¸€ä¸ªå†™ä¸€ä¸ªè¯»ï¼Œå¾—åˆ°çš„ç»“æœå¯ä»¥æ˜¯ä»»æ„æƒ…å†µã€‚è™½ç„¶åœ¨writeXYä»£ç ä¸­çœ‹ä¼¼æ˜¯å…ˆæ‰§è¡ŒX = 10ï¼Œå†æ‰§è¡ŒY = 20ã€‚</p>
<ol>
<li>ç»è¿‡ç¼–è¯‘å™¨çš„ä¼˜åŒ–ä¹‹åï¼Œå¯¹æœ¬çº¿ç¨‹æ¥è¯´ï¼Œè¿™ä¸ªé¡ºåºä¸è¢«ä¿è¯ã€‚</li>
<li>å¦ä¸€ä¸ªçº¿ç¨‹çœ‹åˆ°çš„é¡ºåºå¯èƒ½ä¸æœ¬çº¿ç¨‹çš„æ‰§è¡Œé¡ºåºä¸åŒã€‚</li>
<li>è€ƒè™‘ä¸¤ä¸ªçº¿ç¨‹æ‰§è¡Œçš„å…ˆåé¡ºåºï¼Œæ—¶é—´å·®å¼‚ç­‰æƒ…å†µã€‚</li>
</ol>
<p>å°±é€ æˆäº†å¤šç§å¤šæ ·çš„ç»“æœã€‚</p>
<p>Memory fence functions can be used to enforce some ordering on memory accesses. The memory fence functions differ in the scope in which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device).<br>Memory fence functionsæ­¤æ—¶å°±æ˜¯ç”¨æ¥å¼ºåˆ¶è§„å®šè®¿å­˜é¡ºåºçš„ã€‚Memory fence functionsæ ¹æ®é™åˆ¶è®¿å­˜é¡ºåºçš„èŒƒå›´ä¸åŒè€Œæœ‰æ‰€ä¸åŒï¼Œä½†ä¸è®¿å­˜çš„å†…å­˜ç±»å‹æ— å…³ï¼Œå¯¹æ‰€æœ‰å†…å­˜å‡èµ·åˆ°ç›¸åŒçš„ä½œç”¨ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_block();</span><br></pre></td></tr></table></figure>
<p>ensures that:<br>All writes to all memory made by the calling thread before the call to <strong>threadfence_block() are observed by <strong>all threads in the block of the calling thread</strong> as occurring before all writes to all memory made by the calling thread after the call to </strong>threadfence_block();<br>åœ¨æ‰€æœ‰blockä¸­çš„çº¿ç¨‹çœ‹æ¥ï¼ŒèŒƒå›´æ˜¯<strong>block</strong><br>è°ƒç”¨çº¿ç¨‹çš„åœ¨__threadfence_block()å‰åçš„å†™å…¥ä¿æŒåŸæœ‰é¡ºåºã€‚<br>ä»…ä»…æ˜¯ä¿è¯äº†åŒblockå†…å…¶ä»–çº¿ç¨‹è§‚å¯Ÿåˆ°çš„é¡ºåºã€‚</p>
<p>All reads from all memory made by the calling thread before the call to <strong>threadfence_block() are ordered before all reads from all memory made by the calling thread after the call to </strong>threadfence_block().<br>ä¿è¯è°ƒç”¨çº¿ç¨‹__threadfence_block()å‰åçš„å†…å­˜è¯»å–é¡ºåºã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence();</span><br></pre></td></tr></table></figure>
<p>acts as <strong>threadfence_block() for all threads in the block of the calling thread and also ensures that no writes to all memory made by the calling thread after the call to </strong>threadfence() are observed by <strong>any thread in the device</strong> as occurring before any write to all memory made by the calling thread before the call to <strong>threadfence().<br>åŠŸèƒ½ä¸</strong>threadfence_block()ä¸€è‡´ï¼ŒèŒƒå›´æ˜¯<strong>æ•´ä¸ªDevice</strong>ã€‚</p>
<p>Note that for this ordering guarantee to be true, the observing threads must truly observe the memory and not cached versions of it; this is ensured by using the volatile keyword as detailed in Volatile Qualifier.<br>ä¸ºäº†ä¿è¯è¿™ä¸ªè§‚å¯Ÿåˆ°çš„å†…å­˜é¡ºåºæ˜¯çœŸå®çš„ï¼Œè§‚å¯Ÿçš„çº¿ç¨‹å¿…é¡»è§‚å¯Ÿåˆ°å¯¹åº”çš„memoryè€Œä¸æ˜¯è¿™ä¸ªmemoryçš„cacheã€‚è€Œå®ç°è¿™ä¸ªæ¡ä»¶çš„æ–¹æ³•æ˜¯æ·»åŠ volatileå…³é”®å­—ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_system();</span><br></pre></td></tr></table></figure>
<p>acts as <strong>threadfence_block() for all threads in the block of the calling thread and also ensures that all writes to all memory made by the calling thread before the call to </strong>threadfence_system() are observed by <strong>all threads in the device, host threads, and all threads in peer devices</strong> as occurring before all writes to all memory made by the calling thread after the call to __threadfence_system().<br>èŒƒå›´æ˜¯<strong>æ•´ä¸ªDeviceï¼ŒHostå’ŒPeer Device</strong>ï¼ˆå¤šå¡ååŒï¼‰ã€‚</p>
<p>__threadfence_system() is only supported by devices of compute capability 2.x and higher.</p>
<p>In the previous code sample, we can insert fences in the codes as follows:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> X = <span class="number">1</span>, Y = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">writeXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X = <span class="number">10</span>;</span><br><span class="line">    __threadfence();</span><br><span class="line">    Y = <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">readXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> B = Y;</span><br><span class="line">    __threadfence();</span><br><span class="line">    <span class="type">int</span> A = X;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>é€šè¿‡æ’å…¥__threadfence()ï¼Œä½¿å¾—blockå†…æ‰€æœ‰çº¿ç¨‹è§‚å¯Ÿåˆ°çš„XYå†™å…¥é¡ºåºä¸å˜ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰ã€‚è§‚å¯Ÿçº¿ç¨‹è¯»å–YXçš„é¡ºåºä¹Ÿä¸ä¾¿ï¼Œå…ˆè¯»Yå†è¯»Xã€‚æ‰€ä»¥å¯èƒ½çš„æƒ…å†µå°±åªå‰©ä¸‹ä¸¤ä¸ªçº¿ç¨‹æŒ‡ä»¤æ‰§è¡Œçš„å…ˆåé¡ºåºä¸åŒå¯¼è‡´çš„ç»“æœå·®å¼‚ã€‚</p>
<p>For this code, the following outcomes can be observed:<br>A equal to 1 and B equal to 2, å…ˆè¯»äº†ï¼Œè¿˜æ²¡å†™<br>A equal to 10 and B equal to 2, Bè¯»çš„æ—¶å€™Yè¿˜æ²¡å†™ï¼ŒAè¯»çš„æ—¶å€™Xå†™å®Œäº†<br>A equal to 10 and B equal to 20. è¯»çš„æ—¶å€™éƒ½å†™å®Œäº†</p>
<p>The fourth outcome is not possible, because the frist write must be visible before the second write. If thread 1 and 2 belong to the same block, it is enough to use <strong>threadfence_block(). If thread 1 and 2 do not belong to the same block, </strong>threadfence() must be used if they are CUDA threads from the same device and __threadfence_system() must be used if they are CUDA threads from two different devices.</p>
<p>A common use case is when <strong>threads consume some data produced by other threads</strong> as illustrated by the following code sample of a kernel that computes the sum of an array of N numbers in one call. Each block first sums a subset of the array and stores the result in global memory. When all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result. In order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (see Atomic Functions about atomic functions). The last block is the one that receives the counter value equal to gridDim.x-1. <strong>If no fence is placed between storing the partial sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reach gridDim.x-1 and let the last block start reading partial sums before they have been actually updated in memory.</strong></p>
<p>Memory fence functions only affect the ordering of memory operations by a thread; they do not ensure that these memory operations are visible to other threads (like <strong>syncthreads() does for threads within a block (see Synchronization Functions)). In the code sample below, <strong>the visibility of memory operations on the result variable is ensured by declaring it as volatile</strong> (see Volatile Qualifier).<br><strong>Memory fenceä»…ä¿è¯å†…å­˜é¡ºåºï¼Œä¸ä¿è¯å¯è§æ€§ï¼Œå°±æ˜¯è¯´ä¸ä¿è¯ä¸€å®šå†™å…¥åˆ°äº†çœŸå®çš„å†…å­˜è€Œä¸æ˜¯å†…å­˜çš„Cacheï¼Œè¿˜ä¸ä¿è¯åœ¨è§‚å¯Ÿæ—¶åˆ»å·²ç»å†™å…¥äº†ï¼ˆæ²¡æœ‰æŒ‡ä»¤åŒæ­¥ï¼‰ã€‚è¿™ä¸ªéœ€è¦å°†å˜é‡å£°æ˜ä¸ºvolatileæ¥ä¿è¯ã€‚</strong> è¿™ç‚¹ä¸</strong>syncthreads()å¯¹blockçš„åŒæ­¥ä¸åŒï¼Œåè€…ä¿è¯æ‰€æœ‰å¯¹gmemå’Œsmemçš„è¯»å†™å¯¹blockå†…æ‰€æœ‰çº¿ç¨‹å¯è§ï¼ˆåªè¦ä½ è§‚å¯Ÿï¼Œä¸€å®šèƒ½çœ‹åˆ°ç»“æœï¼‰ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">__shared__ <span class="type">bool</span> isLastBlockDone;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sum</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* array, <span class="type">unsigned</span> <span class="type">int</span> N,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="keyword">volatile</span> <span class="type">float</span>* result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Each block sums a subset of the input array.</span></span><br><span class="line">    <span class="type">float</span> partialSum = <span class="built_in">calculatePartialSum</span>(array, N);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 of each block stores the partial sum</span></span><br><span class="line">        <span class="comment">// to global memory. The compiler will use </span></span><br><span class="line">        <span class="comment">// a store operation that bypasses the L1 cache</span></span><br><span class="line">        <span class="comment">// since the &quot;result&quot; variable is declared as</span></span><br><span class="line">        <span class="comment">// volatile. This ensures that the threads of</span></span><br><span class="line">        <span class="comment">// the last block will read the correct partial</span></span><br><span class="line">        <span class="comment">// sums computed by all other blocks.</span></span><br><span class="line">        result[blockIdx.x] = partialSum;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 makes sure that the incrementation</span></span><br><span class="line">        <span class="comment">// of the &quot;count&quot; variable is only performed after</span></span><br><span class="line">        <span class="comment">// the partial sum has been written to global memory.</span></span><br><span class="line">        __threadfence();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 signals that it is done.</span></span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> value = <span class="built_in">atomicInc</span>(&amp;count, gridDim.x);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 determines if its block is the last</span></span><br><span class="line">        <span class="comment">// block to be done.</span></span><br><span class="line">        isLastBlockDone = (value == (gridDim.x - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Synchronize to make sure that each thread reads</span></span><br><span class="line">    <span class="comment">// the correct value of isLastBlockDone.</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isLastBlockDone) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The last block sums the partial sums</span></span><br><span class="line">        <span class="comment">// stored in result[0 .. gridDim.x-1]</span></span><br><span class="line">        <span class="type">float</span> totalSum = <span class="built_in">calculateTotalSum</span>(result);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Thread 0 of last block stores the total sum</span></span><br><span class="line">            <span class="comment">// to global memory and resets the count</span></span><br><span class="line">            <span class="comment">// varialble, so that the next kernel call</span></span><br><span class="line">            <span class="comment">// works properly.</span></span><br><span class="line">            result[<span class="number">0</span>] = totalSum;</span><br><span class="line">            count = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/10/cuda-2/" class="post-title-link" itemprop="url">CUDAå­¦ä¹ éšè®°2 å…³äºWarp Shuffle Functions *_sync é¦–ä¸ªå‚æ•°maskçš„ä½œç”¨</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 22:22:15" itemprop="dateCreated datePublished" datetime="2022-02-10T22:22:15+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-22 21:52:52" itemprop="dateModified" datetime="2022-02-22T21:52:52+08:00">2022-02-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDAå­¦ä¹ éšè®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="å…³äºWarp-Shuffle-Functions-sync-é¦–ä¸ªå‚æ•°maskçš„ä½œç”¨"><a href="#å…³äºWarp-Shuffle-Functions-sync-é¦–ä¸ªå‚æ•°maskçš„ä½œç”¨" class="headerlink" title="å…³äºWarp Shuffle Functions *_sync é¦–ä¸ªå‚æ•°maskçš„ä½œç”¨"></a>å…³äºWarp Shuffle Functions *_sync é¦–ä¸ªå‚æ•°maskçš„ä½œç”¨</h1><p>é¦–å…ˆæ¥çœ‹CUDA 9.0ä¹‹å‰çš„Warp Shuffle Functionså½¢å¼ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl(T var,<span class="type">int</span> srcLane,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_up(T var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_down(T var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_xor(T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize)</span><br></pre></td></tr></table></figure><br>å†æ¥çœ‹ä»CUDA 9.0å¼€å§‹ï¼Œå¼•å…¥çš„æ”¹è¿›çš„Warp Shuffle Functionså½¢å¼ã€‚<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a><br>Deprecation Notice: <strong>shfl, </strong>shfl_up, <strong>shfl_down, and </strong>shfl_xor have been deprecated as of CUDA 9.0.<br>C++å®˜æ–¹æ‰‹å†Œä¸­è¯´ï¼Œä»CUDA 9.0å¼€å§‹ï¼ŒåŸæœ‰Warp Shuffle Functionsè¢«åºŸå¼ƒï¼Œå…¨éƒ¨æ”¹ç”¨ä»¥ä¸‹æ–°å½¢å¼ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> srcLane, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_up_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_down_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_xor_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize);</span><br></pre></td></tr></table></figure><br>æ–°å½¢å¼ä¸åŸæœ‰Warp Shuffle FunctionsåŠŸèƒ½å®Œå…¨ä¸€è‡´ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä¸åŸæœ‰Warp Shuffle Functionsæœ€å¤§çš„åŒºåˆ«æ˜¯æ–°å½¢å¼å¼•å…¥äº†ä¸€ä¸ªå˜é‡maskã€‚maskä¸ºä¸€ä¸ª32 bitå˜é‡ï¼Œå¯¹åº”ä¸€ä¸ªwarpä¸­çš„32ä¸ªthread (lane)ï¼ŒæŒ‡æ˜å‚ä¸è°ƒç”¨çš„çº¿ç¨‹ï¼Œä¹Ÿå°±æ˜¯å‚ä¸åˆ°Warp Shuffle Functionsçš„çº¿ç¨‹é›†åˆã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¯ä»¥çœ‹åˆ°æ¯ä¸ªå‡½æ•°åé¢éƒ½é¢å¤–å¢åŠ äº†ä¸€ä¸ªè¡¨æ˜åŒæ­¥çš„åç¼€_syncã€‚<strong>åœ¨Pascalä»¥åŠä¹‹å‰çš„æ¶æ„ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“ä¸€ä¸ªwarpä¸­çš„32ä¸ªçº¿ç¨‹æ˜¯å®Œå…¨åŒæ­¥æ‰§è¡Œçš„ã€‚</strong> warp scheduleræ¯æ¬¡åˆ†åˆ«å‘å°„ä¸€æ¡ç›¸åŒçš„æŒ‡ä»¤ç»™ä¸€ä¸ªwarpä¸­çš„32ä¸ªçº¿ç¨‹ï¼Œé™¤éé‡åˆ°warp divergenceæƒ…å†µï¼Œä¸åœ¨å½“å‰æ§åˆ¶æµpathä¸Šçš„çº¿ç¨‹ä¼šè¢«å±è”½ã€‚<br>ç„¶è€Œï¼Œ<strong>ä»Pascalçš„ä¸‹ä¸€ä»£æ¶æ„â€”â€”Voltaæ¶æ„å¼€å§‹ï¼Œwarp scheduleræ”¯æŒIndependent Thread Schedulingï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒåŒä¸€ä¸ªwarpä¸­çš„32ä¸ªthreadsä¸å†ä¿è¯å®Œå…¨åŒæ­¥ã€‚</strong> å› æ­¤ï¼Œä¸ºäº†ä¿è¯Warp Shuffle FunctionsååŒæ“ä½œçš„æ­£ç¡®æ€§ï¼Œæ‰€æœ‰å‚ä¸çš„çº¿ç¨‹å¿…é¡»é¦–å…ˆè¿›è¡ŒåŒæ­¥ï¼Œä»è€Œä¿è¯ç»“æœçš„æ­£ç¡®æ€§ã€‚æ¯”å¦‚reductionæ“ä½œçš„__shfl_down_syncï¼Œ32ä¸ªlaneä¸­çš„å‰16ä¸ªå…ˆè¯»å–å16ä¸ªçš„å¯„å­˜å™¨ï¼Œå¿…é¡»ç­‰å¾…æ“ä½œå®Œæˆä¹‹åï¼Œå‰8ä¸ªæ‰èƒ½å†è¯»å–lane IDä¸º8-15çš„laneçš„å¯„å­˜å™¨ï¼Œä»¥æ­¤ç±»æ¨ã€‚æ¯æ¬¡è¯»å–ä¹‹å‰éœ€è¦ä¸€ä¸ªwarpçº§åˆ«çš„åŒæ­¥ï¼Œæ¥ä¿è¯å¯¹åº”çš„laneä¸­å¯„å­˜å™¨çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ã€‚<strong>å› æ­¤è¿™äº›å¸¦æœ‰_syncåç¼€çš„å‡½æ•°é¦–å…ˆä¼šéšå¼åŒæ­¥æ‰€æœ‰å‚ä¸åˆ°å‡½æ•°è°ƒç”¨çš„çº¿ç¨‹ï¼Œè€Œå“ªäº›çº¿ç¨‹å°†å‚ä¸åˆ°å‡½æ•°è°ƒç”¨ä¸­ï¼Œç”±maskçš„å¯¹åº”bitä½æ¥æŒ‡å®šï¼Œ0ä»£è¡¨ä¸å‚ä¸ï¼Œ1ä»£è¡¨å‚ä¸ã€‚</strong><br>The new *_sync shfl intrinsics take in a mask indicating the threads participating in the call. A bit, representing the threadâ€™s lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. All non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.<br>é‚£ä¹ˆï¼Œæœªè¢«maskæŒ‡å®šçš„çº¿ç¨‹å‘¢ï¼Ÿè¿™é‡Œç®€å•åšä¸€ä¸ªæµ‹è¯•ã€‚å†™ä¸€ä¸ªkernelï¼Œå¹¶ä»¥&lt;&lt;<1,32>&gt;&gt;å¯åŠ¨ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">16</span>);</span><br></pre></td></tr></table></figure><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0x0000000f</span>, val, <span class="number">16</span>);</span><br></pre></td></tr></table></figure><br>è§‚å¯Ÿæ‰§è¡Œåæ¯ä¸ªçº¿ç¨‹ä¸­å˜é‡shfl_valçš„å€¼ï¼Œå¯ä»¥çœ‹åˆ°ä¼ å…¥ä¸¤ä¸ªä¸åŒçš„maskï¼Œå¾—åˆ°çš„ç»“æœç›¸åŒï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚</1,32></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Thread ID</th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">â€¦</th>
<th style="text-align:center">15</th>
<th style="text-align:center">16</th>
<th style="text-align:center">â€¦</th>
<th style="text-align:center">30</th>
<th style="text-align:center">31</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>shfl_val</strong></td>
<td style="text-align:center">16</td>
<td style="text-align:center">17</td>
<td style="text-align:center">â€¦</td>
<td style="text-align:center">31</td>
<td style="text-align:center">16</td>
<td style="text-align:center">â€¦</td>
<td style="text-align:center">30</td>
<td style="text-align:center">31</td>
</tr>
</tbody>
</table>
</div>
<p>å°†å‚æ•°deltaç”±16æ”¹ä¸º8ï¼Œå³ä¸ºæ¯ä¸ªthreadè¯»å½“å‰lane ID + 8çš„threadä¸­çš„å¯„å­˜å™¨ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">8</span>);</span><br></pre></td></tr></table></figure></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Thread ID</th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">â€¦</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th style="text-align:center">â€¦</th>
<th style="text-align:center">14</th>
<th style="text-align:center">15</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>shfl_val</strong></td>
<td style="text-align:center">8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">â€¦</td>
<td style="text-align:center">15</td>
<td style="text-align:center">16</td>
<td style="text-align:center">â€¦</td>
<td style="text-align:center">22</td>
<td style="text-align:center">23</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Thread ID</th>
<th style="text-align:center">16</th>
<th style="text-align:center">17</th>
<th style="text-align:center">â€¦</th>
<th style="text-align:center">23</th>
<th style="text-align:center">24</th>
<th style="text-align:center">â€¦</th>
<th style="text-align:center">30</th>
<th style="text-align:center">31</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>shfl_val</strong></td>
<td style="text-align:center">24</td>
<td style="text-align:center">25</td>
<td style="text-align:center">â€¦</td>
<td style="text-align:center">31</td>
<td style="text-align:center">24</td>
<td style="text-align:center">â€¦</td>
<td style="text-align:center">30</td>
<td style="text-align:center">31</td>
</tr>
</tbody>
</table>
</div>
<p>è¿™ä¹Ÿå°±è¯æ˜äº†:</p>
<ol>
<li>__shfl_down_sync()ï¼Œä¸ºlane IDå°çš„threadè¯»å–å¤§çš„threadçš„å¯„å­˜å™¨ã€‚è¶Šç•Œçš„çº¿ç¨‹ï¼Œå³lane ID + 8 &gt; 31çš„çº¿ç¨‹è¿”å›è‡ªèº«å¯„å­˜å™¨ä¸­çš„å€¼ã€‚</li>
<li>maskæœªæŒ‡å®šçš„threadä¹Ÿä¼šè°ƒç”¨warp shuffle functionsï¼Œè€Œä¸æ˜¯ä¸æ‰§è¡Œï¼Œä½†è¿™éƒ¨åˆ†çº¿ç¨‹ä¸ä¼šè¢«å‡½æ•°éšå¼åŒæ­¥ï¼Œæ‰€å¾—åˆ°çš„ç»“æœæ˜¯undefinedã€‚</li>
</ol>
<p>ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºPascalåŠä¹‹å‰çš„æ¶æ„æ¥è¯´ï¼Œå¸¦æœ‰_syncåç¼€çš„Warp Shuffle Functionsä¸­çš„å‚æ•°maskæ²¡æœ‰ä½œç”¨ã€‚è€Œå¯¹äºä»Voltaå¼€å§‹çš„æ¶æ„æ¥è¯´ï¼Œmaskå°†æŒ‡å®šå‚ä¸Warp Shuffle Functionsçš„çº¿ç¨‹ï¼Œä»è€Œå‘ŠçŸ¥ç¼–è¯‘å™¨ï¼Œæ¥å¯¹è¿™éƒ¨åˆ†çº¿ç¨‹åœ¨æ‰§è¡Œæ—¶è¿›è¡Œå¿…è¦çš„åŒæ­¥ï¼Œæ¥ä¿è¯ç»“æœçš„æ­£ç¡®æ€§ã€‚</p>
<p>å‚è€ƒï¼š<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/what-does-mask-mean-in-warp-shuffle-functions-shfl-sync/67697">What does mask mean in warp shuffle functions (__shfl_sync)</a></p>
<p>å¼•ç”¨å…¶ä¸­ä¸€æ®µå›ç­”ï¼š</p>
<blockquote>
<p>For correctness, you must specify a mask parameter which includes the warp lanes you expect to participate. The behavior of lanes outside the mask is undefined (because, in fact, Volta provides no guarantees of warp convergence, except those the programmer specifically asks for, and therefore a warp lane with a zero bit in the mask implies that the specified lane may or may not participate.)</p>
<p>The mask parameter says the following:</p>
<p>â€œThese are the warp lanes that must participate for correctness.â€</p>
<p>The compiler will generate the necessary instructions to reconverge those threads if they are not already converged.</p>
<p>Thereafter the warp shuffle proceeds for the current state of the warp.</p>
<p>There is no other implied behavior. Regardless of the mask, after the reconvergence step, the result of the warp shuffle operation will be the result you would get for whichever threads happen to be participating. A zero bit in the mask argument does not prevent a warp lane from participating, it merely does not guarantee that such a lane will participate if the warp is in a diverged state.</p>
</blockquote>
<p>maskä¸­çš„0 bitä½ä¸ä¼šé˜»æ­¢ä¸€ä¸ªwarp laneå‚ä¸åˆ°shuffle functionçš„æ‰§è¡Œä¸­ï¼Œè€Œä»…ä»…æ˜¯ä¸ä¿è¯è¿™æ ·ä¸€ä¸ªlaneåœ¨warpå†…å…·æœ‰åˆ†æ­§æ—¶ä¼šå‚ä¸åˆ°shuffle functionçš„æ‰§è¡Œä¸­ã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯ä¸ä¼šåœ¨æ‰§è¡Œå‰åŒæ­¥è¿™ä¸ªlaneï¼Œå¦‚æœå®ƒæ°å¥½æ‰§è¡Œåˆ°è¿™é‡Œï¼Œé‚£ä¹ˆå°±ä¸€èµ·æ‰§è¡Œï¼Œå¦‚æœä¸åœ¨ä¸ç®¡å®ƒã€‚</p>
<blockquote>
<p>There is an important caveat here. The mask parameter will create a reconvergence of the indicated threads. However it cannot cause reconvergence of threads that your code has made impossible.</p>
<p>For example, this is illegal (will result in undefined behavior for warp 0):</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">if</span> (threadIdx.x &gt; <span class="number">3</span>)</span><br><span class="line">   __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, v, offset, <span class="number">8</span>);</span><br></pre></td></tr></table></figure>
<p>æœ‰ä¸€ä¸ªé‡è¦çš„è­¦å‘Šã€‚maskå‚æ•°ä¼šåœ¨shuffle functionä¸­åŒæ­¥æŒ‡å®šthreadsï¼Œç„¶è€Œï¼Œå¹¶ä¸åŒ…æ‹¬ä»£ç ä¸­è§„å®šçš„å¹¶ä¸èƒ½åŒæ­¥çš„çº¿ç¨‹ï¼ˆæ¡ä»¶è¯­å¥ç­‰é€ æˆçš„condition branchesï¼Œæ§åˆ¶æµåˆ†å‰warp divergenceï¼‰ã€‚å¦‚ä¸Šæ‰€ç¤ºä»£ç ï¼Œå¯¹äºwarp 0æ¥è¯´ï¼ŒmaskæŒ‡å®š32ä¸ªwarp laneéƒ½è¦æ‰§è¡Œ<strong>shfl_down_syncï¼Œå› æ­¤è¦å¯¹warp 0ä¸­çš„32ä¸ªçº¿ç¨‹è¿›è¡ŒåŒæ­¥ã€‚ä½†ifè¯­å¥è§„å®šåªæœ‰å28ä¸ªçº¿ç¨‹ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè€Œå‰å››ä¸ªçº¿ç¨‹ä¸ä¼šæ‰§è¡Œï¼ŒäºŒè€…å†²çªï¼Œä¼šå¯¼è‡´undefined behaviorã€‚<br>æ­£ç¡®å†™æ³•ï¼šä½¿ç”¨</strong>ballot_syncé¢„å…ˆå¾—åˆ°æ‰€æœ‰å‚ä¸__shfl_down_sync()çš„çº¿ç¨‹æ‰€å¯¹åº”çš„maskã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(<span class="number">0xFFFFFFFF</span>, threadIdx.x &gt; <span class="number">3</span>);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &gt; <span class="number">3</span>)</span><br><span class="line">    __shfl_down_sync(mask, v, offset, <span class="number">8</span>);</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>é¢å¤–è¿™é‡Œè®°å½•ä¸€ä¸ªæˆ‘è‡ªå·±çœ‹åˆ°çš„å¥½ç©å„¿çš„é—®é¢˜ã€‚<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">developer.nvidia.com/blog/: Using CUDA Warp-Level Primitives</a>ä¸­ï¼Œåœ¨Listing 4ï¼Œä½œè€…å†™äº†è¿™æ ·ä¸€æ®µä»£ç ï¼š<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    â€¦</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    â€¦</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>é…æ–‡ï¼š</p>
<blockquote>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others. Listing 4 shows an example where <strong>all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.</strong><br>On the latest Volta (and future) GPUs, you can run library functions that use warp synchronous primitives without worrying whether the function is called in a thread-divergent branch.</p>
</blockquote>
<p>æ¯«æ— ç–‘é—®ï¼Œè¿™ä¸æˆ‘ä¸Šé¢æ‰€è¯´çš„è­¦å‘Šæœ‰æ‰€å†²çªï¼ŒåŸå› åœ¨äºè¿™é‡ŒFULL_MASKä¸º0xFFFFFFFFï¼Œä½†å¯¹äºå…¶ä¸­ä¸€ä¸ªifåˆ†æ”¯ï¼Œä»…æœ‰odd/evençº¿ç¨‹ä¼šè®¿é—®åˆ°è¿™é‡Œã€‚ä¹Ÿå°±æ˜¯è¯´ä¼šè¿™ä¼šå¯¼è‡´ä¸Šæ–‡ä¸­æåˆ°çš„undefined behavioré—®é¢˜ã€‚å…·ä½“è¿™ä¸ªundefined behavioræ˜¯ä»€ä¹ˆï¼Œåœ¨<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/using-cuda-warp-level-primitives/148673/16">forums.developer.nvidia.com: Using CUDA Warp-Level Primitives</a>ä¸­ï¼Œæœ‰äººæåˆ°ï¼š</p>
<blockquote>
<p>In â€œUpdate Legacy Warp-Level Programmingâ€, it says â€œDonâ€™t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.â€</p>
<p>In listing 4, regardless the thread id is even or odd, the thread in the warp will always execute one of the two __shfl_sync() statements. Therefore, FULL_MASK should be used.</p>
<p>The following code may cause a stall.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">  val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">  â€¦</span><br><span class="line">&gt;&#125;</span><br><span class="line">&gt;<span class="keyword">else</span> &#123;</span><br><span class="line">  â€¦</span><br><span class="line">&gt;&#125;</span><br></pre></td></tr></table></figure><br>å› æ­¤æˆ‘çŒœè¿™é‡Œæ‰€è¯´çš„undefined behavioråº”è¯¥å°±æ˜¯æ­»é”ï¼Œç”±äºFULL_MASKè¦æ±‚åŒæ­¥warpä¸­çš„32ä¸ªlaneï¼Œè€Œæ§åˆ¶æµå¯¼è‡´æœ‰çš„laneæ°¸è¿œä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œï¼Œå› æ­¤é€ æˆæ­»é”ã€‚è¿™ä¸åœ¨ifæ¡ä»¶è¯­å¥ä¸­ä½¿ç”¨blockå†…åŒæ­¥å‡½æ•°<strong>syncthreads()æ‰€å¯¼è‡´çš„æ­»é”åŸç†ç›¸ä¼¼ã€‚<br>Listing 4ä¸­çš„ä»£ç åˆæ³•çš„åŸå› æ˜¯ï¼Œä¸ç®¡æ˜¯oddè¿˜æ˜¯evençº¿ç¨‹ï¼Œéƒ½ä¼šæ‰§è¡Œåˆ°ä¸€æ¡</strong>shfl_sync(FULL_MASK, val, 0)æŒ‡ä»¤ï¼Œè¿™ä¸¤æ¡æŒ‡ä»¤çš„maskéƒ½æ˜¯FULL_MASKï¼Œå› æ­¤å¯¹è¿™32ä¸ªwarp laneåŒæ­¥å¹¶ä¸ä¼šå¯¼è‡´æ­»é”ã€‚è¿™ä¸ªé“ç†ç±»ä¼¼__syncwarp()ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>ç”¨äºåŒæ­¥warpä¸­çš„çº¿ç¨‹ï¼ŒåŒæ­¥å“ªäº›ç”±maskæŒ‡å®šã€‚</p>
</blockquote>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)** before resuming execution. It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</p>
<p>åªè¦maskæŒ‡å®šçš„çº¿ç¨‹æ‰§è¡Œåˆ°äº†å…·æœ‰ç›¸åŒmaskçš„<strong>syncwarp()å³å¯ç»§ç»­æ‰§è¡Œï¼Œå¹¶éæ˜¯ä¸€å®šä½äºåŒä¸€ä¸ªæ§åˆ¶æµåˆ†æ”¯çš„</strong>syncwarp()ã€‚ä½†è¿™ä»…é€‚ç”¨äºsm_6xï¼ˆä¸åŒ…å«ï¼‰ä»¥ä¸Šçš„è®¾å¤‡ï¼Œä¹Ÿå°±æ˜¯Pascalæ¶æ„ï¼ˆsm_60ï¼‰ä¹‹åã€‚</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a>ä¸­æœ‰ä¸€æ®µè­¦å‘Šï¼š</p>
<blockquote>
<p>For .target sm_6x or below, all threads in mask must execute the same __syncwarp() in convergence, and the union of all values in mask must be equal to the active mask. Otherwise, the behavior is undefined.</p>
</blockquote>
<p>sm_6xåŠä»¥ä¸‹çš„è®¾å¤‡ï¼Œmaskä¸­æŒ‡å®šçš„æ‰€æœ‰çº¿ç¨‹å¿…é¡»åŒæ­¥æ‰§è¡ŒåŒä¸€ä¸ª<strong>syncwarp()ï¼Œä¸”maskå¿…é¡»ä¸</strong>active_mask()å‡½æ•°è¿”å›çš„å½“å‰active maskä¸€è‡´ã€‚å¦åˆ™ä¼šå¯¼è‡´undefined behaviorã€‚</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/10/cuda-1/" class="post-title-link" itemprop="url">CUDAå­¦ä¹ éšè®°1 Using CUDA Warp-Level Primitives</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 13:55:56" itemprop="dateCreated datePublished" datetime="2022-02-10T13:55:56+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-28 15:37:19" itemprop="dateModified" datetime="2022-06-28T15:37:19+08:00">2022-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDAå­¦ä¹ éšè®°</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDAå­¦ä¹ éšè®°-Using-CUDA-Warp-Level-Primitives"><a href="#CUDAå­¦ä¹ éšè®°-Using-CUDA-Warp-Level-Primitives" class="headerlink" title="CUDAå­¦ä¹ éšè®°: Using CUDA Warp-Level Primitives"></a>CUDAå­¦ä¹ éšè®°: Using CUDA Warp-Level Primitives</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">3. CUDA C++ Programming Guide</a></p>
<p>primitives: åŸå‡½æ•°ã€åŸºå…ƒåŠŸèƒ½</p>
<p><strong>SIMD: Single Instruction, Multiple Data</strong><br>In a SIMD architecture, each instruction applies the same operation in parallel across many data elements.<br>æ¯æ¡æŒ‡ä»¤å¯¹ä¼—å¤šæ•°æ®å•å…ƒå¹¶è¡Œæ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚<br>SIMD is typically implemented using processors with vector registers and execution units; a scalar thread issues vector instructions that execute in SIMD fashion.<br>é€šè¿‡å…·æœ‰çŸ¢é‡å¯„å­˜å™¨å’Œæ‰§è¡Œå•å…ƒçš„å¤„ç†å™¨å®ç°ã€‚ä¸€ä¸ªæ ‡é‡çº¿ç¨‹å‘å°„çŸ¢é‡SIMDæŒ‡ä»¤ä½œç”¨äºæ•°æ®çŸ¢é‡ã€‚</p>
<p><strong>SIMT: Single Instruction, Multiple Thread</strong><br>In a SIMT architecture, rather than <font color="red">a single thread issuing vector instructions applied to data vectors</font>, <font color="red">multiple threads issue common instructions to arbitrary data</font>.<br>å¤šä¸ªçº¿ç¨‹å‘å°„ç›¸åŒçš„æŒ‡ä»¤ä½œç”¨äºä»»æ„æ•°æ®ã€‚çŸ¢é‡çº¿ç¨‹å‘å°„æ ‡é‡æŒ‡ä»¤ä½œç”¨äºå¤šä¸ªæ ‡é‡æ•°æ®ã€‚</p>
<p>NVIDIA GPUs execute warps of 32 parallel threads using SIMT, which enables each thread to access its own registers, to load and store from divergent addresses, and to follow divergent control flow paths. The CUDA compiler and the GPU work together to ensure the threads of a warp execute the same instruction sequences together as frequently as possible to maximize performance.<br>åˆ©ç”¨SIMTæ¶æ„æ‰§è¡Œä»¥32ä¸ªå¹¶è¡Œçº¿ç¨‹ä¸ºå•ä½çš„warpsï¼Œæ¯ä¸ªçº¿ç¨‹è®¿é—®è‡ªå·±çš„å¯„å­˜å™¨ï¼Œä»ä¸åŒåœ°å€å­˜å–æ•°æ®ï¼Œæ‰§è¡Œåˆ†æ­§æ§åˆ¶æµè·¯å¾„ã€‚</p>
<p>While the high performance obtained by warp execution happens behind the scene, many CUDA programs can achieve even higher performance by using explicit warp-level programming.<br>ç”±warpæ‰§è¡Œæ‰€å¸¦æ¥çš„é«˜æ€§èƒ½å¾€å¾€å‘ç”Ÿåœ¨å¹•åï¼Œè€ŒCUDAç¨‹åºè¿˜å¯ä»¥é€šè¿‡æ˜¾å¼warpçº§åˆ«ç¼–ç¨‹æ¥è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<center><img src="/2022/02/10/cuda-1/reduce_shfl_down.png" width="100%" height="100%"><font color="#708090" size="2">Part of a warp-level parallel reduction using shfl_down_sync().</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> FULL_MASK 0xffffffff</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">    val += __shfl_down_sync(FULL_MASK, val, offset);</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://xh.5156edu.com/page/z1015m9220j18754.html">é¢œè‰²è¡¨åŠhtmlä»£ç </a><br>ä¸€ä¸ªä½¿ç”¨warpå†…å‘ä¸‹shuffleå‡½æ•°å®ç°reductionæ“ä½œçš„ç¤ºä¾‹ã€‚</p>
<p>A warp comprises 32 lanes, with each thread occupying one lane. For a thread at lane X in the warp, __shfl_down_sync(FULL_MASK, val, offset) gets the value of the val variable from the thread at lane X+offset of the same warp. The data exchange is performed between registers, and more efficient than going through shared memory, which requires a load, a store and an extra register to hold the address.<br>å•ä¸ªwarpä¸­çš„threadè¢«ç§°ä¸ºä¸€ä¸ªlaneã€‚warp shuffleå‡½æ•°ç›´æ¥è¯»å–åŒä¸€warpå†…å…¶ä»–laneæ‰€å æœ‰çš„registerï¼Œç›¸æ¯”äºä½¿ç”¨shared memé€Ÿåº¦æ˜¾è€Œæ˜“è§çš„æ›´å¿«ã€‚è¯»å–shared meméœ€è¦ä¸€ä¸ªloadæ“ä½œï¼Œä¸€ä¸ªstoreæ“ä½œï¼Œè¿˜éœ€è¦ä¸€ä¸ªé¢å¤–çš„å¯„å­˜å™¨è£…åœ°å€ï¼ˆè®¿å­˜éœ€è¦è®¡ç®—åœ°å€åç§»ï¼‰ã€‚</p>
<p>CUDA 9 introduced three categories of new or updated warp-level primitives.<br>CUDA 9å¼•å…¥ä¸‰ç±»æ–°çš„/å‡çº§çš„warp-levelåŸºå…ƒå‡½æ•°</p>
<ol>
<li>Synchronized data exchange: exchange data between threads in warp.<br>åŒæ­¥æ•°æ®äº¤æ¢ï¼ŒåŸæœ‰warp shuffleçš„å‡çº§ã€‚<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__all_sync, __any_sync, __uni_sync, __ballot_sync</span><br><span class="line">__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</span><br><span class="line">__match_any_sync, __match_all_sync</span><br></pre></td></tr></table></figure></li>
<li>Active mask query: returns a 32-bit mask indicating which threads in a warp are active with the current executing thread.<br>æ´»åŠ¨æ©ç æŸ¥è¯¢ï¼Œè¿”å›32 bitçš„æ©ç ï¼Œå¯¹åº”32ä¸ªlaneï¼Œè¡¨æ˜åœ¨å½“å‰æ‰§è¡Œä¸­warpä¸­çš„å“ªäº›çº¿ç¨‹æ˜¯activeçš„ã€‚<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask</span><br></pre></td></tr></table></figure></li>
<li>Thread synchronization: synchronize threads in a warp and provide a memory fence.<br>çº¿ç¨‹åŒæ­¥æŒ‡ä»¤ï¼ŒåŒæ­¥warpå†…çš„æ‰€æœ‰çº¿ç¨‹ï¼Œå¹¶æä¾›ä¸€ä¸ªmemory fenceï¼ˆä¸ä¿è¯çº¿ç¨‹é—´çš„è®¿å­˜ç»“æœå½¼æ­¤å¯è§ï¼Œä»…ä¿è¯äº†çº¿ç¨‹å†…éƒ¨çš„è®¿å­˜é¡ºåºï¼‰ã€‚<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp</span><br></pre></td></tr></table></figure>
è¯¦ç»†å†…å®¹å‚è§ï¼š<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></li>
</ol>
<h2 id="Synchronized-Data-Exchange"><a href="#Synchronized-Data-Exchange" class="headerlink" title="Synchronized Data Exchange"></a>Synchronized Data Exchange</h2><p>Each of the â€œsynchronized data exchangeâ€ primitives perform a collective operation among a set of threads in a warp. The set of threads that participates in invoking each primitive is specified using a 32-bit mask, which is the first argument of these primitives. ç”±åŸºå…ƒå‡½æ•°ä¸­ç¬¬ä¸€ä¸ªå‚æ•°maskï¼Œæ¥æŒ‡å®šwarpä¸­å“ªäº›çº¿ç¨‹å‚ä¸è°ƒç”¨åŸºå…ƒå‡½æ•°ã€‚<br>All the participating threads must be synchronized for the collective operation to work correctly. Therefore, these primitives first synchronize the threads if they are not already synchronized. <font color="red">ä¸ºäº†ä¿è¯ååŒæ“ä½œçš„æ­£ç¡®æ€§ï¼Œæ‰€æœ‰å‚ä¸çš„çº¿ç¨‹å¿…é¡»åŒæ­¥ã€‚</font>æ„æ€å°±æ˜¯ï¼Œåœ¨è¯»å–å…¶ä»–laneçš„å¯„å­˜å™¨ä¹‹å‰ï¼Œå¿…é¡»ä¿è¯å¯¹åº”å¾—laneå·²ç»æ‰§è¡Œåˆ°äº†è¿™é‡Œï¼Œå‡†å¤‡å¥½äº†æ•°æ®ï¼Œæ‰èƒ½ä¿è¯æ­£ç¡®æ€§ï¼Œå¦åˆ™å¯èƒ½è¯»å–åˆ°å…¶ä»–çº¿ç¨‹ä¿®æ”¹è¿‡/æœªä¿®æ”¹çš„å€¼ï¼Œé€ æˆé”™è¯¯ã€‚æ¯”å¦‚reductionæ“ä½œçš„__shfl_down_syncï¼Œ32ä¸ªlaneä¸­çš„å‰16ä¸ªå…ˆè¯»å–å16ä¸ªçš„å¯„å­˜å™¨ï¼Œå¿…é¡»æ“ä½œå®Œæˆä¹‹åï¼Œå‰8ä¸ªæ‰èƒ½å†è¯»å–lane IDä¸º8-15çš„laneçš„å¯„å­˜å™¨ï¼Œä»¥æ­¤ç±»æ¨ï¼Œæ¯æ¬¡è¯»å–ä¹‹å‰éœ€è¦ä¸€ä¸ªwarpçº§åˆ«çš„åŒæ­¥ï¼Œæ¥ä¿è¯å¯¹åº”çš„laneä¸­å¯„å­˜å™¨çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ã€‚å› æ­¤è¿™äº›åŸºå…ƒå‡½æ•°é¦–å…ˆä¼šéšå¼åŒæ­¥çº¿ç¨‹ã€‚</p>
<blockquote>
<p>what should I use for the mask argument?</p>
</blockquote>
<p>maskæŒ‡å®šäº†å“ªäº›çº¿ç¨‹éœ€è¦å‚ä¸åˆ°ååŒæ“ä½œä¸­ï¼Œè¿™éƒ¨ä»½çº¿ç¨‹é€šå¸¸ç”±ç¨‹åºé€»è¾‘å†³å®šï¼Œåœ¨ç¨‹åºä¸­æ—©å…ˆçš„ä¸€äº›æ¡ä»¶åˆ†æ”¯ä¸­è®¡ç®—å¾—åˆ°ã€‚ä»¥reductionä¸ºä¾‹ï¼Œå¦‚æœvectorçš„å•å…ƒæ•°æ¯”blockä¸­çš„threadsæ•°é‡å°‘ï¼Œå¯¹åº”çš„ä»£ç å¦‚ä¸‹ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    â€¦</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>ballot_sync()ç”¨äºå¾—åˆ°å‚ä¸</strong>shfl_down_sync()çš„çº¿ç¨‹maskï¼Œè€Œ__ballot_sync()æœ¬èº«ä½¿ç”¨FULL_MASK (0xffffffff for 32 threads)ï¼Œå› ä¸ºæˆ‘ä»¬å‡å®šæ‰€æœ‰çº¿ç¨‹éƒ½è¦æ‰§è¡Œè¿™æ¡æŒ‡ä»¤ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> val, <span class="type">int</span> src_line, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __shfl_down_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> var, <span class="type">unsigned</span> detla, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>Each thread that calls __ballot_sync() receives a bit mask representing all the threads in the warp that pass a true value for the predicate argument.</p>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others.<br>Voltaä»¥åŠä¹‹åçš„æ¶æ„æ”¯æŒIndependent Thread Schedulingï¼Œå…è®¸åŒä¸€warpå†…çš„çº¿ç¨‹æ‰§è¡Œåˆ†æ­§åˆ†æ”¯ï¼Œä¸å†ä¸¥æ ¼ä¿è¯warpå†…çš„åŒæ­¥ã€‚</p>
<center><img src="/2022/02/10/cuda-1/NVIDIA%20GPU%20architecture.jpg" width="100%" height="100%"><font color="#708090" size="2">NVIDIA GPU architecture history, Voltaä¹‹åæœ€æ–°çš„æ¶æ„ä¸ºTuring</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    â€¦</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    â€¦</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Listing 4 shows an example where all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.<br><strong>shfl_sync()ç”¨äºè·å–æŒ‡å®šlane IDï¼ˆä»£ç ä¸­ä¸ºlane 0ï¼‰ä¸­å¯„å­˜å™¨ä¸­çš„æ•°æ®ã€‚ç¤ºä¾‹ä¸­ï¼Œwarpå†…æ‰€æœ‰çº¿ç¨‹è·å–lane 0ä¸­çš„valå˜é‡å€¼ï¼Œè€Œå¥‡æ•°çº¿ç¨‹ä¸å¶æ•°çº¿ç¨‹åˆ†åˆ«æ‰§è¡Œä¸€æ¡ifè¯­å¥çš„ä¸åŒåˆ†æ”¯ã€‚å¯ä»¥çœ‹åˆ°maskå‚æ•°è¿™é‡Œç»™å®šFULL_MASKï¼Œè¿™æ˜¯ç”±äºä¸ç®¡æ˜¯åŸºæ•°çº¿ç¨‹è¿˜æ˜¯å¶æ•°çº¿ç¨‹ï¼Œéƒ½ä¼šæ‰§è¡Œåˆ°ä¸€æ¡</strong>shfl_sync(FULL_MASK, val, 0)æŒ‡ä»¤ï¼Œå› æ­¤å¯¹è¿™32ä¸ªwarp laneåŒæ­¥å¹¶ä¸ä¼šå¯¼è‡´æ­»é”ã€‚</p>
<h2 id="Active-Mask-Query"><a href="#Active-Mask-Query" class="headerlink" title="Active Mask Query"></a>Active Mask Query</h2><p><strong>activemask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same </strong>activemask(). This is useful for the :opportunistic warp-level programmingâ€ technique we explain later, as well as for debugging and understanding program behavior.<br><strong>activemask()å°±æ˜¯è¿”å›å½“å‰warpä¸­æ‰€æœ‰activeçº¿ç¨‹çš„å¯¹åº”maskã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Incorrect use of __activemask()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    <span class="type">unsigned</span> mask = __activemask(); </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    â€¦</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>å¦‚ä¸Šä»£ç æ˜¯ä¸€ç§é”™è¯¯ç”¨æ³•ã€‚ç›¸æ¯”äºä½¿ç”¨<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br></pre></td></tr></table></figure><br>æ¥è®¡ç®—maskï¼Œä»£ç ä¸­åœ¨ifåˆ†æ”¯ä¸­ä½¿ç”¨<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __activemask(); </span><br></pre></td></tr></table></figure><br>æ¥è®¡ç®—maskã€‚é”™è¯¯çš„åŸå› åœ¨äºï¼Œä»Voltaæ¶æ„å¼€å§‹ï¼Œä¸å†ä¿è¯åŒä¸€warpå†…æ‰€æœ‰çº¿ç¨‹çš„å®Œå…¨åŒæ­¥æ‰§è¡Œï¼Œè€Œå½“æŸä¸€ä¸ªçº¿ç¨‹è°ƒç”¨</strong>activemask()ï¼Œå…¶è¿”å›çš„maskæ˜¯å½“å‰æ—¶åˆ»warpä¸­æ‰€æœ‰active threadsæ‰€å¯¹åº”çš„maskï¼Œè€Œä¸æ˜¯æ‰€æœ‰å°†ä¼šæ‰§è¡Œåˆ°è¿™é‡Œçš„threadsæ‰€å¯¹åº”çš„maskã€‚<br>The CUDA execution model does not guarantee that all threads taking the branch together will execute the __activemask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>
<h2 id="Warp-Synchronization"><a href="#Warp-Synchronization" class="headerlink" title="Warp Synchronization"></a>Warp Synchronization</h2><p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the <strong>syncwarp() primitive to synchronize threads in a warp. It is similar to the </strong>syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>ç”¨äºåŒæ­¥warpä¸­çš„çº¿ç¨‹ï¼ŒåŒæ­¥å“ªäº›ç”±maskæŒ‡å®šã€‚</p>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence** to allow threads to communicate via memory before and after calling the primitive.</p>
<p>åªè¦maskæŒ‡å®šçš„çº¿ç¨‹æ‰§è¡Œåˆ°äº†å…·æœ‰ç›¸åŒmaskçš„<strong>syncwarp()å³å¯ç»§ç»­æ‰§è¡Œï¼Œå¹¶éæ˜¯ä¸€å®šä½äºåŒä¸€ä¸ªæ§åˆ¶æµåˆ†æ”¯çš„</strong>syncwarp()ã€‚è¿˜é¢å¤–æä¾›ä¸€ä¸ªmemory fenceï¼Œä¿è¯äº†å½“å‰çº¿ç¨‹ä¸­å†…å­˜æ“ä½œçš„é¡ºåºã€‚<br>Memory fence functions only affect the ordering of memory operations by a thread; they do not ensure that these memory operations are visible to other threads (like <strong>syncthreads() does for threads within a block (see Synchronization Functions)).<br>Memory fenceä»…å½±å“å½“å‰çº¿ç¨‹å†…å†…å­˜æ“ä½œçš„é¡ºåºï¼Œå¹¶ä¸ä¿è¯å¯¹å…¶ä»–çº¿ç¨‹çš„å¯è§æ€§ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure><br>waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to </strong>syncthreads() are visible to all threads in the block.<br>ç­‰å¾…blockå†…æ‰€æœ‰çº¿ç¨‹åˆ°è¾¾ï¼Œå¹¶ä¿è¯åŒæ­¥æŒ‡ä»¤ä¹‹å‰çš„æ‰€æœ‰è®¿å­˜å¯¹æ•´ä¸ªblockå†…æ‰€æœ‰çº¿ç¨‹å¯è§ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> val = <span class="built_in">get_value</span>(â€¦);</span><br><span class="line">__shared__ <span class="type">float</span> smem[<span class="number">4</span>][<span class="number">8</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// matrix with row-major indexingè¡Œä¸»å…ƒçŸ©é˜µ</span></span><br><span class="line"><span class="comment">//   0  1  2  3  4  5  6  7 </span></span><br><span class="line"><span class="comment">//   8  9 10 11 12 13 14 15 </span></span><br><span class="line"><span class="comment">//  16 17 18 19 20 21 22 23</span></span><br><span class="line"><span class="comment">//  24 25 26 27 28 29 30 31</span></span><br><span class="line"><span class="type">int</span> x1 = threadIdx.x % <span class="number">8</span>;</span><br><span class="line"><span class="type">int</span> y1 = threadIdx.x / <span class="number">8</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// matrix with column-major indexingåˆ—ä¸»å…ƒçŸ©é˜µ</span></span><br><span class="line"><span class="comment">//   0  4  8 12 16 20 24 28</span></span><br><span class="line"><span class="comment">//   1  5 10 13 17 21 25 29</span></span><br><span class="line"><span class="comment">//   2  6 11 14 18 22 26 30</span></span><br><span class="line"><span class="comment">//   3  7 12 15 19 23 27 31</span></span><br><span class="line"><span class="type">int</span> x2= threadIdx.x / <span class="number">4</span>;</span><br><span class="line"><span class="type">int</span> y2 = threadIdx.x % <span class="number">4</span>;</span><br><span class="line"> </span><br><span class="line">smem[y1][x1] = val;</span><br><span class="line">__syncwarp();</span><br><span class="line">val = smem[y2][x2];</span><br><span class="line"> </span><br><span class="line"><span class="built_in">use</span>(val);</span><br></pre></td></tr></table></figure>
<p>å¦‚ä¸Šæ˜¯ä¸€ä¸ªçŸ©é˜µè½¬ç½®ä»£ç ï¼Œå¯ä»¥çœ‹å‡ºä½œè€…launch block sizeé€‰å–4 * 8 = 32ï¼Œæ°å¥½æ˜¯ä¸€ä¸ªwarpå¤§å°ã€‚å› æ­¤åœ¨å°†å„ä¸ªthreadå¯„å­˜å™¨ä¸­å†…å®¹å­˜å…¥shared memåï¼Œä½¿ç”¨<strong>syncwarp()åŒæ­¥ï¼Œç¡®ä¿shared memä¸­çš„æ•°æ®å·²ç»å®Œå…¨å‡†å¤‡å¥½ï¼Œå†è¿›è¡Œè¯»å–ï¼Œä»è€Œèµ·åˆ°ä¸€ä¸ªè½¬ç½®çš„ä½œç”¨ã€‚<br>Make sure that </strong>syncwarp() separates shared memory reads and writes to avoid race conditions.<br>é€šè¿‡åŒæ­¥åˆ†ç¦»äº†smemçš„è¯»å–å’Œå†™å…¥ï¼Œé¿å…äº†å†…å­˜ç«äº‰ã€‚</p>
<blockquote>
<p>A tree sum reduction in shared memory</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Incorrect use of __syncwarp()</span></span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>å‡å¦‚æƒ³åœ¨ warp å†…é€šè¿‡ shared memory åš reduction, å¹¶ä¸”å‡è®¾æ•°æ®åœ¨ shared memory å·²ç» readyï¼Œå¤§å®¶è§‰å¾—ä»£ç è¿™æ ·å†™æœ‰é—®é¢˜å—ï¼Ÿ</p>
</blockquote>
<p>é¦–å…ˆå‡è®¾blockä¸­åªæœ‰ä¸€ä¸ªwarpï¼Œsizeä¸º32ï¼Œä¸”shared memoryå¼€è¾Ÿäº†è¶³å¤Ÿå¤§ï¼Œä¸å­˜åœ¨è¶Šç•Œé—®é¢˜ï¼Œå°±æ˜¯è¯´shmemæœ‰48ä¸ªå…ƒç´ ã€‚<br>å¯¹äºPascalæ¶æ„åŠä»¥ä¸‹æ¥è¯´ï¼Œæˆ‘ä¸ªäººè®¤ä¸ºæ²¡æœ‰é—®é¢˜ï¼Œå› ä¸ºwarpä¸­æ‰€æœ‰çº¿ç¨‹çš„æŒ‡ä»¤æ˜¯å®Œå…¨åŒæ­¥çš„ï¼Œæ‰€æœ‰çš„smemè¯»ä¼šå‘ç”Ÿåœ¨æ‰€æœ‰smemå†™ä¹‹å‰è¿›è¡Œã€‚<br>è€Œå¯¹äºä»Voltaæ¶æ„å¼€å§‹ï¼Œæ”¯æŒwarpå†…çº¿ç¨‹ç‹¬ç«‹è°ƒåº¦ï¼Œä¸å†ä¿è¯è¿™ç§å®Œå…¨çš„åŒæ­¥ã€‚é‚£ä¹ˆç”±äºå¹¶æ²¡æœ‰æ·»åŠ tid &lt; 16çš„æ¡ä»¶åˆ¤æ–­ï¼Œå› æ­¤å¯¹äºshmem[16-31]è¿™éƒ¨åˆ†å†…å­˜æ¥è¯´ï¼Œæ¯ä¸ªåœ°å€ä¼šè¢«ä¸€ä¸ªçº¿ç¨‹è¯»å–ï¼Œè¢«å¦ä¸€ä¸ªçº¿ç¨‹å†™å…¥ï¼ŒäºŒè€…å‘ç”Ÿçš„é¡ºåºæ²¡æœ‰ä¿è¯ï¼ˆrace conditionï¼‰ï¼Œå› æ­¤æ˜¯ä¸æ­£ç¡®çš„ã€‚æ”¹æ­£çš„åŠæ³•å¾ˆç®€å•ï¼Œå°±æ˜¯åœ¨æ‹†åˆ†è¯»å†™ï¼Œåœ¨ä¸­é—´åŠ ä¸€ä¸ªåŒæ­¥æŒ‡ä»¤ï¼Œæœ€ç»ˆçš„ç»“æœä¸Pascalæ¶æ„çš„åšæ³•ä¸€è‡´ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line"><span class="type">int</span> v = shmem[tid];</span><br><span class="line"></span><br><span class="line">v += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;</span><br></pre></td></tr></table></figure>
<p>The CUDA compiler may elide some of these synchronization instructions in the final generated code depending on the target architecture (e.g. on pre-Volta architectures).<br>ç¼–è¯‘å™¨ä¼šé’ˆå¯¹ç›®æ ‡æ¶æ„åœ¨æœ€ç»ˆç”Ÿæˆçš„ä»£ç ä¸­çœç•¥è¿™æ ·çš„ä¸€äº›åŒæ­¥æŒ‡ä»¤ï¼Œæ¯”å¦‚åœ¨Voltaä¹‹å‰çš„æ¶æ„ä¸Šï¼Œè¿™äº›æ¶æ„warpå®Œå…¨åŒæ­¥æ‰§è¡Œï¼Œæ²¡æœ‰ç‹¬ç«‹è°ƒåº¦åŠŸèƒ½ä¸éœ€è¦åŒæ­¥æŒ‡ä»¤ã€‚</p>
<p>On the latest Volta (and future) GPUs, you can also use <strong>syncwarp() in thread-divergent branches to synchronize threads from both branches. But once they return from the primitive, the threads will become divergent again. See Listing 13 for such an example.<br>å¯ä»¥åœ¨åˆ†æ”¯ä¸­ä½¿ç”¨</strong>syncwarp()åŒæ­¥warpï¼Œä¸€æ—¦å‡½æ•°è¿”å›ï¼Œè¿™äº›çº¿ç¨‹é‡æ–°è¿›å…¥ä¸åŒåˆ†æ”¯ã€‚</p>
<h2 id="Opportunistic-Warp-level-Programming"><a href="#Opportunistic-Warp-level-Programming" class="headerlink" title="Opportunistic Warp-level Programming"></a>Opportunistic Warp-level Programming</h2><p>åœ¨è¿›å…¥åˆ†æ”¯ä¹‹å‰ï¼Œä¸€èˆ¬éœ€è¦å…ˆè®¡ç®—maskï¼Œç„¶åä¸€ç›´å‘ä¸‹ä¼ é€’è¿™ä¸ªmaskã€‚åœ¨ä¸€äº›library functionsï¼Œä½ ä¸èƒ½æ”¹å˜å‡½æ•°çš„interfaceï¼Œå°±æ— æ³•åœ¨è¿›è¡Œwarp-level programmingï¼Œå› ä¸ºä¸èƒ½å‘å‡½æ•°ä¸­ä¼ å…¥æ‰€éœ€è¦çš„maskã€‚</p>
<p>Some computations can use whatever threads happen to be executing together. We can use a technique called opportunistic warp-level programming, as the following example illustrates. </p>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/">CUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics</a><br>On warp-aggregated atomics for more information on the algorithm<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cooperative-groups/">Cooperative Groups: Flexible CUDA Thread Programming</a><br>For discussion of how Cooperative Groups makes the implementation much simpler.</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__INT.html">CUDA Math API</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   <span class="comment">// increment the value at ptr by 1 and return the old value</span></span><br><span class="line"><span class="number">2</span>   <span class="function">__device__ <span class="type">int</span> <span class="title">atomicAggInc</span><span class="params">(<span class="type">int</span> *ptr)</span> </span>&#123;</span><br><span class="line"><span class="number">3</span>       <span class="type">int</span> mask = __match_any_sync(__activemask(), (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)ptr);</span><br><span class="line"><span class="number">4</span>       <span class="type">int</span> leader = __ffs(mask) â€“ <span class="number">1</span>;    <span class="comment">// select a leader</span></span><br><span class="line"><span class="number">5</span>       <span class="type">int</span> res;</span><br><span class="line"><span class="number">6</span>       <span class="keyword">if</span>(<span class="built_in">lane_id</span>() == leader)                  <span class="comment">// leader does the update</span></span><br><span class="line"><span class="number">7</span>           res = <span class="built_in">atomicAdd</span>(ptr, __popc(mask));</span><br><span class="line"><span class="number">8</span>       res = __shfl_sync(mask, res, leader);    <span class="comment">// get leaderâ€™s old value</span></span><br><span class="line"><span class="number">9</span>       <span class="keyword">return</span> res + __popc(mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) â€“ <span class="number">1</span>)); <span class="comment">//compute old value</span></span><br><span class="line"><span class="number">10</span>  &#125;</span><br></pre></td></tr></table></figure>
<p>atomicAggInc() atomically increments the value pointed to by ptr by 1 and returns the old value. It uses the atomicAdd() function, which may incur contention. To reduce contention, atomicAggInc replaces the per-thread atomicAdd() operation with a per-warp atomicAdd().<br>å‡½æ•°atomicAggIncç»™è¾“å…¥æŒ‡é’ˆptræŒ‡å‘çš„å˜é‡+1ï¼Œç„¶åè¿”å›åŸå§‹å€¼ã€‚<br>ç›´æ¥ä½¿ç”¨atomicAdd()ä¼šå¼•å‘çº¿ç¨‹é—´å†…å­˜ç«äº‰å¯¼è‡´é¡ºåºæ‰§è¡Œå»¶è¿Ÿï¼ˆè¿™é‡Œåªæœ‰ä¸€ä¸ªåœ°å€ï¼Œç”±ptræŒ‡å‘ï¼‰ã€‚ä¸ºäº†å‡å°‘è¿™ç§å†…å­˜ç«äº‰ï¼ŒatomicAggIncä½¿ç”¨ä¸€ä¸ªwarpçº§åˆ«çš„atomicAdd()æ›¿æ¢æ‰äº†åŸæœ‰çš„threadçº§åˆ«çš„atomicAdd()ã€‚</p>
<p>The <strong>activemask() in line 3 finds the set of threads in the warp that are about to perform the atomic operation.<br>è¿”å›warpä¸­å°†è¦è¿›è¡ŒåŸå­æ“ä½œçš„çº¿ç¨‹é›†åˆã€‚</strong>activemask(): è¿”å›å½“å‰warpä¸­active threadsçš„maskã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_any_sync(<span class="type">unsigned</span> mask, T value);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_all_sync(<span class="type">unsigned</span> mask, T value, <span class="type">int</span> *pred);</span><br></pre></td></tr></table></figure>
<p>__match_any_sync()è¿”å›è¾“å…¥çš„æ©ç maskä¸­æŒ‡å®šçš„threadsä¸­ï¼Œvalueå€¼ç›¸åŒçš„çº¿ç¨‹çš„çº¿ç¨‹çš„æ©ç ã€‚å¯¹maskè¿›ä¸€æ­¥ç­›é€‰ï¼Œæ¡ä»¶æ˜¯valueå€¼ç›¸åŒã€‚<br>Returns mask of threads that have same value of value in mask.</p>
<p>__match_all_sync()ï¼Œè‹¥è¾“å…¥çš„æ©ç maskä¸­æŒ‡å®šçš„æ‰€æœ‰threadséƒ½å…·æœ‰ç›¸åŒçš„valueï¼Œåˆ™è¿”å›æ©ç ã€predæŒ‡å‘trueï¼Œå¦åˆ™è¿”å›0ã€predæŒ‡å‘falseã€‚</p>
<p><strong>match_any_sync() returns the bit mask of the threads that have the same value ptr, partitioning the incoming threads into groups whose members have the same ptr value.<br>è¿”å›</strong>activemask()è¿”å›çš„maskæŒ‡å®šçš„threadsä¸­ï¼Œptrå€¼ç›¸åŒçš„threadsæ‰€æ„æˆçš„æ©ç ã€‚ä¹Ÿå°±æ˜¯å¯¹æ­¤æ—¶activeçš„çº¿ç¨‹æ ¹æ®ptrçš„å€¼è¿›è¡Œè¿›ä¸€æ­¥åˆ†ç±»ã€‚<br>Returns mask if all threads in mask have the same value for value; otherwise 0 is returned. Predicate pred is set to true if all threads in mask have the same value of value; otherwise the predicate is set to false.</p>
<p>Each group elects a leader thread (line 4), which performs the atomicAdd() (line 7) for the whole group.<br>åˆ†ç±»å¾—åˆ°çš„æ¯ç»„çº¿ç¨‹é€‰å–ä¸€ä¸ªleaderï¼ˆé¦–ä¸ªmaskä¸­bitä½ä¸º1çš„çº¿ç¨‹ï¼‰ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªçº¿ç¨‹æ‰§è¡ŒatomicAdd()ï¼ŒåŠ çš„æ•°é‡ä¸ºmaskä¸­bitä¸º1çš„æ•°é‡ï¼Œæ­£å¥½å¯¹åº”æ­¤æ—¶ç»è¿‡ä¸¤æ¬¡ç­›é€‰å‰©ä¸‹çš„çº¿ç¨‹æ•°é‡ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__â€‹ <span class="type">int</span> __ffs ( <span class="type">int</span>  x )</span><br></pre></td></tr></table></figure><br>Find the position of the least significant bit set to 1 in a 32-bit integer.<br>Bit Operations: åœ¨ä¸€ä¸ª32bitæ•´å‹ä¸­æ‰¾åˆ°ç½®ä¸º1çš„æœ€ä½æœ‰æ•ˆä½ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__â€‹ <span class="type">int</span> __popc ( <span class="type">unsigned</span> <span class="type">int</span>  x )</span><br></pre></td></tr></table></figure><br>Count the number of bits that are set to 1 in a 32-bit integer.<br>Bit Operations: è¿”å›ä¸€ä¸ª32bitæ•´å‹ä¸­ä¸º1çš„bitä½æ•°é‡ã€‚</p>
<p>Every thread gets the old value from the leader (line 8) returned by the atomicAdd().<br>warpä¸­å…¶ä»–çº¿ç¨‹é€šè¿‡__shfl_sync()è¯»å–leaderçº¿ç¨‹ä¸­çš„reså˜é‡ã€‚</p>
<p>Line 9 computes and returns the old value the current thread would get from atomicInc() if it were to call the function instead of atomicAggInc.<br>æœ€ç»ˆå‡½æ•°è¿”å›è‹¥çº¿ç¨‹è°ƒç”¨atomicInc()è€Œä¸æ˜¯atomicAggInc()è€Œåº”è¯¥å¾—åˆ°çš„è¿”å›å€¼ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>())                        <span class="comment">//å¾—åˆ°å½“å‰çº¿ç¨‹å¯¹åº”bitç½®1çš„mask</span></span><br><span class="line">((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) â€“ <span class="number">1</span>)                  <span class="comment">//å¾—åˆ°å½“å‰çº¿ç¨‹çš„æ‰€æœ‰ä½ä½ç½®1çš„mask</span></span><br><span class="line">mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) â€“ <span class="number">1</span>)           <span class="comment">//ä¿ç•™å½“å‰çº¿ç¨‹æ‰€æœ‰ä½ä½çº¿ç¨‹æ‰€å¯¹åº”çš„mask</span></span><br><span class="line">__popc(mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) â€“ <span class="number">1</span>))   <span class="comment">//è®¡æ•°</span></span><br></pre></td></tr></table></figure><br>ä¹Ÿå°±æ˜¯å¯¹å½“å‰warpä¸­çš„çº¿ç¨‹ç”±ä½åˆ°é«˜åšä¸€ä¸ªå•è°ƒé€’å¢çš„ç´¯åŠ ã€‚å¦‚æœä½ä½ä¸€ä¸ªçº¿ç¨‹ä¸º1ï¼Œé‚£ä¹ˆé«˜ä½æ‰€æœ‰è¿”å›å€¼+1ã€‚</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/54055195/activemask-vs-ballot-sync"><strong>activemask() vs </strong>ballot_sync()</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__ballot_sync(<span class="type">unsigned</span> mask, predicate):</span><br></pre></td></tr></table></figure>
<p>Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if and only if <strong>predicate evaluates to non-zero for the Nth thread of the warp and the Nth thread is active</strong>.<br>æ˜¯ä¸€ä¸ªshuffle operationï¼Œéšå«ä¸€ä¸ªå¯¹maskæŒ‡å®šçš„çº¿ç¨‹çš„åŒæ­¥ä½œç”¨ï¼Œæ²¡æŒ‡å®šçš„ä¸çº¦æŸï¼Œè¡Œä¸ºæœªå®šä¹‰ï¼ˆå¯èƒ½æ‰§è¡Œäº†shuffleä¹Ÿå¯èƒ½æ²¡æ‰§è¡Œshuffleï¼‰ã€‚è¿”å›ä¸€ä¸ªmaskï¼Œå¦‚æœè¾“å…¥maskä¸­æŒ‡å®šçš„çº¿ç¨‹å¯¹åº”predicateä¸ºtrueï¼Œé‚£ä¹ˆå¯¹åº”bitç½®1ï¼Œå¦åˆ™ä¸º0ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask():</span><br></pre></td></tr></table></figure>
<p>Returns a 32-bit integer mask of <strong>all currently active threads in the calling warp</strong>. The Nth bit is set if <strong>the Nth lane in the warp is active when __activemask() is called</strong>. Inactive threads are represented by 0 bits in the returned mask. Threads which have exited the program are always marked as inactive. Note that threads that are convergent at an __activemask() call are not guaranteed to be convergent at subsequent instructions unless those instructions are synchronizing warp-builtin functions.<br>ä»…æ˜¯ä¸€ä¸ªæŸ¥è¯¢åŠŸèƒ½ï¼Œæ²¡æœ‰ä»»ä½•æ“ä½œï¼Œä¸å¯¹çº¿ç¨‹è¿›è¡Œä»»ä½•é™åˆ¶ï¼Œä»…è¿”å›å½“å‰warpä¸­activeï¼ˆconvergentï¼‰çš„çº¿ç¨‹å¯¹åº”çš„maskã€‚</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-shfl-sync">PTX ISA: 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync</a></p>
<p>As an additional clarification on the usage of the mask parameter, note the usage statements in the PTX guide. In particular, the mask parameter is not intended to be an exclusion method. If you wish threads to be excluded from the shuffle operation, you must use conditional code to do that. This is important in light of the following statement from the PTX guide:<br>maskä»…æŒ‡å‡ºå“ªäº›çº¿ç¨‹å¿…é¡»è¦è¿›è¡ŒåŒæ­¥ã€å‚åŠ shuffle operationï¼Œå¹¶ä¸ä¼šå°†æ²¡æŒ‡å®šçš„çº¿ç¨‹æ’é™¤åœ¨å¤–ã€‚å¦‚æœéœ€è¦å°†æŸäº›çº¿ç¨‹æ’é™¤åœ¨å¤–ï¼Œéœ€è¦è®¤ä¸ºä½¿ç”¨conditional codeï¼Œåˆ¶é€ divergenceã€‚</p>
<blockquote>
<p>The behavior of shfl.sync is undefined if the executing thread is not in the membermask.</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> mask = __match_any_sync(__activemask(), (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)ptr);</span><br></pre></td></tr></table></figure>
<p>å›çœ‹è¿™æ¡æŒ‡ä»¤ï¼Œ<strong>activemask()æŒ‡å‡ºå½“å‰åŒæ­¥çš„çº¿ç¨‹æœ‰å“ªäº›ï¼Œä»¥è¿™äº›çº¿ç¨‹çš„maskä¸ºå‚æ•°ä¼ ç»™</strong>match_any_sync()ï¼Œæ­¤å‡½æ•°è¿›ä¸€æ­¥æŒ‡å‡ºè¿™äº›çº¿ç¨‹ä¸­ï¼ŒptræŒ‡å‘çš„å€¼ç›¸ç­‰çš„æœ‰å“ªäº›ï¼Œå¹¶ä»¥è¿™äº›çº¿ç¨‹ä¸ºmaskç»§ç»­å‘ä¸‹æ‰§è¡Œã€‚</p>
<p>this statement is saying â€œtell me which threads are convergedâ€ (i.e. the <strong>activemask() request), and then â€œuse (at least) those threads to perform the </strong>match_all operation. This is perfectly legal and will use whatever threads happen to be converged at that point. As that listing 9 example continues, the mask computed in the above step is used in the only other warp-cooperative primitive:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = __shfl_sync(mask, res, leader); </span><br></pre></td></tr></table></figure><br><strong>shfl_sync()å‡½æ•°åˆå¸¦äº†ä¸€ä¸ªå¯¹maskä¸­æŒ‡å®šçº¿ç¨‹çš„åŒæ­¥ï¼Œå°±æ˜¯è¯´maskæ‰€æŒ‡å®šçš„çº¿ç¨‹åœ¨</strong>match_any_sync()ä¸__shfl_sync()ä¹‹é—´çš„æ“ä½œå¯ä»¥åŸºæœ¬è®¤ä¸ºæ˜¯åŒæ­¥çš„ã€‚</p>
<h2 id="Implicit-Warp-Synchronous-Programming-is-Unsafe"><a href="#Implicit-Warp-Synchronous-Programming-is-Unsafe" class="headerlink" title="Implicit Warp-Synchronous Programming is Unsafe"></a>Implicit Warp-Synchronous Programming is Unsafe</h2><p>CUDA toolkits prior to version 9.0 provided a (now legacy) version of warp-level primitives. Compared with the CUDA 9 primitives, the legacy primitives do not accept a mask argument.<br>æ—©äºCUDA 9.0çš„ç‰ˆæœ¬çš„warp-level primitivesç‰ˆæœ¬æ²¡æœ‰maskå‚æ•°ï¼ˆä¸éœ€è¦warpåŒæ­¥ï¼‰ã€‚<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __any(<span class="type">int</span> predicate)</span><br><span class="line"><span class="type">int</span> __any_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate)</span><br></pre></td></tr></table></figure><br>The mask argument, as explained previously, specifies the set of threads in a warp that must participate in the primitives. The new primitives perform intra-warp thread-level synchronization if the threads specified by the mask are not already synchronized during execution.<br>CUDA 9.0å¼•å…¥çš„æ–°ç‰ˆæœ¬æä¾›äº†warpå†…çº§åˆ«çš„çº¿ç¨‹åŒæ­¥ï¼Œç”±maskæŒ‡å®šçš„çº¿ç¨‹å¿…é¡»åŒæ­¥ï¼Œå…¶ä½™çº¿ç¨‹ä¸åšè¦æ±‚ã€‚</p>
<p>The legacy warp-level primitives do not allow programmers to specify the required threads and do not perform synchronization. Therefore, the threads that must participate in the warp-level operation are not explicitly expressed by the CUDA program. The correctness of such a program depends on implicit warp-synchronous behavior, which may change from one hardware architecture to another, from one CUDA toolkit release to another (due to changes in <strong>compiler optimizations</strong>, for example), or even from one run-time execution to another. Such implicit warp-synchronous programming is unsafe and may not work correctly.<br>åŸå…ˆçš„warp-levelåŸºå…ƒå‡½æ•°åœ¨ä½¿ç”¨æ—¶ä¸è¦æ±‚æŒ‡å®šå¿…é¡»å‚ä¸çš„çº¿ç¨‹ï¼Œä¹Ÿä¸è¿›è¡Œçº¿ç¨‹åŒæ­¥ã€‚å› æ­¤ï¼ŒCUDAç¨‹åºæ²¡æœ‰æ˜¾å¼æŒ‡å®šå“ªäº›çº¿ç¨‹å¿…é¡»å‚ä¸warp-levelæ“ä½œï¼Œè¿™ç§ç¨‹åºçš„æ­£ç¡®æ€§æœ‰èµ–äºéšå¼warpåŒæ­¥ã€‚è¿™ç§ç¨‹åºéšç€æ¶æ„ã€CUDAç‰ˆæœ¬ã€è¿è¡Œæ—¶çš„ä¸åŒå¯èƒ½å…·æœ‰ä¸åŒè¡Œä¸ºã€‚æ­£ç¡®æ€§æ— æ³•å¾—åˆ°ä¿è¯ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming all 32 threads in a warp execute line 1 together.</span></span><br><span class="line"><span class="built_in">assert</span>(__ballot(<span class="number">1</span>) == FULL_MASK);</span><br><span class="line"><span class="type">int</span> result;</span><br><span class="line"><span class="keyword">if</span> (thread_id % <span class="number">2</span>) &#123;</span><br><span class="line">    result = <span class="built_in">foo</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    result = <span class="built_in">bar</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">unsigned</span> ballot_result = __ballot(result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">assert</span><span class="params">( <span class="type">int</span> expression )</span></span>;</span><br></pre></td></tr></table></figure>
<p>Evaluates an expression and, when the result is false, prints a diagnostic message and aborts the program.<br>è‹¥è¡¨è¾¾å¼ä¸ºfalseï¼Œè¾“å‡ºè¯Šæ–­æ¶ˆæ¯å¹¶ç»ˆæ­¢ç¨‹åºã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> __ballot_sync(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p><strong>ballot()æ˜¯</strong>ballot_sync()çš„æ—©æœŸç‰ˆæœ¬ã€‚æ¥å—ä¸€ä¸ªint predicateå‚æ•°ã€‚<br>return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp.<br>è¿”å›ä¸€ä¸ªæ•´å½¢ï¼Œè‹¥ç¬¬Nä¸ªçº¿ç¨‹predicateä¸ºçœŸï¼Œåˆ™æ•´å‹å¯¹åº”bitç½®ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚<br><strong>ballot(1) == FULL_MASKï¼Œæ¯«æ— ä½œç”¨ï¼Œè¾“å…¥å‚æ•°ä¸º1ï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½ä¸º1ã€‚<br>foo, barè¿™é‡Œç†è§£ä¸ºéšæ„ä¸¤ä¸ªå‡½æ•°ï¼Œå¼ ä¸‰æå››ä¹‹ç±»çš„<br>å‡è®¾warpä¸­çš„32ä¸ªçº¿ç¨‹åŒæ—¶æ‰§è¡Œäº†ç¬¬ä¸€æ¬¡</strong>ballot()ï¼Œè€Œéšåçš„if elseå¯¼è‡´äº†çº¿ç¨‹çš„divergenceï¼Œè€Œä¸åŒåˆ†æ”¯ä¸­å‡½æ•°æ‰§è¡Œæ—¶é—´æœªå¿…ä¸€è‡´ï¼Œå› æ­¤åœ¨æ‰§è¡Œåˆ°ç¬¬äºŒæ¬¡<strong>ballot()æ—¶ï¼Œwarpçš„re-convergenceä¸èƒ½ä¿è¯ã€‚å› æ­¤ballot_resultä¸­å¯èƒ½ä¸ä¼šåŒ…å«warpä¸­æ‰€æœ‰çº¿ç¨‹çš„ç»“æœï¼Œè€Œä»…åŒ…å«å½“å‰çº¿ç¨‹è°ƒç”¨</strong>ballot()æ—¶convergent threadsçš„ç»“æœã€‚<br>åœ¨ç¬¬äºŒæ¬¡<strong>ballot()ä¹‹å‰ä½¿ç”¨</strong>syncwarp()åŒæ­¥æ•´ä¸ªwarpä¹Ÿä¸ä¿è¯æ­£ç¡®æ€§ï¼Œè¿™ä¹Ÿæ˜¯éšå¼åŒæ­¥implicit warp-synchronous programmingï¼Œå…¶éšå«ä¸€ä¸ªæ¡ä»¶ï¼šä¸€æ—¦åŒæ­¥ä¹‹åï¼Œwarpä¸­æ‰€æœ‰çº¿ç¨‹ä¿æŒåŒæ­¥ç›´åˆ°é‡åˆ°ä¸‹ä¸€æ¬¡thread-divergent branchã€‚è¿™ä¸ªè™½ç„¶çœ‹ä¼¼æ­£ç¡®ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰å¾—åˆ°CUDAçš„å®˜æ–¹è®¤å¯ã€‚æœ€å®‰å…¨çš„åŠæ³•å°±æ˜¯å°†<strong>ballot()æ›¿æ¢ä¸º</strong>ballot_sync()ï¼Œå¹¶æŠŠæ‰€æœ‰çº¿ç¨‹éƒ½æ ‡è®°åœ¨maskå‚æ•°ä¸­ã€‚</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp();</span><br><span class="line"><span class="type">unsigned</span> ballot_result = __ballot(result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> ballot_result = __ballot_sync(FULL_MASK, result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   v = <span class="built_in">foo</span>();</span><br><span class="line"><span class="number">2</span>   <span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line"><span class="number">3</span>       __syncwarp();</span><br><span class="line"><span class="number">4</span>       v = __shfl(<span class="number">0</span>);       <span class="comment">// L3 will get undefined result because lane 0 </span></span><br><span class="line"><span class="number">5</span>       __syncwarp();        <span class="comment">// is not active when L3 is executed. L3 and L6</span></span><br><span class="line"><span class="number">6</span>   &#125; <span class="keyword">else</span> &#123;                 <span class="comment">// will execute divergently.</span></span><br><span class="line"><span class="number">7</span>       __syncwarp();</span><br><span class="line"><span class="number">8</span>       v = __shfl(<span class="number">0</span>);</span><br><span class="line"><span class="number">9</span>       __syncwarp();</span><br><span class="line"><span class="number">10</span>  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp(); </span><br><span class="line">v = __shfl(<span class="number">0</span>); </span><br><span class="line">__syncwarp(); </span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shfl_sync(FULL_MASK, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>äºŒè€…å¹¶ä¸ç­‰åŒã€‚</p>
<p><strong>Reason 1:</strong><br>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence<strong> to allow threads to communicate via memory before and after calling the primitive.<br>çº¿ç¨‹é¦–å…ˆä¼šåŒæ­¥(convergent)åœ¨line 3 &amp; 7ã€‚æ¥ä¸‹æ¥ç”±äºif elseåˆ†æ”¯çš„å…³ç³»ï¼Œthreads in a warp become divergentï¼Œå› æ­¤line 4æ‰§è¡Œçš„æ—¶å€™ï¼Œlane 0ç”±äºä¸åœ¨æ­¤åˆ†æ”¯ä¸Šï¼Œå› æ­¤æœ‰å¯èƒ½æ˜¯inactiveçš„ï¼ˆè¢«å±è”½äº†ï¼‰ï¼Œå¯¹lane 0ä½¿ç”¨<strong>shfl()ç»“æœundefinedã€‚æ”¹ç”¨</strong>shfl_sync(FULL_MASK, 0)å°±ä¼šè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ­¤å‡½æ•°ä¼šå¼ºåˆ¶maskä¸­æŒ‡å®šçš„æ‰€æœ‰threads convergeï¼Œç„¶åæ‰§è¡Œå¯¹åº”shuffle operationã€‚
</strong>Reason 2:<em>*<br>__syncwarp()ä»…èƒ½ä¿è¯åœ¨æ‰§è¡Œæ­¤å‡½æ•°æ—¶ï¼Œwarpä¸­æ‰€æœ‰threads convergeï¼Œå¹¶ä¸èƒ½ä¿è¯åœ¨æ‰§è¡Œå®Œè¿™æ¡ä»£ç ä¹‹åï¼Œthreadsè¿˜èƒ½ä¿æŒconvergentï¼Œè™½ç„¶å¤§å¤šæ•°æƒ…å†µæ˜¯convergentçš„ï¼Œä½†æ²¡æœ‰å¾—åˆ°å®˜æ–¹è®¤å¯is not guaranteedã€‚å±äºimplicit warp-synchronous programmingã€‚Thread convergence is guaranteed only within explicitly synchronous warp-level primitives like </em>_sync functions. éšå¼åŒæ­¥is unsafe programï¼Œå› æ­¤CUDA 9.0ä¹‹åèˆå¼ƒäº†åŸæœ‰çš„shuffleå‡½æ•°ã€‚</p>
<h2 id="Update-Legacy-Warp-Level-Programming"><a href="#Update-Legacy-Warp-Level-Programming" class="headerlink" title="Update Legacy Warp-Level Programming"></a>Update Legacy Warp-Level Programming</h2><p>å°†åŸæ¥çš„ç¨‹åºæ›´æ–°ä¸ºä½¿ç”¨æ–°shuffleå‡½æ•°ï¼ˆthe sync version of the primitivesï¼‰çš„ç¨‹åºã€‚<br>Any form of implicit warp-synchronous programming, such as communicating between threads of a warp without synchronization.<br>ä¹Ÿå¯ä»¥é€‰æ‹©use Cooperative Groups restructure your codeã€‚å…¶æä¾›äº†ä¸€ä¸ªæ›´é«˜çº§åˆ«çš„ç¼–ç»„æŠ½è±¡ï¼Œå¯ä»¥ä»»æ„ç»„ç»‡çº¿ç¨‹å¹¶åŒæ­¥ç»„å†…çº¿ç¨‹ï¼Œsuch as multi-block synchronizationã€‚</p>
<p>ä½¿ç”¨warp-level primitivesæœ€å›°éš¾çš„éƒ¨ä»½å°±æ˜¯ç¡®å®šå‡½æ•°æ‰€ä½¿ç”¨çš„maskï¼ˆå‚ä¸shuffle operationsçš„threadsï¼‰ã€‚Here are some suggestions:</p>
<blockquote>
<ol>
<li><p>Donâ€™t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.<br>ä¸è¦ç®€å•çš„ä½¿ç”¨FULL_MASKå½“maskå‚æ•°ä¼ å…¥primitivesï¼Œå¦‚æœwarpä¸­æœ‰threadsæ‰§è¡Œä¸åˆ°ä¼šé€ æˆæ­»é”hangã€‚</p>
</li>
<li><p>Donâ€™t just use <strong>activemask() as the mask value. </strong>activemask() tells you what threads happen to be convergent when the function is called, which can be different from what you want to be in the collective operation.<br>ä¹Ÿä¸è¦ç®€å•çš„ä½¿ç”¨<strong>activemask()çš„è¿”å›å€¼å½“ä½œmaskï¼Œ</strong>activemask()ä»…ä»…è¿”å›å½“å‰ç¢°å·§convergentçš„threadsï¼Œä¸ä½ å¸Œæœ›çš„threadså¯èƒ½ä¼šæœ‰å‡ºå…¥ã€‚</p>
</li>
<li><p>Do analyze the program logic and understand the membership requirements. Compute the mask ahead based on your program logic.<br>åˆ†æç¨‹åºé€»è¾‘ï¼Œç†è§£éœ€æ±‚ï¼Œåœ¨ç¨‹åºé€»è¾‘åˆ†æ”¯ä¹‹å‰å…ˆè¡Œè®¡ç®—maskã€‚</p>
</li>
<li><p>If your program does opportunistic warp-synchronous programming, use â€œdetectiveâ€ functions such as <strong>activemask() and </strong>match_any_sync() to find the right mask.<br>å¦‚æœè¦åœ¨ç¨‹åºä¸­ä½¿ç”¨opportunistic warp-synchronous programmingï¼ˆä¸åšåŒæ­¥ï¼Œç›´æ¥æŸ¥è¯¢å½“å‰active threadsï¼Œå†ç­›é€‰å‡ºæ»¡è¶³æ¡ä»¶çš„threadsï¼Œæœ‰å“ªäº›ç”¨å“ªäº›ï¼‰ï¼Œä½¿ç”¨ä¸€äº›å…·æœ‰æŸ¥è¯¢æ£€æµ‹åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚<strong>activemask() and </strong>match_any_sync()ï¼Œæ¥è¿”å›æ»¡è¶³è¦æ±‚çš„maskã€‚</p>
</li>
<li><p>Use <strong>syncwarp() to separate operations with intra-warp dependences. Do not assume lock-step execution.<br>ä¸è¦å‡å®šçº¿ç¨‹åŒæ­¥implicit synchronousï¼Œä½¿ç”¨</strong>syncwarp()åˆ†ç¦»å…·æœ‰intra-warp dependencesçš„æ“ä½œã€‚warpå†…å‰åä¾èµ–ï¼ŒæŒ‡åç»­æ“ä½œä¾èµ–å‰é¢æ“ä½œçš„ç»“æœã€‚</p>
</li>
</ol>
</blockquote>
<p>If your existing CUDA program gives a different result on Volta architecture GPUs, and you suspect the difference is caused by Voltaâ€™s new independent thread scheduling which can change warp synchronous behavior, you may want to recompile your program with <strong>nvcc options -arch=compute_60 -code=sm_70</strong>. Such compiled programs opt-in to Pascalâ€™s thread scheduling.<br>Caused by implicit warp-synchronous programming.</p>
<p>Voltaâ€™s new independent thread scheduling:<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/inside-volta/">Inside Volta: The Worldâ€™s Most Advanced Data Center GPU</a></p>
<p>Volta independent thread scheduling enables interleaved execution of statements from divergent branches. This enables execution of fine-grain parallel algorithms where threads within a warp may synchronize and communicate.</p>
<center><img src="/2022/02/10/cuda-1/Volta_independent_thread_scheduling.png" width="100%" height="100%"><font color="#708090" size="2">Volta_independent_thread_scheduling.</font></center>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/09/NVIDIA-intern-paper-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/09/NVIDIA-intern-paper-md/" class="post-title-link" itemprop="url">NVIDIA Intern Paper 1 Smoke Simulation Method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-09 23:02:28" itemprop="dateCreated datePublished" datetime="2022-02-09T23:02:28+08:00">2022-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-11 15:11:38" itemprop="dateModified" datetime="2022-02-11T15:11:38+08:00">2022-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation/" itemprop="url" rel="index"><span itemprop="name">Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation/1-Smoke-Simulation-Method/" itemprop="url" rel="index"><span itemprop="name">1 Smoke Simulation Method</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation"><a href="#Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation" class="headerlink" title="Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation"></a>Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation</h1><h2 id="1-Smoke-Simulation-Method"><a href="#1-Smoke-Simulation-Method" class="headerlink" title="1. Smoke Simulation Method"></a>1. Smoke Simulation Method</h2><h3 id="1-1-Operator-Splitting-Method"><a href="#1-1-Operator-Splitting-Method" class="headerlink" title="1.1. Operator-Splitting Method"></a>1.1. Operator-Splitting Method</h3><p><strong>Reference:</strong> Harris M J. Fast fluid dynamics simulation on the GPU[J]. SIGGRAPH Courses, 2005, 220(10.1145): 1198555-1198790.<br>&ensp;&ensp;The smoke simulation solves an incompressible NS equation expressed as</p>
<script type="math/tex; mode=display">\frac{\partial \mathbf{u} }{\partial t}=-\left( \mathbf{u}\cdot \nabla  \right)\mathbf{u}-\frac{1}{\rho }\nabla p+\nu { {\nabla }^{2} }\mathbf{u}+\mathbf{F} \tag {1.1.1}</script><p>, where <strong>u</strong> is the velocity field and for an incompressible fluid, it satisfies</p>
<script type="math/tex; mode=display">\nabla \cdot \mathbf{u}=0. \tag {1.1.2}</script><p>With the Operator-Splitting Method, the solution of the NS equation is calculated via composition of transformations on the state. In other words, each component of the NS equation is a step that takes a field as input, and produces a new field as output. $\color{red}{[Fast Fluid Dynamics Simulation on the GPU.]}$ Define an operator <strong>S</strong> that is equivalent to the solution of NS equation over a single time step. Then, the operator S can be decomposed into the operators for advection <strong>A</strong>, diffusion <strong>D</strong>, force application <strong>F</strong>, and projection <strong>P</strong>.</p>
<center><img src="/2022/02/09/NVIDIA-intern-paper-md/1.1.1_1.png" width="50%" height="50%"></center>

<p>&ensp;&ensp;After <strong>F</strong>, <strong>D</strong>, and <strong>A</strong> operations, we can get a divergent velocity field <strong>w</strong>. Then we can get the velocity field <strong>u</strong> with zero divergence by adopting the Helmholtz-Hodge Decomposition Theorem.</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{u}+\nabla p \tag {1.1.3}</script><p>This process is called the projection corresponding to <strong>P</strong>. A projection operation is always needed to guarantee the incompressibility of the fluid.<br>&ensp;&ensp;If we apply the divergence operator to both sides of the above equation, we obtain</p>
<script type="math/tex; mode=display">\nabla \cdot \mathbf{u}+{ {\nabla }^{2} }p={ {\nabla }^{2} }p=\nabla \cdot \mathbf{w}. \tag {1.1.4}</script><p>Note that for an incompressible fluid, we have $\nabla \cdot \mathbf{u}=0$. Then, it becomes a Poisson equation for the pressure of the fluid, sometimes called the Poisson-pressure equation. With this equation, we can get the pressure p, and then use <strong>w</strong> and p to compute the new divergence-free velocity field <strong>u</strong> by</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{u}+\nabla p. \tag {1.1.5}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/1970/01/01/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="é‚µå¤§å®çš„å­¦ä¹ Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/1970/01/01/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 1970-01-01 08:00:00" itemprop="dateCreated datePublished" datetime="1970-01-01T08:00:00+08:00">1970-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-09 19:43:51" itemprop="dateModified" datetime="2022-02-09T19:43:51+08:00">2022-02-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiang Shao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Shao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
