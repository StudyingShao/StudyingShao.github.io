<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="邵大宝的学习Blog">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="邵大宝的学习Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jiang Shao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>邵大宝的学习Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">邵大宝的学习Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">最爱严小跳</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/22/cuda-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/22/cuda-5/" class="post-title-link" itemprop="url">CUDA学习随记5 Asynchronous copy cp.async</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-22 21:51:12" itemprop="dateCreated datePublished" datetime="2022-02-22T21:51:12+08:00">2022-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-14 23:17:25" itemprop="dateModified" datetime="2022-06-14T23:17:25+08:00">2022-06-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDA学习随记5-Asynchronous-copy-cp-async"><a href="#CUDA学习随记5-Asynchronous-copy-cp-async" class="headerlink" title="CUDA学习随记5 Asynchronous copy cp.async"></a>CUDA学习随记5 Asynchronous copy cp.async</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-asynchronous-copy">1. PTX (Parallel Thread Execution) ISA (Instruction Set Architecture)</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities (用于查询SASS code)</a></p>
<p>An asynchronous copy operation copies data from one state space to another asynchronously <strong>without blocking the executing thread</strong>.<br>Initiation of an asynchronous copy operation simply dispatches the copy operation from the source memory location to the destination memory location.<br>异步拷贝，不阻塞线程，仅发起一个异步拷贝请求。</p>
<p>There are two ways to wait for the completion of an asynchronous copy operation:<br>两种方式等待异步拷贝操作完成：</p>
<ol>
<li>使用<strong>cp.async-groups</strong><br>a. Initiate asynchronous copy operations.<br>b. Commit copy operations into a cp.async-group. <strong>Using cp.async.commit_group</strong><br>c. Wait for cp.async-group to complete the copy. <strong>Using cp.async.wait_group</strong><br>d. Once the cp.async-group completes, the writes performed by the copy operation in that cp.async-group are made <strong>visible to the thread that initiated the copy operations</strong>.</li>
<li>使用<strong>mbarrier</strong>对象<br>a. Initiate asynchronous copy operations.<br>b. Make an mbarrier object track the asynchronous copy operations.<br>c. Wait for the mbarrier object to complete the phase阶段 using mbarrier.test_wait.<br>d. Once mbarrier.test_wait returns True, the writes performed by the copy operation are made <strong>visible to all the threads which waited on the mbarrier object</strong>.</li>
</ol>
<p>A sequence of asynchronous copy operations initiated by a thread can be batched into a <strong>per-thread group</strong> referred to as cp.async-group.<br>由单个线程发出的一系列异步拷贝操作可以打包为a per-thread group，称为cp.async-group。</p>
<p>A commit operation creates <strong>a cp.async-group containing all prior asynchronous copy operations initiated by the executing thread</strong> but none of the asynchronous copy operations following the commit operation. A committed asynchronous copy operation belongs to a single cp.async-group.<br>创建一个cp.async-group将包含所有由本线程发出的之前的异步拷贝操作，不包括之后的。每个异步拷贝操作仅属于一个cp.async-group。<br>由于执行到cp.async-group时，其之前的所有异步拷贝操作均已被对应cp.async-group收录，因此下一个cp.async-group仅包含两条cp.async-group之间的新发出的异步拷贝操作，所以说每个异步拷贝操作仅属于一个cp.async-group。</p>
<p>When a cp.async-group completes, all the asynchronous copy operations belonging in that group are complete and the executing thread that initiated copy operations can read copied results. All cp.async-groups committed by an executing thread always complete in the order in which they were committed. There is no ordering between asynchronous copy operations within a cp.async-group.<br>当cp.async-group完成时，其所包含的所有异步拷贝操作均已完成，发出异步拷贝请求的线程（也就是当前线程）可以安全读取拷贝结果。线程发射的cp.async-groups永远按照顺序完成，而cp.async-group中包含的异步拷贝操作的执行顺序是随机的。</p>
<p>Writes performed by an asynchronous copy operation are visible to the thread that initiated the asynchronous copy operation only after the cp.async-group completes or mbarrier object tracking the asynchronous copy has completed the phase.<br>仅当cp.async-group完成 or mbarrier追踪到异步拷贝完成，当前线程才能安全读取由当前线程发出的异步拷贝操作写入的数据（数据可见）。</p>
<p>Once an asynchronous copy operation is initaited, modifiying the source memory location or reading from the destination memory location before the asynchronous copy operation completes, will cause unpredictable results.<br>一旦一个异步拷贝操作被发起，不等待异步拷贝操作完成就 修改源内存地址中的数据 or 就读取目标内存地址中的数据，将会产生unpredictable results。</p>
<h4 id="cp-async"><a href="#cp-async" class="headerlink" title="cp.async"></a>cp.async</h4><p>Initiates an asynchronous copy operation from one state space to another.<br>发起一个异步拷贝操作。<br>Operand src specifies a location in the global state space and dst specifies a location in the shared state space.<br>异步拷贝只能是从global mem拷贝到shared mem。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cp.async.ca.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], cp-size&#123;, src-size&#125;&#123;, cache-policy&#125; ;</span><br><span class="line">cp.async.cg.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], <span class="number">16</span>&#123;, src-size&#125;&#123;, cache-policy&#125; ;</span><br><span class="line">cp.async.ca.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], cp-size&#123;, ignore-src&#125;&#123;, cache-policy&#125; ;</span><br><span class="line">cp.async.cg.shared.global&#123;.level::cache_hint&#125;&#123;.level::prefetch_size&#125;</span><br><span class="line">                         [dst], [src], <span class="number">16</span>&#123;, ignore-src&#125;&#123;, cache-policy&#125; ;</span><br></pre></td></tr></table></figure><br>The <strong>.cg</strong> qualifier indicates caching of data only at global level cache L2 and not at L1 whereas <strong>.ca</strong> qualifier indicates caching of data at all levels including L1 cache. Cache operator are treated as performance hints only.<br><strong>.cg</strong>表示仅用L2 cache，数据传输绕过L1 cache与register。对应LDGSTS.BYPASS。<br><strong>.ca</strong>表示同时使用L1 cache与L2 cache，数据传输仅绕过register。对应LDGSTS.ACCESS。<br>这俩标识符只是对编译器的建议，具体用不用要看编译器的意思。实际上具体使用哪种取决于传输数据的大小与对齐方式。4 Bytes-&gt;access，16 Bytes-&gt;bypass。</p>
<p>Operand cp-size is an integer constant which specifies the size of data in bytes to be copied to the destination dst. cp-size can only be 4, 8 and 16.<br>cp-size指定了传输数据的大小（in bytes），可以是4、8、16，可以看到cp-size最小是4，而.cg指令cp-size直接指定为16，正好符合上面的说法。</p>
<center><img src="/2022/02/22/cuda-5/LDGSTS%20access%20and%20bypass.png" width="100%" height="100%"><font color="#708090" size="2">Two types of LDGSTS: access and bypass.</font></center>

<p>Instruction cp.async allows optionally specifying a 32-bit integer operand <strong>src-size</strong>. Operand src-size represents the size of the data in bytes to be copied from src to dst and must be less than <strong>cp-size</strong>. In such case, remaining bytes in destination dst are filled with zeros. Specifying src-size larger than cp-size results in undefined behavior.<br>src-size，指定拷贝源数据大小，必须比cp-size小，否则undefined behavior。很简答，如果比cp-size大，则拷贝数据不完整，再读取肯定不对。cp-size比src-size多出来的部份在dst中会被填充0。</p>
<p>The optional and non-immediate predicate argument <strong>ignore-src</strong> specifies whether the data from the source location src should be ignored completely. If the source data is ignored then zeros will be copied to destination dst. If the argument ignore-src is not specified then it defaults to False.<br>指定是否忽略scr中的数据，如果置为true，则向dst中拷贝0。默认为false。</p>
<p>The mandatory(强制的) <strong>.async</strong> qualifier indicates that the cp instruction will <strong>initiate the memory copy operation asynchronously</strong> and <strong>control will return to the executing thread before the copy operation is complete</strong>. The executing thread can then use <strong>cp.async.wait_all</strong> or <strong>cp.async.wait_group</strong> or <strong>mbarrier instructions</strong> to wait for completion of the asynchronous copy operation. No other synchronization mechanisms described in Memory Consistency Model can be used to guarantee the completion of the asynchronous copy operations.<br>三种用来同步异步数据传输操作的方法：<br><strong>cp.async.wait_all</strong> or <strong>cp.async.wait_group</strong> or <strong>mbarrier instructions</strong></p>
<p>There is no ordering guarantee between two cp.async operations if they are not explicitly synchronized using cp.async.wait_all or cp.async.wait_group or mbarrier instructions.<br>如果没有显式同步，两个cp.async之间没有顺序保证。</p>
<p>The <strong>.level::prefetch_size</strong> qualifier is a hint to <strong>fetch additional data of the specified size into the respective cache level</strong>.The sub-qualifier prefetch_size can be set to either of 64B, 128B, 256B thereby allowing the prefetch size to be 64 Bytes, 128 Bytes or 256 Bytes respectively.</p>
<p>The qualifier .level::prefetch_size may only be used with .global state space and with generic addressing where the address points to .global state space. If the generic address does not fall within the address window of the global memory, then the prefetching behavior is undefined.</p>
<p>The .level::prefetch_size qualifier is treated as a performance hint only.</p>
<p>When the optional argument <strong>cache-policy</strong> is specified, the qualifier <strong>.level::cache_hint</strong> is required. The 64-bit operand cache-policy specifies <strong>the cache eviction policy</strong> that may be used during the memory access.</p>
<p>The qualifier .level::cache_hint is only supported for .global state space and for generic addressing where the address points to the .global state space.</p>
<p>cache-policy is a hint to the cache subsystem and may not always be respected. It is treated as a performance hint only, and does not change the memory consistency behavior of the program.</p>
<p>.level::prefetch_size与.level::cache_hint都是与缓存相关的qualifier。都是hints to the compiler，并不一定被编译器所采纳。由于是缓存相关的，所以操作对象地址必须在global memory中，否则会导致undefined behavior.<br>.level::prefetch_size用于在缓存中除了要拷贝的数据之外，额外fetch一定大小的数据到对应cache中，用于增加缓存命中率，减少从device memory中读取数据的次数。<br>.level::cache_hint用于cache-policy指定时，用于指定缓存清除策略，如何使用缓存。</p>
<h4 id="cp-async-commit-group"><a href="#cp-async-commit-group" class="headerlink" title="cp.async.commit_group"></a>cp.async.commit_group</h4><p>Commits all prior initiated but uncommitted cp.async instructions into a cp.async-group.<br>将之前的未被commit的cp.async commit到一个cp.async-group。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp.async.commit_group;</span><br></pre></td></tr></table></figure>
<p>这条PTX code所产生的SASS code就是<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LDGDEPBAR <span class="comment">//Global Load Dependency Barrier</span></span><br></pre></td></tr></table></figure><br>会导致一个很大的stall long scoreboard。</p>
<p><strong>cp.async.commit_group</strong> instruction creates a new cp.async-group <strong>per thread</strong> and batches all prior cp.async instructions <strong>initiated by the executing thread but not committed to any cp.async-group</strong> into the new cp.async-group. If there are no uncommitted cp.async instructions then cp.async.commit_group results in an empty cp.async-group.</p>
<p>An executing thread can wait for the completion of all cp.async operations in a cp.async-group using cp.async.wait_group.</p>
<p>There is no memory ordering guarantee provided between any two cp.async operations within the same cp.async-group. So two or more cp.async operations within a cp.async-group copying data to the same location results in undefined behavior.<br>在同一个cp.async-group中的cp.async没有顺序保证，因此在同一cp.async-group中对同一地址的cp.async会导致undefined behavior。</p>
<h4 id="cp-async-wait-group-cp-async-wait-all"><a href="#cp-async-wait-group-cp-async-wait-all" class="headerlink" title="cp.async.wait_group/cp.async.wait_all"></a>cp.async.wait_group/cp.async.wait_all</h4><p>Wait for completion of prior asynchronous copy operations.<br>等待先前的异步拷贝操作完成。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp.async.wait_group N;</span><br><span class="line">cp.async.wait_all;</span><br></pre></td></tr></table></figure>
<p><strong>cp.async.wait_group</strong> instruction will cause executing thread to wait till <strong>only N or fewer of the most recent cp.async-groups are pending</strong> and <strong>all the prior cp.async-groups committed by the executing threads are complete</strong>. For example, when N is 0, the executing thread waits on all the prior cp.async-groups to complete. Operand N is an integer constant.</p>
<p>cp.async.wait_group N; 等待execution thread直到还剩 N 个或更少个<strong>最近的</strong>cp.async-group还在进行中、未完成。由于cp.async-group按照被commit的顺序依次完成，因此先发出的必先完成，等待指令等待后发出的cp.async-group。</p>
<p>cp.async.wait_all; 等待execution thread先前的所有cp.async都完成。也就是等价于commit一个cp.async-group然后马上等待所有先前的cp.async-group完成。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp.async.commit_group; <span class="comment">//将仍未被commit的cp.async commit到一个cp.async-group中</span></span><br><span class="line">cp.async.wait_group <span class="number">0</span>; <span class="comment">//等待所有先前的cp.async-group完成</span></span><br></pre></td></tr></table></figure></p>
<p>如果N为0，那么这条PTX code所产生的SASS code就是<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEPBAR.LE SB0, <span class="number">0x0</span> ;<span class="comment">//等待直到SB0的计数降至0</span></span><br></pre></td></tr></table></figure><br>DEPBAR的作用是等待，直到某一Scoreboard的计数小于特定值。（比如DEPBAR.LE SB5, 0x6 ;表示stall直至第5个Scoreboard的值降到6或以下）</p>
<p>SB指的就是scoreboard。记录cp.async-group的数量（一个counter）与每个cp.async-group所对应的cp.async (LDGSTS)的数量。每次一个cp.async-group完成，counter - 1。counter的数量刚好对应N的值。<br>N 指明线程等待直到只有 N 或更少个cp.async-group未完成才继续。如果是0，那么就是等待只有0个未完成，代表等待所有的cp.async-group完成。由于cp.async-group是保持发射顺序的，因此N等待的永远是倒数的N个cp.async-group，就是最近发射的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example of .wait_all:</span></span><br><span class="line">cp.async.ca.shared.global [shrd1], [gbl1], <span class="number">4</span>;</span><br><span class="line">cp.async.cg.shared.global [shrd2], [gbl2], <span class="number">16</span>;</span><br><span class="line">cp.async.wait_all;  <span class="comment">// waits for all prior cp.async to complete</span></span><br></pre></td></tr></table></figure>
<p>cp.async.wait_all等待前面所有的cp.async完成。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example of .wait_group :</span></span><br><span class="line">cp.async.ca.shared.global [shrd3], [gbl3], <span class="number">8</span>;</span><br><span class="line">cp.async.commit_group;  <span class="comment">// End of group 1</span></span><br><span class="line"></span><br><span class="line">cp.async.cg.shared.global [shrd4], [gbl4], <span class="number">16</span>;</span><br><span class="line">cp.async.commit_group;  <span class="comment">// End of group 2</span></span><br><span class="line"></span><br><span class="line">cp.async.cg.shared.global [shrd5], [gbl5], <span class="number">16</span>;</span><br><span class="line">cp.async.commit_group;  <span class="comment">// End of group 3</span></span><br><span class="line"></span><br><span class="line">cp.async.wait_group <span class="number">1</span>;  <span class="comment">// waits for group 1 and group 2 to complete</span></span><br></pre></td></tr></table></figure>
<p>cp.async.wait_group 1 允许有一个cp.async-group未完成，按照顺序，此条指令等待group 1 and 2完成。</p>
<p>Writes performed by cp.async operations are made <strong>visible to the executing thread</strong> only after :</p>
<ol>
<li>The completion of cp.async.wait_all or</li>
<li>The completion of cp.async.wait_group on the cp.async-group in which the cp.async belongs to or</li>
<li>mbarrier.test_wait returns True on an mbarrier object which is tracking the completion of the cp.async operation.<br>三种情况下，由cp.async写入的数据对当前线程可见。cp.async.wait_all完成了，代表此处之前所有的cp.async都已完成。cp.async所在的cp.async-group完成了，使用cp.async.wait_group实现。追踪此cp.async的mbarrier的mbarrier.test_wait返回true。</li>
</ol>
<p>There is no ordering between two cp.async operations that are not synchronized with cp.async.wait_all or cp.async.wait_group or mbarrier objects.</p>
<p><strong>cp.async.wait_group and cp.async.wait_all does not provide any ordering and visibility guarantees for any other memory operation apart from cp.async.</strong><br>cp.async.wait_group and cp.async.wait_all仅影响cp.async的顺序，其他内存操作与此无关。很容易看出嘛。前缀都是cp.async.*，明显就是这个专用的。这里没说mbarrier，说明这玩应是对所有内存操作都有效的，是同步障碍。</p>
<h3 id="与Julien-Demouth-x3c-x6a-100-x65-109-x6f-x75-x74-x68-64-110-x76-105-x64-105-x61-46-x63-x6f-109-gt-的邮件往来"><a href="#与Julien-Demouth-x3c-x6a-100-x65-109-x6f-x75-x74-x68-64-110-x76-105-x64-105-x61-46-x63-x6f-109-gt-的邮件往来" class="headerlink" title="与Julien Demouth &#x3c;&#x6a;&#100;&#x65;&#109;&#x6f;&#x75;&#x74;&#x68;&#64;&#110;&#x76;&#105;&#x64;&#105;&#x61;&#46;&#x63;&#x6f;&#109;&gt;的邮件往来"></a>与Julien Demouth <a href="&#x6d;&#97;&#x69;&#108;&#x74;&#111;&#58;&#x3c;&#x6a;&#100;&#x65;&#109;&#x6f;&#x75;&#x74;&#x68;&#64;&#110;&#x76;&#105;&#x64;&#105;&#x61;&#46;&#x63;&#x6f;&#109;">&#x3c;&#x6a;&#100;&#x65;&#109;&#x6f;&#x75;&#x74;&#x68;&#64;&#110;&#x76;&#105;&#x64;&#105;&#x61;&#46;&#x63;&#x6f;&#109;</a>&gt;的邮件往来</h3><h4 id="我愚蠢的问题"><a href="#我愚蠢的问题" class="headerlink" title="我愚蠢的问题"></a>我愚蠢的问题</h4><p>Hi Julien<br>Sorry to bother you. I am an intern in devtech team. I have seen your email about LDGDEPBAR with LDGSTS.  I have a question about it.<br>Can LDGSTS bands to a certain gmem barrier？For example, I want to do a prefetch on smem. In a for cycle, can i issue a set of LDGSTS for variable A and LDS variable B.  Then i sync LDGSTS for A. next loop i issue a set of LDGSTS for B and then LDS A. THEN I sync LDGSTS for B. then run this two loop many times.<br>is this possible by using inlinePTX？<br>I will really appreciate if you can answer my question.<br>thanks a lot. </p>
<blockquote>
<p>这里我想做一个SMEM的prefetch，在当前循环步使用LDGSTS从GMEM中预先异步读取下一个循环步要使用到的数据到SMEM。而当前循环步使用LDS从SMEM中读取上一循环预读的数据。</p>
</blockquote>
<h4 id="大佬的回答"><a href="#大佬的回答" class="headerlink" title="大佬的回答"></a>大佬的回答</h4><p>Hi Jiang,</p>
<p>You are not bothering me 😊. I hope you enjoy your internship with NVIDIA and the Devtech team.</p>
<p>Say you have two LDGSTS for A and two LDGSTS for B and you want to alternate between both:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>  LDGSTS A0 <span class="comment">// write to shared memory</span></span><br><span class="line"><span class="number">2</span>  LDGSTS A1 <span class="comment">// write to shared memory – you have to make sure that’s a different region from A0</span></span><br><span class="line"><span class="number">3</span>  LDGDEPBAR </span><br><span class="line"><span class="number">4</span> </span><br><span class="line"><span class="number">5</span>  LDS B0 <span class="comment">// load from shared memory – that buffer must have been filled earlier</span></span><br><span class="line"><span class="number">6</span>  LDS B1</span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span>  DEPBAR <span class="number">0</span> <span class="comment">// make sure the instructions before the LAST LDGDEPBAR have completed =&gt; A0/A1 are in shared memory</span></span><br><span class="line"><span class="number">9</span>  BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">11</span> LDGSTS B2 <span class="comment">// trigger the next LDGSTS for B – you can reuse the same memory buffer than the one for B0</span></span><br><span class="line"><span class="number">12</span> LDGSTS B3</span><br><span class="line"><span class="number">13</span> LDGDEPBAR </span><br><span class="line"><span class="number">14</span>  </span><br><span class="line"><span class="number">15</span> LDS A0 </span><br><span class="line"><span class="number">16</span> LDS A1</span><br><span class="line"><span class="number">17</span> </span><br><span class="line"><span class="number">18</span> DEPBAR <span class="number">0</span> </span><br><span class="line"><span class="number">19</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>&emsp;&emsp;解读一下这段代码，Line 2 特意告知我 “you have to make sure that’s a different region from A0” 是由于同一个 cp.async_group 中的 cp.saync 操作的先后顺序是没有保证的，因此对同一地址的拷贝会导致未定义的结果。<br>&emsp;&emsp;Line 3 是cp.async.commit_group，创建一个cp.async_group将前两个LDGSTS包含在内。到此为止的三行发起了从GMEM到SMEM的异步拷贝操作，并将其归纳到了同一个cp.async_group。<br>&emsp;&emsp;Line 5 &amp; 6 使用LDS从SMEM中读取上一循环中准备好的数据。<br>&emsp;&emsp;Line 8 为cp.async.wait_group 0; 等待直到还剩下0个cp.async_group未完成，也就是等待所有cp.async_group完成。这里为确保A0、A1数据准备就绪。<br>&emsp;&emsp;BAR.SYNC 为__syncthreads()，同步block内所有线程。（从SMEM中读取数据必须保证同步，保证所有线程都已经读取完毕）<br>&emsp;&emsp;后续操作重复这一过程，区别是交换了预读使用的与当前读取的SMEM buffer。即为上一循环预读数据存入A0、A1，使用B0、B1已经读好的数据。下一循环预读数据存入B0、B1，使用A0、A1已经读好的数据。以此达到隐藏访存延迟的效果。</p>
</blockquote>
<p>This code does the trick. </p>
<p>That said if you have enough shared memory you can do smarter things:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>  LDGSTS A0</span><br><span class="line"><span class="number">2</span>  LDGSTS A1</span><br><span class="line"><span class="number">3</span>  LDGDEPBAR </span><br><span class="line"><span class="number">4</span> </span><br><span class="line"><span class="number">5</span>  LDGSTS B2 <span class="comment">// It must write to a different buffer in shared memory than the one used for B0/B1</span></span><br><span class="line"><span class="number">6</span>  LDGSTS B3</span><br><span class="line"><span class="number">7</span>  LDGDEPBAR </span><br><span class="line"><span class="number">8</span> </span><br><span class="line"><span class="number">9</span>  LDS B0</span><br><span class="line"><span class="number">10</span> LDS B1</span><br><span class="line"><span class="number">11</span> </span><br><span class="line"><span class="number">12</span> DEPBAR <span class="number">1</span> <span class="comment">// The LDGSTS B2/B3 can still be in flight</span></span><br><span class="line"><span class="number">13</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">14</span> </span><br><span class="line"><span class="number">15</span> LDGSTS A2 <span class="comment">// It must write to a different buffer in shared memory than the one used for A0/A1 – shared memory for B0/B1?</span></span><br><span class="line"><span class="number">16</span> LDGSTS A3</span><br><span class="line"><span class="number">17</span> LDGDEPBAR </span><br><span class="line"><span class="number">18</span> </span><br><span class="line"><span class="number">19</span> LDS A0</span><br><span class="line"><span class="number">20</span> LDS A1</span><br><span class="line"><span class="number">21</span>  </span><br><span class="line"><span class="number">22</span> DEPBAR <span class="number">1</span> <span class="comment">// A2/A3 can still be in flight </span></span><br><span class="line"><span class="number">23</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br></pre></td></tr></table></figure>
<p>I hope it makes sense. If not, do not hesitate to ask questions.</p>
<blockquote>
<p>&emsp;&emsp;再来看一下这段代码，大体上基本相似。Line 3、7分别创建了两个cp.async-group，Line 3包含了Line 1、2的异步拷贝操作，而Line 7则包含Line 5、6的异步拷贝。随后LDS从SMEM中B0、B1地址读取在之前已经准备好的数据。<br>&emsp;&emsp;Line 12 DEPBAR 1;指定线程等待直到仅剩一个cp.async-group，这里只有两个，为等待A0、A1访存结束。此时B2、B3还在传输中。<br>&emsp;&emsp;Line 17又创建了一个cp.async-group，包含了Line 15、16的异步拷贝操作。<br>&emsp;&emsp;由于A0、A1数据已经准备就绪，因此可直接LDS访问A0、A1。<br>&emsp;&emsp;执行到Line 22时，同样指定线程等待直到仅剩一个cp.async-group，此时仅剩2个cp.async-group仍在执行，即为等待B2、B3数据拷贝结束。</p>
</blockquote>
<p>补充一下后续其他操作，这种改动相当于是使用更多的SMEM，将每次LDGSTS中间穿插两次LDS，增大发起从GMEM中取回数据的异步拷贝操作与LDS之间的时间间隔(间隔两个循环步的prefetch)，更好地隐藏访存延迟。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>  LDGSTS A0</span><br><span class="line"><span class="number">2</span>  LDGSTS A1</span><br><span class="line"><span class="number">3</span>  LDGDEPBAR </span><br><span class="line"><span class="number">4</span> </span><br><span class="line"><span class="number">5</span>  LDGSTS B2 <span class="comment">// It must write to a different buffer in shared memory than the one used for B0/B1</span></span><br><span class="line"><span class="number">6</span>  LDGSTS B3</span><br><span class="line"><span class="number">7</span>  LDGDEPBAR </span><br><span class="line"><span class="number">8</span>  <span class="comment">// A0 A1 B2 B3 PENDING B0 B1 READY</span></span><br><span class="line"><span class="number">9</span>  LDS B0</span><br><span class="line"><span class="number">10</span> LDS B1</span><br><span class="line"><span class="number">11</span> </span><br><span class="line"><span class="number">12</span> DEPBAR <span class="number">1</span> <span class="comment">// The LDGSTS B2/B3 can still be in flight</span></span><br><span class="line"><span class="number">13</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">14</span> <span class="comment">// B2 B3 PENDING B0 B1 A0 A1 READY</span></span><br><span class="line"><span class="number">15</span> LDGSTS A2 <span class="comment">// It must write to a different buffer in shared memory than the one used for A0/A1 – shared memory for B0/B1?</span></span><br><span class="line"><span class="number">16</span> LDGSTS A3</span><br><span class="line"><span class="number">17</span> LDGDEPBAR </span><br><span class="line"><span class="number">18</span> <span class="comment">// B2 B3 A2 A3 PENDING B0 B1 A0 A1 READY</span></span><br><span class="line"><span class="number">19</span> LDS A0</span><br><span class="line"><span class="number">20</span> LDS A1</span><br><span class="line"><span class="number">21</span>  </span><br><span class="line"><span class="number">22</span> DEPBAR <span class="number">1</span> <span class="comment">// A2/A3 can still be in flight </span></span><br><span class="line"><span class="number">23</span> BAR.SYNC <span class="comment">// __syncthreads</span></span><br><span class="line"><span class="number">24</span> <span class="comment">// A2 A3 PENDING B0 B1 A0 A1 B2 B3 READY</span></span><br><span class="line"><span class="number">25</span> LDGSTS B0</span><br><span class="line"><span class="number">26</span> LDGSTS B1</span><br><span class="line"><span class="number">27</span> LDGDEPBAR</span><br><span class="line"><span class="number">28</span> <span class="comment">// A2 A3 B0 B1 PENDING A0 A1 B2 B3 READY</span></span><br><span class="line"><span class="number">29</span> LDS B2</span><br><span class="line"><span class="number">30</span> LDS B3</span><br><span class="line"><span class="number">31</span> </span><br><span class="line"><span class="number">32</span> DEPBAR <span class="number">1</span> </span><br><span class="line"><span class="number">33</span> BAR.SYNC</span><br><span class="line"><span class="number">34</span> <span class="comment">// B0 B1 PENDING A0 A1 B2 B3 A2 A3 READY</span></span><br></pre></td></tr></table></figure></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/20/cuda-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/20/cuda-4/" class="post-title-link" itemprop="url">CUDA学习随记4 Volta’s Independent Thread Scheduling</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-20 20:09:48" itemprop="dateCreated datePublished" datetime="2022-02-20T20:09:48+08:00">2022-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-28 16:55:12" itemprop="dateModified" datetime="2022-06-28T16:55:12+08:00">2022-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Volta’s-Independent-Thread-Scheduling"><a href="#Volta’s-Independent-Thread-Scheduling" class="headerlink" title="Volta’s Independent Thread Scheduling"></a>Volta’s Independent Thread Scheduling</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/inside-volta/">Inside Volta: The World’s Most Advanced Data Center GPU</a></p>
<p>High Performance Computing (HPC) is a fundamental pillar of modern science. From predicting weather, to discovering drugs, to finding new energy sources, researchers use large computing systems to simulate and predict our world. AI extends traditional HPC by allowing researchers to analyze large volumes of data for rapid insights where simulation alone cannot fully predict the real world.<br>高性能计算HPC是现代科学的基本支柱。从天气预测到药物研究，再到获取新能源，研究人员使用大型的计算系统来模拟与预测我们的世界。人工智能AI通过允许研究人员分析大量的数据以便快速获得洞察，拓展了传统的高性能计算，用于仅靠模拟不能完全预测现实世界的情况。<br>Computational Science for scientific simulation.<br>Data Science for finding insights in data.</p>
<center><img src="/2022/02/20/cuda-4/Volta%20GV100%20architecture.png" width="100%" height="100%"><font color="#708090" size="2">Volta GV100 Full GPU with 84 SM Units.</font></center>

<p><strong>每块 GV100 包含：</strong><br>6 x Graphics Processing Clusters (GPCs)<br>每个GPC包含 7 x Texture Processing Clusters (TPCs)，共 42<br>每个TPC包含 2 x Streaming Multiprocessors (SMs)，共 84<br>8 x 512-bit Memory Controllers(4096 bits total) 用于与显存进行数据交换</p>
<p><strong>每个 SM 包含：</strong><br>64 FP32 Cores<br>64 INT32 Cores<br>32 FP64 Cores<br><em>8 new Tensor Cores</em><br>4 texture units</p>
<center><img src="/2022/02/20/cuda-4/Volta%20GV100%20SM.png" width="100%" height="100%"><font color="#708090" size="2">Volta GV100 Full GPU with 84 SM Units.</font></center>

<p><strong>每个SM被划分为四个部份partitions，每个部份包含：</strong><br>8 FP64 Cores<br>16 INT32 Cores<br>16 FP32 Cores<br>2 new mixed-precision Tensor Cores for deep learning matrix arithmetic<br>a new L0 instruction cache<br>a warp scheduler<br>a dispatch unit<br>a 64 KB Register File</p>
<p>Unlike Pascal GPUs, which could not execute FP32 and INT32 instructions simultaneously, the Volta GV100 SM includes separate FP32 and INT32 cores, allowing simultaneous execution of FP32 and INT32 operations at full throughput, while also increasing instruction issue throughput. </p>
<p>Pascal GPU所有的CUDA核心都是FP32+INT32混合核心，这种核心每个周期只能进行浮点运算或整型运算的一种。整块SM分区（具有一个warp scheduler）同时切换，这块分区某一时刻只能进行FP或INT运算的一种。</p>
<p>Volta/Turing GPU单独开辟了一条INT型通道，其具有独立的FP32与INT32核心，可同时执行FP32与INT32操作。</p>
<blockquote>
<p>关于NVIDIA三代GPU架构CUDA核心的变化（Pascal, Turing, Ampere）<br><a target="_blank" rel="noopener" href="https://weibo.com/tv/show/1034:4548031207637036?from=old_pc_videoshow">微博 林亦LYi: 什么是「显卡测不准原理」</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1a64y1F7Gp?from=search&amp;seid=665276990359275781&amp;spm_id_from=333.337.0.0">B站 林亦LYi: 什么是「显卡测不准原理」</a></p>
</blockquote>
<center><img src="/2022/02/20/cuda-4/FP_INT.png" width="100%" height="100%"><font color="#708090" size="2">Floating Point vs Integer.</font></center>
<center><img src="/2022/02/20/cuda-4/CUDA_cores.png" width="100%" height="100%"><font color="#708090" size="2">SM of Volta & Turing</font></center>

<blockquote>
<p>RTX20系显卡使用Turing架构，而RTX30系显卡使用Ampere架构。<br>图中可以看出，FP32核心要比INT32核心更大一些，这是由于用于浮点型运算的核心要更复杂一些，也只有FP32核心才被称为CUDA核心。<br>Turing架构中，INT32核心和FP32核心是独立的，每个SM中各有64个。<br>Ampere架构中，除了64个FP32核心，原有的64个INT32核心转变为了64个FP32+INT32核心，而这部分核心也算作CUDA核心，因此每个SM上共有64+64=128个CUDA核心。<br>FP32+INT32核心在一个周期内，只能进行浮点型或整数型运算的一种。二选一。混合核心并不能提供完全的浮点运算能力。程序的整型运算越多，浮点运算能力就会下降越明显。这是由于混合核心在FP32与INT32核心间的切换导致的。每次切换时，以SM中的一块partition为单位切换，partition中的所有混合核心同时切换。</p>
</blockquote>
<center><img src="/2022/02/20/cuda-4/CUDA_cores_1.png" width="100%" height="100%"><font color="#708090" size="2">SM of Volta & Turing</font></center>

<blockquote>
<p>Pascal架构中所有CUDA核心都是混合核心，SM每个分区一个时刻只能进行一种运算。<br>Turing架构中为INT型单独开辟一条通道，可同时执行INT型运算与FP运算，但这样INT型运算性能过剩，多数时间都在摸鱼。<br>Ampere架构中把INT型核心又换回了混合核心，这样就避免了INT型核心空闲的问题。</p>
</blockquote>
<h2 id="Tensor-Cores"><a href="#Tensor-Cores" class="headerlink" title="Tensor Cores"></a>Tensor Cores</h2><p>Matrix-Matrix multiplication (BLAS GEMM) operations are at the core of neural network training and inferencing, and are used to multiply large matrices of input data and weights in the connected layers of the network.<br>矩阵乘操作是神经网络训练和推理的核心，被用于做输入数据和对应权重矩阵的乘法。<br>BLAS: Basic Linear Algebra Subprograms 基本线性代数子程序<br>GEMM: General Matrix Multiplication 通用矩阵乘法</p>
<p>Each Tensor Core provides a 4x4x4 matrix processing array which performs the operation $\textbf{D} = \textbf{A} \times \textbf{B} + \textbf{C}$, where $\textbf{A}$, $\textbf{B}$, $\textbf{C}$, and $\textbf{D}$ are 4×4 matrices as shown below. The matrix multiply inputs $\textbf{A}$ and $\textbf{B}$ are FP16 matrices, while the accumulation matrices $\textbf{C}$ and $\textbf{D}$ may be FP16 or FP32 matrices.<br>每个Tensor Core提供一个4x4x4的矩阵处理阵列，分别包括ABCD四个4x4的矩阵，乘法矩阵AB是FP16（半精度half类型）的，而加法矩阵CD可以是FP16也可以是FP32。</p>
<center><img src="/2022/02/20/cuda-4/Tensor%20Core%204x4x4%20matrix%20multiply%20and%20accumulate.png" width="100%" height="100%"><font color="#708090" size="2">Tensor Core 4x4x4 matrix multiply and accumulate.</font></center>

<p>Each Tensor Core performs 64 floating point FMA mixed-precision operations per clock.<br>FMA: Fused Multiply Add/Accumulate 融合乘加<br>4x4 矩阵乘$\textbf{A} \times \textbf{B}$，一共做 4x4x4 = 64次乘法，4x4x3 = 48次加法。结果矩阵加一个 4x4 矩阵$\textbf{C}$。整个计算一共64次乘，64次加，因此为64 FMA operations。<br>D00 = A00*B00 + C00 FMA<br>D00 = A01*B10 + D00 FMA<br>D00 = A02*B20 + D00 FMA<br>D00 = A03*B30 + D00 FMA<br>结果矩阵$\textbf{D}$中一个数对应4次FMA。</p>
<p>During program execution, multiple Tensor Cores are used concurrently by a full warp of execution. The threads within a warp provide a larger 16x16x16 matrix operation to be processed by the Tensor Cores. CUDA exposes these operations as Warp-Level Matrix Operations in the CUDA C++ API. These C++ interfaces provide specialized matrix load, matrix multiply and accumulate, and matrix store operations to efficiently utilize Tensor Cores in CUDA C++ programs.<br>多个Tensor cores被一个完整的warp同时使用，为Warp-Level Matrix Operations，包括matrix load, matrix multiply and accumulate, and matrix store operations。<br>一个Tensor Core可以提供一个<strong>4x4x4</strong>的矩阵操作，一个warp可使用多个Tensor Cores，从而提供一个<strong>16x16x16</strong>的矩阵操作，为warp级别的操作。</p>
<p>In addition to CUDA C++ interfaces to program Tensor Cores directly, CUDA 9 cuBLAS and cuDNN libraries include new library interfaces to make use of Tensor Cores for deep learning applications and frameworks.<br>除了CUDA C++之外，CUDA 9.0中的cuBLAS与cuDNN库也提供了调用Tensor core的接口。</p>
<p>从Volta开始，L1 data cache与shared mem合并为一块内存。共享128 KB/SM的内存空间。<br>The new combined L1 data cache and shared memory subsystem of the Volta SM significantly improves performance while also simplifying programming and reducing the tuning required to attain at or near-peak application performance.<br>这样做在提升了程序性能的同时，简化了编程，也减少了达到峰值性能所需的调优。<br>narrows the gap between applications that are manually tuned to keep data in shared memory and those that access data in device memory directly.<br>由于现在L1与shared mem实际为一块内存，具有相同的特性，因此也缩小了人为使用shared mem优化程序与直接访问device mem的程序性能差距。<br>allow L1 cache operations to attain the benefits of shared memory performance. Shared memory provides high bandwidth and low latency, but the CUDA programmer needs to explicitly manage this memory. Volta narrows the gap between applications that explicitly manage shared memory and those that access data in device memory directly.<br>Shared mem带宽高延迟低，但需要人为控制。因此合并L1与shared mem可以使得L1 cache获得shared mem的特性，从而缩小直接访问device mem与人为使用shared mem的性能差距，简化编程。</p>
<h2 id="Independent-Thread-Scheduling"><a href="#Independent-Thread-Scheduling" class="headerlink" title="Independent Thread Scheduling"></a>Independent Thread Scheduling</h2><p>Volta GV100 is the first GPU to support independent thread scheduling, which enables finer-grain synchronization and cooperation between parallel threads in a program.<br>Volta GV100是首个支持独立线程调度的GPU。</p>
<h4 id="Prior-NVIDIA-GPU-SIMT-Models"><a href="#Prior-NVIDIA-GPU-SIMT-Models" class="headerlink" title="Prior NVIDIA GPU SIMT Models"></a>Prior NVIDIA GPU SIMT Models</h4><p>Pascal and earlier NVIDIA GPUs execute <strong>groups of 32 threads, known as warps</strong>, in SIMT (Single Instruction, Multiple Thread) fashion. The Pascal warp uses <strong>a single program counter shared amongst all 32 threads</strong>, combined with an “<strong>active mask</strong>” that specifies which threads of the warp are active at any given time.  This means that divergent execution paths leave some threads inactive, <strong>serializing execution</strong> for different portions of the warp as shown below. The original mask is stored until the warp reconverges at the end of the divergent section, at which point the mask is restored and the threads run together once again.<br>warp中的32个线程共享同一个程序计数器（也称为指令指针，或指令地址寄存器），同步执行相同的指令，同时使用一个mask标记当前哪些线程是active的。当遇到分支路径时，顺序执行不同的分支并屏蔽掉不在此路径上的线程。在分支部份开始时切换顺序执行，在分支结束时重新同步reconverge所有线程。</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%201.png" width="100%" height="100%"><font color="#708090" size="2">Thread scheduling under the SIMT warp execution model of Pascal and earlier NVIDIA GPUs.</font></center>

<p>如图所示，会先执行其中一个分支直到结束，屏蔽掉不在这条分支上的线程，再开始执行另一条分支，在分支结束时所有线程重新同步。</p>
<p>The Pascal SIMT execution model maximizes efficiency by reducing the quantity of resources required to track thread state and by aggressively reconverging threads to maximize parallelism. Tracking thread state in aggregate for the whole warp, however, means that when the execution pathway diverges, the threads which take different branches lose concurrency until they reconverge. This loss of concurrency means that threads from the same warp in divergent regions or different states of execution cannot signal each other or exchange data. This presents an inconsistency in which threads from different warps continue to run concurrently, but diverged threads from the same warp run sequentially until they reconverge. This means, for example, that algorithms requiring fine-grained sharing of data guarded by locks or mutexes can easily lead to deadlock, depending on which warp the contending threads come from. Therefore, on Pascal and earlier GPUs, programmers have to avoid fine-grained synchronization or rely on lock-free or warp-aware algorithms.<br>The Pascal SIMT execution model通过减少记录线程状态所需资源（同一warp同步执行，只记录一个active mask）、积极同步线程最大化并行度（分支结束即立刻同步warp中的线程）来最大化效率。<br>串行化逐个执行分支，也就意味着同一个warp中执行不同分支的线程之间失去了并行性，彼此不能通信、交换数据。但不同warp中的线程仍然是同步的，分支仅屏蔽掉本warp中的一部分线程而执行另一部分，这就显示出一种矛盾。这也就阻止了更细粒度（warp-level）的编程，同一个warp中不同线程交换数据使用锁或互斥体很容易导致死锁问题（一个线程等待另一个线程，但另一个线程需要等待本线程执行完毕才能执行，互相等待导致死锁）。</p>
<h4 id="Volta-SIMT-Model"><a href="#Volta-SIMT-Model" class="headerlink" title="Volta SIMT Model"></a>Volta SIMT Model</h4><p>enabling equal concurrency between all threads, regardless of warp.<br>maintaining execution state per thread, including the program counter and call stack.<br>Volta通过维护每一个线程的执行状态，包括程序计数器（指令地址寄存器）和调用栈，在所有线程间实现了相同的并发性，即同一个warp中的线程也是并发的。</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%202.png" width="100%" height="100%"><font color="#708090" size="2">Volta (bottom) independent thread scheduling architecture block diagram compared to Pascal and earlier architectures (top).</font></center>

<p>Volta maintains per-thread scheduling resources such as program counter (PC) and call stack (S), while earlier architectures maintained these resources per warp.<br>Volta每个线程维护一个program counter (PC) and call stack (S)，而之前的架构每个warp维护一个。</p>
<p>To maximize parallel efficiency, Volta includes a <strong>schedule optimizer</strong> which determines how to group <strong>active threads from the same warp</strong> together into SIMT units. Threads can now diverge and reconverge at sub-warp granularity, and Volta will still <strong>group together threads which are executing the same code and run them in parallel</strong>.</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%203.png" width="100%" height="100%"><font color="#708090" size="2">Volta independent thread scheduling enables interleaved execution of statements from divergent branches.</font></center>

<p>Statements from the if and else branches in the program can now be interleaved in time.<br>if else分支中的语句现在可以交错执行。<br>Note that execution is still SIMT: at any given clock cycle CUDA cores execute the same instruction for all active threads in a warp just as before, retaining the execution efficiency of previous architectures.<br>仍然是SIMT执行架构，单指令流多线程。每个clock cycle，所有active threads在CUDA cores上执行相同的指令。<br>Importantly, Volta’s ability to independently schedule threads within a warp makes it possible to implement complex, <strong>fine-grained algorithms (sub-warp)</strong> and data structures in a more natural way. While the scheduler supports independent execution of threads, it optimizes non-synchronizing code to maintain <strong>as much convergence as possible</strong> for maximum SIMT efficiency.<br>Independently schedule threads within a warp，使得位于不同分支的语句（由不同active threads执行）可以交错执行，从而warp中的线程具有concurrency，可以互相同步、交换数据，使得sub-warp算法成为可能。Scheduler尽可能地优化非同步代码，以实现最大化的convergence（并行性越好，SIMT efficiency越高）。</p>
<p>It is interesting to note that the Figure does not show execution of statement Z by all threads in the warp at the same time. This is because the scheduler must conservatively assume that Z may produce data required by other divergent branches of execution in which case it would be unsafe to automatically enforce reconvergence. In the common case where A, B, X, and Y do not consist of synchronizing operations, the scheduler can identify that it is safe for the warp to naturally reconverge on Z, as on prior architectures.<br>从图中可以看出，位于if else分支语句之外的Z语句，并没有被所有线程同步执行。也就是说在divergence结束的部份，warp中的线程并不一定会reconverge。<br>这是由于编译器保守地假定本线程中的Z语句会产生其他divergent线程执行所需要的数据，如果同步会导致其他线程一直处于等待这部分数据的情况unsafe。一般情况下，如果if else中的指令未包含同步操作，scheduler能识别出在执行Z之前reconverge是safe的，那么就会reconverge，从而提高并行性提高SIMT efficiency。<br>但divergence之后是否reconverge warp中的线程是编译器擅自做主，没保证，最安全的还是人为使用CUDA 9 新引入的__syncwarp()函数to force reconvergence同步all threads in a warp。</p>
<center><img src="/2022/02/20/cuda-4/independent%20thread%20scheduling%204.png" width="100%" height="100%"><font color="#708090" size="2">Programs can use explicit synchronization __syncwarp() to reconverge threads in a warp.</font></center>

<p>In this case, the divergent portions of the warp might not execute Z together, but all execution pathways from threads within a warp will complete before any thread reaches the statement after the <strong>syncwarp(). Similarly, placing the call to </strong>syncwarp() before the execution of Z would force reconvergence before executing Z, potentially enabling greater SIMT efficiency if the developer knows that this is safe for their application.<br>到达__syncwarp()时，所有warp中的divergent portions执行完毕，在此处reconverge。</p>
<h4 id="Starvation-Free-Algorithms"><a href="#Starvation-Free-Algorithms" class="headerlink" title="Starvation-Free Algorithms"></a>Starvation-Free Algorithms</h4><p>Starvation-free algorithms are a key pattern enabled by independent thread scheduling.</p>
<p>Starvation: 并发进程永久地无法获得执行工作所需的某个资源的情况。饥饿可能会导致程序无效或不正确，因为无法获得资源的线程没有正确地完成工作。<br>Starvation-Free: 无饥饿。<strong>某个线程，总是可以获取到某个资源，获取资源的时间不作限制，可长可短，只要能获取到即可</strong>。取决于线程之间是否有优先级的存在，如果系统允许高优先级的线程插队，这样有可能导致低优先级线程产生饥饿，资源被高优先级线程一直锁住，导致低优先级线程一直无法访问。Starvation-Free的线程可以是阻塞的，可以spinning on a lock等待数据可供访问。</p>
<p>Starvation-Free Algorithms are concurrent computing algorithms that are guaranteed to execute correctly so long as the system ensures that <strong>all threads have adequate access to a contended resource</strong>.<br>并发算法，需要保证所有线程对竞争资源都具有充分的访问权限。</p>
<p>Inserting nodes into a doubly-linked list in a multithreaded application.<br>在一个多线程应用中，向一个双向链表中插入节点。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">insert_after</span><span class="params">(Node *a, Node *b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Node *c;</span><br><span class="line">    <span class="built_in">lock</span>(a); <span class="built_in">lock</span>(a-&gt;next);</span><br><span class="line">    c = a-&gt;next;</span><br><span class="line"></span><br><span class="line">    a-&gt;next = b;</span><br><span class="line">    b-&gt;prev = a;</span><br><span class="line"></span><br><span class="line">    b-&gt;next = c;</span><br><span class="line">    c-&gt;prev = b;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">unlock</span>(c); <span class="built_in">unlock</span>(a);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>双向链表，前后节点都要记录。<br>a -&gt; a_next(c)<br>a -&gt; b -&gt; a_next(c)<br>由于要修改a与a_next中的数据，因此首先锁定这两个节点，修改之后解锁。</p>
<p>each element of a doubly linked list has at least three components: a next pointer, a previous pointer, and a lock providing the owner <strong>exclusive access</strong> to update the node.<br>独占访问权</p>
<center><img src="/2022/02/20/cuda-4/a%20double-linked%20list%20with%20locks.png" width="100%" height="100%"><font color="#708090" size="2">A doubly-linked list with fine-grained locks. Per-node locks are acquired (left) before inserting node B into the list (right).</font></center>

<p>Independent thread scheduling in Volta ensures that even if a thread T0 currently holds the lock for node A, another thread T1 in the same warp can successfully wait for the lock to become available without impeding the progress of thread T0. Note, however, that because active threads in a warp execute together, threads spinning on a lock may degrade the performance of the thread holding the lock.<br>Volta的独立线程调度机制使得，即便一个thread T0正持有node A的lock（正在使用node A的数据，其他线程不可访问，需等待T0使用完毕之后unlock），相同warp中的另一个线程T1也可以在不干扰线程T0的情况下等到lock解锁。<br>解释一下，这是由于独立线程调度允许在divergent branches之间交替执行，但也是屏蔽掉不在这条path上的线程。因此此时T0正在持有lock，而T1正在spinning on the lock，等待使用状态。正是由于这样的原因，当T1 spinning的时候，T0是被屏蔽了的，而T0执行的时候，T1是被屏蔽的。因此这里说spinning on a lock的线程会影响此时holding the lock的线程的性能。<br>若没有独立线程调度机制，这种算法无法实现，原因是同一warp中的线程只能执行相同的指令，要么就是被屏蔽掉了。当两线程同时access node A，一个成功hold the lock，另一个只能spin on the lock。而由于发生divergent时，不同分支是交替执行的，无法确保哪个优先执行，有可能执行到spinning状态，这时另一个线程是处于一直被屏蔽状态的，没有机会unlock，导致程序挂起。</p>
<p>It is also important to note that the use of a per-node lock in the above example is critical for performance on the GPU. Traditional doubly-linked list implementations may use a coarse-grained lock that provides exclusive access to the entire structure, rather than separately protecting individual nodes. This approach typically leads to poor performance in applications with many threads—Volta may have up to 163,840 concurrent threads—caused by extremely high contention for the lock. By using a fine-grained lock on each node, the average per-node contention in large lists will usually be low except under certain pathological node insertion patterns.</p>
<p>使用这种per-node lock对GPU执行性能收益很大（除非是那种病态访问情况：线程间访问的节点非常集中）。传统双向链表只能用更粗粒度的lock，提供对整个structure的独占访问权，而不是单独检测每个node。当有数量非常庞大的线程时，线程间竞争非常激烈，并行就完全退化为了串行。</p>
<p><a target="_blank" rel="noopener" href="https://images.nvidia.cn/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">Tesla V100 architecture white paper</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/16/ComSys-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/16/ComSys-1/" class="post-title-link" itemprop="url">计算机系统学习随记 原子操作是如何实现的</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-02-16 01:46:36 / Modified: 01:53:28" itemprop="dateCreated datePublished" datetime="2022-02-16T01:46:36+08:00">2022-02-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">计算机系统学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="原子操作是如何实现的"><a href="#原子操作是如何实现的" class="headerlink" title="原子操作是如何实现的"></a>原子操作是如何实现的</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33445834">知乎: 原子操作是如何实现的？</a></p>
<blockquote>
<p>X86架构(The X86 architecture)</p>
</blockquote>
<p>X86架构(The X86 architecture)是微处理器执行的计算机语言指令集，对应32位。而平时常说的x64，全称是x86-64，对应64位，是x86指令集的64位扩展超集，具备向下兼容的特点。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/11/cuda-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/11/cuda-3/" class="post-title-link" itemprop="url">CUDA学习随记3 Memory Fence与Synchronization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-11 17:13:37" itemprop="dateCreated datePublished" datetime="2022-02-11T17:13:37+08:00">2022-02-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-28 20:39:41" itemprop="dateModified" datetime="2022-06-28T20:39:41+08:00">2022-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Memory-Fence与Synchronization"><a href="#Memory-Fence与Synchronization" class="headerlink" title="Memory Fence与Synchronization"></a>Memory Fence与Synchronization</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></p>
<h2 id="Synchronization-Functions"><a href="#Synchronization-Functions" class="headerlink" title="Synchronization Functions"></a>Synchronization Functions</h2><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">B.6. Synchronization Functions</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure>
<p>waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to <strong>syncthreads() are visible to all threads in the block.<br>等待block中所有线程到达（<strong>指令同步</strong>），并使得</strong>syncthreads()前所有线程的gmem和smem访存对block内所有线程可见（<strong>内存顺序，访存可见性</strong>）。这个可见包含了另一层含义，就是所有内存写入，保证真实的写到了对应的物理内存上，而不仅仅是驻留在Cache中。</p>
<p>__syncthreads() is used to coordinate communication between the threads of the same block. When some threads within a block access the same addresses in shared or global memory, there are potential read-after-write, write-after-read, or write-after-write hazards for some of these memory accesses. These data hazards can be avoided by synchronizing threads in-between these accesses.</p>
<p><strong>syncthreads() is allowed in conditional code but only if the conditional evaluates identically across the entire thread block, otherwise the code execution is likely to hang or produce unintended side effects.<br>必须block内的所有线程都能同时执行到</strong>syncthreads()否则会造成死锁。</p>
<p>Devices of compute capability 2.x and higher support three variations of <strong>syncthreads() described below.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __syncthreads_count(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure><br>is identical to </strong>syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns the number of threads for which predicate evaluates to non-zero.<br>除了同步之外，还会返回block中predicate为非0（true）的threads的数量。（作为函数的返回值）用于判断block中满足条件线程的个数。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __syncthreads_and(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for all of them.<br>除了同步，当且仅当block中所有threads的predicate都为非0，函数返回一个非0值。用于判断block中所有线程是否都满足条件。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __syncthreads_or(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them.<br>除了同步，block中任一thread的predicate非0，那么函数就返回一个非0值。用于判断block中是否具有满足条件的线程。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=<span class="number">0xffffffff</span>);</span><br></pre></td></tr></table></figure>
<p>will cause the executing thread to wait until all warp lanes named in mask have executed a <strong>syncwarp() (with the same mask) before resuming execution. All non-exited threads named in mask must execute a corresponding </strong>syncwarp() with the same mask, or the result is undefined.<br>用于同步warp内部所有被mask所指定的线程，Pascal及以下架构没用，因为其warp是完全同步执行的。<br>所有mask标记的线程必须执行到一个具有相同mask的__syncwarp() ，否则会产生未定义结果。</p>
<p>Executing <strong>syncwarp() guarantees memory ordering among threads participating in the barrier. Thus, threads within a warp that wish to communicate via memory can store to memory, execute </strong>syncwarp(), and then safely read values stored by other threads in the warp.<br>执行__syncwarp()会保证所有被mask标记的warp内线程的memory order，线程间可安全的互相访存。（<strong>内存顺序，访存可见性</strong>）</p>
<p>Note: For .target sm_6x or below, all threads in mask must execute the same <strong>syncwarp() in convergence, and the union of all values in mask must be equal to the active mask. Otherwise, the behavior is undefined.<br>sm_6x及以下的设备（Pascal架构及以下），mask中指定的所有线程必须同步执行同一个</strong>syncwarp()，且mask必须与__active_mask()函数返回的当前active mask一致。否则会导致undefined behavior。</p>
<h2 id="Memory-Fence-Functions"><a href="#Memory-Fence-Functions" class="headerlink" title="Memory Fence Functions"></a>Memory Fence Functions</h2><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-fence-functions">B.5. Memory Fence Functions</a></p>
<p>The CUDA programming model assumes a device with a weakly-ordered memory model, that is the order in which a CUDA thread writes data to shared memory, global memory, page-locked host memory, or the memory of a peer device is not necessarily the order in which the data is observed being written by another CUDA or host thread. It is undefined behaviour for two threads read from or write to the same memory location without synchronization.<br>CUDA本身具有弱内存顺序模型weakly-ordered memory model，就是说一个CUDA线程的访存顺序，与另一个线程或者主机线程看到的访存顺序未必一致。如果在不进行同步的情况下，使用两个线程对同一个内存地址进行操作，结果将是未定义的。</p>
<p>In the following example, thread 1 executes writeXY(), while thread 2 executes readXY().</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> X = <span class="number">1</span>, Y = <span class="number">2</span>;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">writeXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X = <span class="number">10</span>;</span><br><span class="line">    Y = <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">readXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> B = Y;</span><br><span class="line">    <span class="type">int</span> A = X;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The two threads read and write from the same memory locations X and Y simultaneously. Any data-race is undefined behaviour, and has no defined semantics. The resulting values for A and B can be anything.<br>两个线程一个写一个读，得到的结果可以是任意情况。虽然在writeXY代码中看似是先执行X = 10，再执行Y = 20。</p>
<ol>
<li>经过编译器的优化之后，对本线程来说，这个顺序不被保证。</li>
<li>另一个线程看到的顺序可能与本线程的执行顺序不同。</li>
<li>考虑两个线程执行的先后顺序，时间差异等情况。</li>
</ol>
<p>就造成了多种多样的结果。</p>
<p>Memory fence functions can be used to enforce some ordering on memory accesses. The memory fence functions differ in the scope in which the orderings are enforced but they are independent of the accessed memory space (shared memory, global memory, page-locked host memory, and the memory of a peer device).<br>Memory fence functions此时就是用来强制规定访存顺序的。Memory fence functions根据限制访存顺序的范围不同而有所不同，但与访存的内存类型无关，对所有内存均起到相同的作用。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_block();</span><br></pre></td></tr></table></figure>
<p>ensures that:<br>All writes to all memory made by the calling thread before the call to <strong>threadfence_block() are observed by <strong>all threads in the block of the calling thread</strong> as occurring before all writes to all memory made by the calling thread after the call to </strong>threadfence_block();<br>在所有block中的线程看来，范围是<strong>block</strong><br>调用线程的在__threadfence_block()前后的写入保持原有顺序。<br>仅仅是保证了同block内其他线程观察到的顺序。</p>
<p>All reads from all memory made by the calling thread before the call to <strong>threadfence_block() are ordered before all reads from all memory made by the calling thread after the call to </strong>threadfence_block().<br>保证调用线程__threadfence_block()前后的内存读取顺序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence();</span><br></pre></td></tr></table></figure>
<p>acts as <strong>threadfence_block() for all threads in the block of the calling thread and also ensures that no writes to all memory made by the calling thread after the call to </strong>threadfence() are observed by <strong>any thread in the device</strong> as occurring before any write to all memory made by the calling thread before the call to <strong>threadfence().<br>功能与</strong>threadfence_block()一致，范围是<strong>整个Device</strong>。</p>
<p>Note that for this ordering guarantee to be true, the observing threads must truly observe the memory and not cached versions of it; this is ensured by using the volatile keyword as detailed in Volatile Qualifier.<br>为了保证这个观察到的内存顺序是真实的，观察的线程必须观察到对应的memory而不是这个memory的cache。而实现这个条件的方法是添加volatile关键字。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_system();</span><br></pre></td></tr></table></figure>
<p>acts as <strong>threadfence_block() for all threads in the block of the calling thread and also ensures that all writes to all memory made by the calling thread before the call to </strong>threadfence_system() are observed by <strong>all threads in the device, host threads, and all threads in peer devices</strong> as occurring before all writes to all memory made by the calling thread after the call to __threadfence_system().<br>范围是<strong>整个Device，Host和Peer Device</strong>（多卡协同）。</p>
<p>__threadfence_system() is only supported by devices of compute capability 2.x and higher.</p>
<p>In the previous code sample, we can insert fences in the codes as follows:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> X = <span class="number">1</span>, Y = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">writeXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X = <span class="number">10</span>;</span><br><span class="line">    __threadfence();</span><br><span class="line">    Y = <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">readXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> B = Y;</span><br><span class="line">    __threadfence();</span><br><span class="line">    <span class="type">int</span> A = X;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过插入__threadfence()，使得block内所有线程观察到的XY写入顺序不变（包括自身）。观察线程读取YX的顺序也不便，先读Y再读X。所以可能的情况就只剩下两个线程指令执行的先后顺序不同导致的结果差异。</p>
<p>For this code, the following outcomes can be observed:<br>A equal to 1 and B equal to 2, 先读了，还没写<br>A equal to 10 and B equal to 2, B读的时候Y还没写，A读的时候X写完了<br>A equal to 10 and B equal to 20. 读的时候都写完了</p>
<p>The fourth outcome is not possible, because the frist write must be visible before the second write. If thread 1 and 2 belong to the same block, it is enough to use <strong>threadfence_block(). If thread 1 and 2 do not belong to the same block, </strong>threadfence() must be used if they are CUDA threads from the same device and __threadfence_system() must be used if they are CUDA threads from two different devices.</p>
<p>A common use case is when <strong>threads consume some data produced by other threads</strong> as illustrated by the following code sample of a kernel that computes the sum of an array of N numbers in one call. Each block first sums a subset of the array and stores the result in global memory. When all blocks are done, the last block done reads each of these partial sums from global memory and sums them to obtain the final result. In order to determine which block is finished last, each block atomically increments a counter to signal that it is done with computing and storing its partial sum (see Atomic Functions about atomic functions). The last block is the one that receives the counter value equal to gridDim.x-1. <strong>If no fence is placed between storing the partial sum and incrementing the counter, the counter might increment before the partial sum is stored and therefore, might reach gridDim.x-1 and let the last block start reading partial sums before they have been actually updated in memory.</strong></p>
<p>Memory fence functions only affect the ordering of memory operations by a thread; they do not ensure that these memory operations are visible to other threads (like <strong>syncthreads() does for threads within a block (see Synchronization Functions)). In the code sample below, <strong>the visibility of memory operations on the result variable is ensured by declaring it as volatile</strong> (see Volatile Qualifier).<br><strong>Memory fence仅保证内存顺序，不保证可见性，就是说不保证一定写入到了真实的内存而不是内存的Cache，还不保证在观察时刻已经写入了（没有指令同步）。这个需要将变量声明为volatile来保证。</strong> 这点与</strong>syncthreads()对block的同步不同，后者保证所有对gmem和smem的读写对block内所有线程可见（只要你观察，一定能看到结果）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">__shared__ <span class="type">bool</span> isLastBlockDone;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sum</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* array, <span class="type">unsigned</span> <span class="type">int</span> N,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="keyword">volatile</span> <span class="type">float</span>* result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Each block sums a subset of the input array.</span></span><br><span class="line">    <span class="type">float</span> partialSum = <span class="built_in">calculatePartialSum</span>(array, N);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 of each block stores the partial sum</span></span><br><span class="line">        <span class="comment">// to global memory. The compiler will use </span></span><br><span class="line">        <span class="comment">// a store operation that bypasses the L1 cache</span></span><br><span class="line">        <span class="comment">// since the &quot;result&quot; variable is declared as</span></span><br><span class="line">        <span class="comment">// volatile. This ensures that the threads of</span></span><br><span class="line">        <span class="comment">// the last block will read the correct partial</span></span><br><span class="line">        <span class="comment">// sums computed by all other blocks.</span></span><br><span class="line">        result[blockIdx.x] = partialSum;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 makes sure that the incrementation</span></span><br><span class="line">        <span class="comment">// of the &quot;count&quot; variable is only performed after</span></span><br><span class="line">        <span class="comment">// the partial sum has been written to global memory.</span></span><br><span class="line">        __threadfence();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 signals that it is done.</span></span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> value = <span class="built_in">atomicInc</span>(&amp;count, gridDim.x);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 determines if its block is the last</span></span><br><span class="line">        <span class="comment">// block to be done.</span></span><br><span class="line">        isLastBlockDone = (value == (gridDim.x - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Synchronize to make sure that each thread reads</span></span><br><span class="line">    <span class="comment">// the correct value of isLastBlockDone.</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isLastBlockDone) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The last block sums the partial sums</span></span><br><span class="line">        <span class="comment">// stored in result[0 .. gridDim.x-1]</span></span><br><span class="line">        <span class="type">float</span> totalSum = <span class="built_in">calculateTotalSum</span>(result);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Thread 0 of last block stores the total sum</span></span><br><span class="line">            <span class="comment">// to global memory and resets the count</span></span><br><span class="line">            <span class="comment">// varialble, so that the next kernel call</span></span><br><span class="line">            <span class="comment">// works properly.</span></span><br><span class="line">            result[<span class="number">0</span>] = totalSum;</span><br><span class="line">            count = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/10/cuda-2/" class="post-title-link" itemprop="url">CUDA学习随记2 关于Warp Shuffle Functions *_sync 首个参数mask的作用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 22:22:15" itemprop="dateCreated datePublished" datetime="2022-02-10T22:22:15+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-22 21:52:52" itemprop="dateModified" datetime="2022-02-22T21:52:52+08:00">2022-02-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关于Warp-Shuffle-Functions-sync-首个参数mask的作用"><a href="#关于Warp-Shuffle-Functions-sync-首个参数mask的作用" class="headerlink" title="关于Warp Shuffle Functions *_sync 首个参数mask的作用"></a>关于Warp Shuffle Functions *_sync 首个参数mask的作用</h1><p>首先来看CUDA 9.0之前的Warp Shuffle Functions形式。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl(T var,<span class="type">int</span> srcLane,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_up(T var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_down(T var,<span class="type">unsigned</span> <span class="type">int</span> delta,<span class="type">int</span> width=warpSize)</span><br><span class="line">T __shfl_xor(T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize)</span><br></pre></td></tr></table></figure><br>再来看从CUDA 9.0开始，引入的改进的Warp Shuffle Functions形式。<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a><br>Deprecation Notice: <strong>shfl, </strong>shfl_up, <strong>shfl_down, and </strong>shfl_xor have been deprecated as of CUDA 9.0.<br>C++官方手册中说，从CUDA 9.0开始，原有Warp Shuffle Functions被废弃，全部改用以下新形式。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> srcLane, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_up_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_down_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_xor_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize);</span><br></pre></td></tr></table></figure><br>新形式与原有Warp Shuffle Functions功能完全一致。可以看到，与原有Warp Shuffle Functions最大的区别是新形式引入了一个变量mask。mask为一个32 bit变量，对应一个warp中的32个thread (lane)，指明参与调用的线程，也就是参与到Warp Shuffle Functions的线程集合。除此之外，可以看到每个函数后面都额外增加了一个表明同步的后缀_sync。<strong>在Pascal以及之前的架构中，我们知道一个warp中的32个线程是完全同步执行的。</strong> warp scheduler每次分别发射一条相同的指令给一个warp中的32个线程，除非遇到warp divergence情况，不在当前控制流path上的线程会被屏蔽。<br>然而，<strong>从Pascal的下一代架构——Volta架构开始，warp scheduler支持Independent Thread Scheduling，也就是说，同一个warp中的32个threads不再保证完全同步。</strong> 因此，为了保证Warp Shuffle Functions协同操作的正确性，所有参与的线程必须首先进行同步，从而保证结果的正确性。比如reduction操作的__shfl_down_sync，32个lane中的前16个先读取后16个的寄存器，必须等待操作完成之后，前8个才能再读取lane ID为8-15的lane的寄存器，以此类推。每次读取之前需要一个warp级别的同步，来保证对应的lane中寄存器的数据已经准备好。<strong>因此这些带有_sync后缀的函数首先会隐式同步所有参与到函数调用的线程，而哪些线程将参与到函数调用中，由mask的对应bit位来指定，0代表不参与，1代表参与。</strong><br>The new *_sync shfl intrinsics take in a mask indicating the threads participating in the call. A bit, representing the thread’s lane id, must be set for each participating thread to ensure they are properly converged before the intrinsic is executed by the hardware. All non-exited threads named in mask must execute the same intrinsic with the same mask, or the result is undefined.<br>那么，未被mask指定的线程呢？这里简单做一个测试。写一个kernel，并以&lt;&lt;<1,32>&gt;&gt;启动。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">16</span>);</span><br></pre></td></tr></table></figure><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0x0000000f</span>, val, <span class="number">16</span>);</span><br></pre></td></tr></table></figure><br>观察执行后每个线程中变量shfl_val的值，可以看到传入两个不同的mask，得到的结果相同，如下所示。</1,32></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Thread ID</th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">…</th>
<th style="text-align:center">15</th>
<th style="text-align:center">16</th>
<th style="text-align:center">…</th>
<th style="text-align:center">30</th>
<th style="text-align:center">31</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>shfl_val</strong></td>
<td style="text-align:center">16</td>
<td style="text-align:center">17</td>
<td style="text-align:center">…</td>
<td style="text-align:center">31</td>
<td style="text-align:center">16</td>
<td style="text-align:center">…</td>
<td style="text-align:center">30</td>
<td style="text-align:center">31</td>
</tr>
</tbody>
</table>
</div>
<p>将参数delta由16改为8，即为每个thread读当前lane ID + 8的thread中的寄存器。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> val = threadIdx.x;</span><br><span class="line"><span class="type">int</span> shfl_val = __shfl_down_sync(<span class="number">0xffffffff</span>, val, <span class="number">8</span>);</span><br></pre></td></tr></table></figure></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Thread ID</th>
<th style="text-align:center">0</th>
<th style="text-align:center">1</th>
<th style="text-align:center">…</th>
<th style="text-align:center">7</th>
<th style="text-align:center">8</th>
<th style="text-align:center">…</th>
<th style="text-align:center">14</th>
<th style="text-align:center">15</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>shfl_val</strong></td>
<td style="text-align:center">8</td>
<td style="text-align:center">9</td>
<td style="text-align:center">…</td>
<td style="text-align:center">15</td>
<td style="text-align:center">16</td>
<td style="text-align:center">…</td>
<td style="text-align:center">22</td>
<td style="text-align:center">23</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Thread ID</th>
<th style="text-align:center">16</th>
<th style="text-align:center">17</th>
<th style="text-align:center">…</th>
<th style="text-align:center">23</th>
<th style="text-align:center">24</th>
<th style="text-align:center">…</th>
<th style="text-align:center">30</th>
<th style="text-align:center">31</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>shfl_val</strong></td>
<td style="text-align:center">24</td>
<td style="text-align:center">25</td>
<td style="text-align:center">…</td>
<td style="text-align:center">31</td>
<td style="text-align:center">24</td>
<td style="text-align:center">…</td>
<td style="text-align:center">30</td>
<td style="text-align:center">31</td>
</tr>
</tbody>
</table>
</div>
<p>这也就证明了:</p>
<ol>
<li>__shfl_down_sync()，为lane ID小的thread读取大的thread的寄存器。越界的线程，即lane ID + 8 &gt; 31的线程返回自身寄存器中的值。</li>
<li>mask未指定的thread也会调用warp shuffle functions，而不是不执行，但这部分线程不会被函数隐式同步，所得到的结果是undefined。</li>
</ol>
<p>也就是说，对于Pascal及之前的架构来说，带有_sync后缀的Warp Shuffle Functions中的参数mask没有作用。而对于从Volta开始的架构来说，mask将指定参与Warp Shuffle Functions的线程，从而告知编译器，来对这部分线程在执行时进行必要的同步，来保证结果的正确性。</p>
<p>参考：<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/what-does-mask-mean-in-warp-shuffle-functions-shfl-sync/67697">What does mask mean in warp shuffle functions (__shfl_sync)</a></p>
<p>引用其中一段回答：</p>
<blockquote>
<p>For correctness, you must specify a mask parameter which includes the warp lanes you expect to participate. The behavior of lanes outside the mask is undefined (because, in fact, Volta provides no guarantees of warp convergence, except those the programmer specifically asks for, and therefore a warp lane with a zero bit in the mask implies that the specified lane may or may not participate.)</p>
<p>The mask parameter says the following:</p>
<p>“These are the warp lanes that must participate for correctness.”</p>
<p>The compiler will generate the necessary instructions to reconverge those threads if they are not already converged.</p>
<p>Thereafter the warp shuffle proceeds for the current state of the warp.</p>
<p>There is no other implied behavior. Regardless of the mask, after the reconvergence step, the result of the warp shuffle operation will be the result you would get for whichever threads happen to be participating. A zero bit in the mask argument does not prevent a warp lane from participating, it merely does not guarantee that such a lane will participate if the warp is in a diverged state.</p>
</blockquote>
<p>mask中的0 bit位不会阻止一个warp lane参与到shuffle function的执行中，而仅仅是不保证这样一个lane在warp内具有分歧时会参与到shuffle function的执行中。换句话说，就是不会在执行前同步这个lane，如果它恰好执行到这里，那么就一起执行，如果不在不管它。</p>
<blockquote>
<p>There is an important caveat here. The mask parameter will create a reconvergence of the indicated threads. However it cannot cause reconvergence of threads that your code has made impossible.</p>
<p>For example, this is illegal (will result in undefined behavior for warp 0):</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">if</span> (threadIdx.x &gt; <span class="number">3</span>)</span><br><span class="line">   __shfl_down_sync(<span class="number">0xFFFFFFFF</span>, v, offset, <span class="number">8</span>);</span><br></pre></td></tr></table></figure>
<p>有一个重要的警告。mask参数会在shuffle function中同步指定threads，然而，并不包括代码中规定的并不能同步的线程（条件语句等造成的condition branches，控制流分叉warp divergence）。如上所示代码，对于warp 0来说，mask指定32个warp lane都要执行<strong>shfl_down_sync，因此要对warp 0中的32个线程进行同步。但if语句规定只有后28个线程会执行到这里，而前四个线程不会执行，二者冲突，会导致undefined behavior。<br>正确写法：使用</strong>ballot_sync预先得到所有参与__shfl_down_sync()的线程所对应的mask。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(<span class="number">0xFFFFFFFF</span>, threadIdx.x &gt; <span class="number">3</span>);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &gt; <span class="number">3</span>)</span><br><span class="line">    __shfl_down_sync(mask, v, offset, <span class="number">8</span>);</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>额外这里记录一个我自己看到的好玩儿的问题。<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">developer.nvidia.com/blog/: Using CUDA Warp-Level Primitives</a>中，在Listing 4，作者写了这样一段代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>配文：</p>
<blockquote>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others. Listing 4 shows an example where <strong>all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.</strong><br>On the latest Volta (and future) GPUs, you can run library functions that use warp synchronous primitives without worrying whether the function is called in a thread-divergent branch.</p>
</blockquote>
<p>毫无疑问，这与我上面所说的警告有所冲突，原因在于这里FULL_MASK为0xFFFFFFFF，但对于其中一个if分支，仅有odd/even线程会访问到这里。也就是说会这会导致上文中提到的undefined behavior问题。具体这个undefined behavior是什么，在<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/using-cuda-warp-level-primitives/148673/16">forums.developer.nvidia.com: Using CUDA Warp-Level Primitives</a>中，有人提到：</p>
<blockquote>
<p>In “Update Legacy Warp-Level Programming”, it says “Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.”</p>
<p>In listing 4, regardless the thread id is even or odd, the thread in the warp will always execute one of the two __shfl_sync() statements. Therefore, FULL_MASK should be used.</p>
<p>The following code may cause a stall.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">  val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">  …</span><br><span class="line">&gt;&#125;</span><br><span class="line">&gt;<span class="keyword">else</span> &#123;</span><br><span class="line">  …</span><br><span class="line">&gt;&#125;</span><br></pre></td></tr></table></figure><br>因此我猜这里所说的undefined behavior应该就是死锁，由于FULL_MASK要求同步warp中的32个lane，而控制流导致有的lane永远不会执行到这里，因此造成死锁。这与在if条件语句中使用block内同步函数<strong>syncthreads()所导致的死锁原理相似。<br>Listing 4中的代码合法的原因是，不管是odd还是even线程，都会执行到一条</strong>shfl_sync(FULL_MASK, val, 0)指令，这两条指令的mask都是FULL_MASK，因此对这32个warp lane同步并不会导致死锁。这个道理类似__syncwarp()。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>用于同步warp中的线程，同步哪些由mask指定。</p>
</blockquote>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)** before resuming execution. It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</p>
<p>只要mask指定的线程执行到了具有相同mask的<strong>syncwarp()即可继续执行，并非是一定位于同一个控制流分支的</strong>syncwarp()。但这仅适用于sm_6x（不包含）以上的设备，也就是Pascal架构（sm_60）之后。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a>中有一段警告：</p>
<blockquote>
<p>For .target sm_6x or below, all threads in mask must execute the same __syncwarp() in convergence, and the union of all values in mask must be equal to the active mask. Otherwise, the behavior is undefined.</p>
</blockquote>
<p>sm_6x及以下的设备，mask中指定的所有线程必须同步执行同一个<strong>syncwarp()，且mask必须与</strong>active_mask()函数返回的当前active mask一致。否则会导致undefined behavior。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cuda-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/10/cuda-1/" class="post-title-link" itemprop="url">CUDA学习随记1 Using CUDA Warp-Level Primitives</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-10 13:55:56" itemprop="dateCreated datePublished" datetime="2022-02-10T13:55:56+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-28 15:37:19" itemprop="dateModified" datetime="2022-06-28T15:37:19+08:00">2022-06-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA%E5%AD%A6%E4%B9%A0%E9%9A%8F%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA学习随记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDA学习随记-Using-CUDA-Warp-Level-Primitives"><a href="#CUDA学习随记-Using-CUDA-Warp-Level-Primitives" class="headerlink" title="CUDA学习随记: Using CUDA Warp-Level Primitives"></a>CUDA学习随记: Using CUDA Warp-Level Primitives</h1><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/">1. NVIDIA DEVELOPER BLOG: Using CUDA Warp-Level Primitives</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">2. CUDA Binary Utilities</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">3. CUDA C++ Programming Guide</a></p>
<p>primitives: 原函数、基元功能</p>
<p><strong>SIMD: Single Instruction, Multiple Data</strong><br>In a SIMD architecture, each instruction applies the same operation in parallel across many data elements.<br>每条指令对众多数据单元并行执行相同的操作。<br>SIMD is typically implemented using processors with vector registers and execution units; a scalar thread issues vector instructions that execute in SIMD fashion.<br>通过具有矢量寄存器和执行单元的处理器实现。一个标量线程发射矢量SIMD指令作用于数据矢量。</p>
<p><strong>SIMT: Single Instruction, Multiple Thread</strong><br>In a SIMT architecture, rather than <font color="red">a single thread issuing vector instructions applied to data vectors</font>, <font color="red">multiple threads issue common instructions to arbitrary data</font>.<br>多个线程发射相同的指令作用于任意数据。矢量线程发射标量指令作用于多个标量数据。</p>
<p>NVIDIA GPUs execute warps of 32 parallel threads using SIMT, which enables each thread to access its own registers, to load and store from divergent addresses, and to follow divergent control flow paths. The CUDA compiler and the GPU work together to ensure the threads of a warp execute the same instruction sequences together as frequently as possible to maximize performance.<br>利用SIMT架构执行以32个并行线程为单位的warps，每个线程访问自己的寄存器，从不同地址存取数据，执行分歧控制流路径。</p>
<p>While the high performance obtained by warp execution happens behind the scene, many CUDA programs can achieve even higher performance by using explicit warp-level programming.<br>由warp执行所带来的高性能往往发生在幕后，而CUDA程序还可以通过显式warp级别编程来达到更好的性能。</p>
<center><img src="/2022/02/10/cuda-1/reduce_shfl_down.png" width="100%" height="100%"><font color="#708090" size="2">Part of a warp-level parallel reduction using shfl_down_sync().</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> FULL_MASK 0xffffffff</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">    val += __shfl_down_sync(FULL_MASK, val, offset);</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://xh.5156edu.com/page/z1015m9220j18754.html">颜色表及html代码</a><br>一个使用warp内向下shuffle函数实现reduction操作的示例。</p>
<p>A warp comprises 32 lanes, with each thread occupying one lane. For a thread at lane X in the warp, __shfl_down_sync(FULL_MASK, val, offset) gets the value of the val variable from the thread at lane X+offset of the same warp. The data exchange is performed between registers, and more efficient than going through shared memory, which requires a load, a store and an extra register to hold the address.<br>单个warp中的thread被称为一个lane。warp shuffle函数直接读取同一warp内其他lane所占有的register，相比于使用shared mem速度显而易见的更快。读取shared mem需要一个load操作，一个store操作，还需要一个额外的寄存器装地址（访存需要计算地址偏移）。</p>
<p>CUDA 9 introduced three categories of new or updated warp-level primitives.<br>CUDA 9引入三类新的/升级的warp-level基元函数</p>
<ol>
<li>Synchronized data exchange: exchange data between threads in warp.<br>同步数据交换，原有warp shuffle的升级。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__all_sync, __any_sync, __uni_sync, __ballot_sync</span><br><span class="line">__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</span><br><span class="line">__match_any_sync, __match_all_sync</span><br></pre></td></tr></table></figure></li>
<li>Active mask query: returns a 32-bit mask indicating which threads in a warp are active with the current executing thread.<br>活动掩码查询，返回32 bit的掩码，对应32个lane，表明在当前执行中warp中的哪些线程是active的。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask</span><br></pre></td></tr></table></figure></li>
<li>Thread synchronization: synchronize threads in a warp and provide a memory fence.<br>线程同步指令，同步warp内的所有线程，并提供一个memory fence（不保证线程间的访存结果彼此可见，仅保证了线程内部的访存顺序）。<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp</span><br></pre></td></tr></table></figure>
详细内容参见：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA C++ Programming Guide</a></li>
</ol>
<h2 id="Synchronized-Data-Exchange"><a href="#Synchronized-Data-Exchange" class="headerlink" title="Synchronized Data Exchange"></a>Synchronized Data Exchange</h2><p>Each of the “synchronized data exchange” primitives perform a collective operation among a set of threads in a warp. The set of threads that participates in invoking each primitive is specified using a 32-bit mask, which is the first argument of these primitives. 由基元函数中第一个参数mask，来指定warp中哪些线程参与调用基元函数。<br>All the participating threads must be synchronized for the collective operation to work correctly. Therefore, these primitives first synchronize the threads if they are not already synchronized. <font color="red">为了保证协同操作的正确性，所有参与的线程必须同步。</font>意思就是，在读取其他lane的寄存器之前，必须保证对应得lane已经执行到了这里，准备好了数据，才能保证正确性，否则可能读取到其他线程修改过/未修改的值，造成错误。比如reduction操作的__shfl_down_sync，32个lane中的前16个先读取后16个的寄存器，必须操作完成之后，前8个才能再读取lane ID为8-15的lane的寄存器，以此类推，每次读取之前需要一个warp级别的同步，来保证对应的lane中寄存器的数据已经准备好。因此这些基元函数首先会隐式同步线程。</p>
<blockquote>
<p>what should I use for the mask argument?</p>
</blockquote>
<p>mask指定了哪些线程需要参与到协同操作中，这部份线程通常由程序逻辑决定，在程序中早先的一些条件分支中计算得到。以reduction为例，如果vector的单元数比block中的threads数量少，对应的代码如下。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>ballot_sync()用于得到参与</strong>shfl_down_sync()的线程mask，而__ballot_sync()本身使用FULL_MASK (0xffffffff for 32 threads)，因为我们假定所有线程都要执行这条指令。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __shfl_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> val, <span class="type">int</span> src_line, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __shfl_down_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> var, <span class="type">unsigned</span> detla, <span class="type">int</span> width=warpSize);</span><br><span class="line"><span class="type">int</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p>Each thread that calls __ballot_sync() receives a bit mask representing all the threads in the warp that pass a true value for the predicate argument.</p>
<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others.<br>Volta以及之后的架构支持Independent Thread Scheduling，允许同一warp内的线程执行分歧分支，不再严格保证warp内的同步。</p>
<center><img src="/2022/02/10/cuda-1/NVIDIA%20GPU%20architecture.jpg" width="100%" height="100%"><font color="#708090" size="2">NVIDIA GPU architecture history, Volta之后最新的架构为Turing</font></center>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    val += __shfl_sync(FULL_MASK, val, <span class="number">0</span>);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Listing 4 shows an example where all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.<br><strong>shfl_sync()用于获取指定lane ID（代码中为lane 0）中寄存器中的数据。示例中，warp内所有线程获取lane 0中的val变量值，而奇数线程与偶数线程分别执行一条if语句的不同分支。可以看到mask参数这里给定FULL_MASK，这是由于不管是基数线程还是偶数线程，都会执行到一条</strong>shfl_sync(FULL_MASK, val, 0)指令，因此对这32个warp lane同步并不会导致死锁。</p>
<h2 id="Active-Mask-Query"><a href="#Active-Mask-Query" class="headerlink" title="Active Mask Query"></a>Active Mask Query</h2><p><strong>activemask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same </strong>activemask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.<br><strong>activemask()就是返回当前warp中所有active线程的对应mask。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Incorrect use of __activemask()</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">if</span> (threadIdx.x &lt; NUM_ELEMENTS) &#123; </span><br><span class="line">    <span class="type">unsigned</span> mask = __activemask(); </span><br><span class="line">    val = input[threadIdx.x]; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">16</span>; offset &gt; <span class="number">0</span>; offset /= <span class="number">2</span>)</span><br><span class="line">        val += __shfl_down_sync(mask, val, offset);</span><br><span class="line">    …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如上代码是一种错误用法。相比于使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);</span><br></pre></td></tr></table></figure><br>来计算mask，代码中在if分支中使用<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> mask = __activemask(); </span><br></pre></td></tr></table></figure><br>来计算mask。错误的原因在于，从Volta架构开始，不再保证同一warp内所有线程的完全同步执行，而当某一个线程调用</strong>activemask()，其返回的mask是当前时刻warp中所有active threads所对应的mask，而不是所有将会执行到这里的threads所对应的mask。<br>The CUDA execution model does not guarantee that all threads taking the branch together will execute the __activemask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>
<h2 id="Warp-Synchronization"><a href="#Warp-Synchronization" class="headerlink" title="Warp Synchronization"></a>Warp Synchronization</h2><p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the <strong>syncwarp() primitive to synchronize threads in a warp. It is similar to the </strong>syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncwarp(<span class="type">unsigned</span> mask=FULL_MASK);</span><br></pre></td></tr></table></figure><br>用于同步warp中的线程，同步哪些由mask指定。</p>
<p>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence** to allow threads to communicate via memory before and after calling the primitive.</p>
<p>只要mask指定的线程执行到了具有相同mask的<strong>syncwarp()即可继续执行，并非是一定位于同一个控制流分支的</strong>syncwarp()。还额外提供一个memory fence，保证了当前线程中内存操作的顺序。<br>Memory fence functions only affect the ordering of memory operations by a thread; they do not ensure that these memory operations are visible to other threads (like <strong>syncthreads() does for threads within a block (see Synchronization Functions)).<br>Memory fence仅影响当前线程内内存操作的顺序，并不保证对其他线程的可见性。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure><br>waits until all threads in the thread block have reached this point and all global and shared memory accesses made by these threads prior to </strong>syncthreads() are visible to all threads in the block.<br>等待block内所有线程到达，并保证同步指令之前的所有访存对整个block内所有线程可见。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> val = <span class="built_in">get_value</span>(…);</span><br><span class="line">__shared__ <span class="type">float</span> smem[<span class="number">4</span>][<span class="number">8</span>];</span><br><span class="line"> </span><br><span class="line"><span class="comment">// matrix with row-major indexing行主元矩阵</span></span><br><span class="line"><span class="comment">//   0  1  2  3  4  5  6  7 </span></span><br><span class="line"><span class="comment">//   8  9 10 11 12 13 14 15 </span></span><br><span class="line"><span class="comment">//  16 17 18 19 20 21 22 23</span></span><br><span class="line"><span class="comment">//  24 25 26 27 28 29 30 31</span></span><br><span class="line"><span class="type">int</span> x1 = threadIdx.x % <span class="number">8</span>;</span><br><span class="line"><span class="type">int</span> y1 = threadIdx.x / <span class="number">8</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// matrix with column-major indexing列主元矩阵</span></span><br><span class="line"><span class="comment">//   0  4  8 12 16 20 24 28</span></span><br><span class="line"><span class="comment">//   1  5 10 13 17 21 25 29</span></span><br><span class="line"><span class="comment">//   2  6 11 14 18 22 26 30</span></span><br><span class="line"><span class="comment">//   3  7 12 15 19 23 27 31</span></span><br><span class="line"><span class="type">int</span> x2= threadIdx.x / <span class="number">4</span>;</span><br><span class="line"><span class="type">int</span> y2 = threadIdx.x % <span class="number">4</span>;</span><br><span class="line"> </span><br><span class="line">smem[y1][x1] = val;</span><br><span class="line">__syncwarp();</span><br><span class="line">val = smem[y2][x2];</span><br><span class="line"> </span><br><span class="line"><span class="built_in">use</span>(val);</span><br></pre></td></tr></table></figure>
<p>如上是一个矩阵转置代码，可以看出作者launch block size选取4 * 8 = 32，恰好是一个warp大小。因此在将各个thread寄存器中内容存入shared mem后，使用<strong>syncwarp()同步，确保shared mem中的数据已经完全准备好，再进行读取，从而起到一个转置的作用。<br>Make sure that </strong>syncwarp() separates shared memory reads and writes to avoid race conditions.<br>通过同步分离了smem的读取和写入，避免了内存竞争。</p>
<blockquote>
<p>A tree sum reduction in shared memory</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Incorrect use of __syncwarp()</span></span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>假如想在 warp 内通过 shared memory 做 reduction, 并且假设数据在 shared memory 已经 ready，大家觉得代码这样写有问题吗？</p>
</blockquote>
<p>首先假设block中只有一个warp，size为32，且shared memory开辟了足够大，不存在越界问题，就是说shmem有48个元素。<br>对于Pascal架构及以下来说，我个人认为没有问题，因为warp中所有线程的指令是完全同步的，所有的smem读会发生在所有smem写之前进行。<br>而对于从Volta架构开始，支持warp内线程独立调度，不再保证这种完全的同步。那么由于并没有添加tid &lt; 16的条件判断，因此对于shmem[16-31]这部分内存来说，每个地址会被一个线程读取，被另一个线程写入，二者发生的顺序没有保证（race condition），因此是不正确的。改正的办法很简单，就是在拆分读写，在中间加一个同步指令，最终的结果与Pascal架构的做法一致。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> tid = threadIdx.x;</span><br><span class="line"><span class="type">int</span> v = shmem[tid];</span><br><span class="line"></span><br><span class="line">v += shmem[tid+<span class="number">16</span>]; __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">8</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">4</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">2</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;     __syncwarp();</span><br><span class="line">v += shmem[tid+<span class="number">1</span>];  __syncwarp();</span><br><span class="line">shmem[tid] = v;</span><br></pre></td></tr></table></figure>
<p>The CUDA compiler may elide some of these synchronization instructions in the final generated code depending on the target architecture (e.g. on pre-Volta architectures).<br>编译器会针对目标架构在最终生成的代码中省略这样的一些同步指令，比如在Volta之前的架构上，这些架构warp完全同步执行，没有独立调度功能不需要同步指令。</p>
<p>On the latest Volta (and future) GPUs, you can also use <strong>syncwarp() in thread-divergent branches to synchronize threads from both branches. But once they return from the primitive, the threads will become divergent again. See Listing 13 for such an example.<br>可以在分支中使用</strong>syncwarp()同步warp，一旦函数返回，这些线程重新进入不同分支。</p>
<h2 id="Opportunistic-Warp-level-Programming"><a href="#Opportunistic-Warp-level-Programming" class="headerlink" title="Opportunistic Warp-level Programming"></a>Opportunistic Warp-level Programming</h2><p>在进入分支之前，一般需要先计算mask，然后一直向下传递这个mask。在一些library functions，你不能改变函数的interface，就无法在进行warp-level programming，因为不能向函数中传入所需要的mask。</p>
<p>Some computations can use whatever threads happen to be executing together. We can use a technique called opportunistic warp-level programming, as the following example illustrates. </p>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/">CUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics</a><br>On warp-aggregated atomics for more information on the algorithm<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cooperative-groups/">Cooperative Groups: Flexible CUDA Thread Programming</a><br>For discussion of how Cooperative Groups makes the implementation much simpler.</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__INT.html">CUDA Math API</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   <span class="comment">// increment the value at ptr by 1 and return the old value</span></span><br><span class="line"><span class="number">2</span>   <span class="function">__device__ <span class="type">int</span> <span class="title">atomicAggInc</span><span class="params">(<span class="type">int</span> *ptr)</span> </span>&#123;</span><br><span class="line"><span class="number">3</span>       <span class="type">int</span> mask = __match_any_sync(__activemask(), (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)ptr);</span><br><span class="line"><span class="number">4</span>       <span class="type">int</span> leader = __ffs(mask) – <span class="number">1</span>;    <span class="comment">// select a leader</span></span><br><span class="line"><span class="number">5</span>       <span class="type">int</span> res;</span><br><span class="line"><span class="number">6</span>       <span class="keyword">if</span>(<span class="built_in">lane_id</span>() == leader)                  <span class="comment">// leader does the update</span></span><br><span class="line"><span class="number">7</span>           res = <span class="built_in">atomicAdd</span>(ptr, __popc(mask));</span><br><span class="line"><span class="number">8</span>       res = __shfl_sync(mask, res, leader);    <span class="comment">// get leader’s old value</span></span><br><span class="line"><span class="number">9</span>       <span class="keyword">return</span> res + __popc(mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>)); <span class="comment">//compute old value</span></span><br><span class="line"><span class="number">10</span>  &#125;</span><br></pre></td></tr></table></figure>
<p>atomicAggInc() atomically increments the value pointed to by ptr by 1 and returns the old value. It uses the atomicAdd() function, which may incur contention. To reduce contention, atomicAggInc replaces the per-thread atomicAdd() operation with a per-warp atomicAdd().<br>函数atomicAggInc给输入指针ptr指向的变量+1，然后返回原始值。<br>直接使用atomicAdd()会引发线程间内存竞争导致顺序执行延迟（这里只有一个地址，由ptr指向）。为了减少这种内存竞争，atomicAggInc使用一个warp级别的atomicAdd()替换掉了原有的thread级别的atomicAdd()。</p>
<p>The <strong>activemask() in line 3 finds the set of threads in the warp that are about to perform the atomic operation.<br>返回warp中将要进行原子操作的线程集合。</strong>activemask(): 返回当前warp中active threads的mask。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_any_sync(<span class="type">unsigned</span> mask, T value);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_all_sync(<span class="type">unsigned</span> mask, T value, <span class="type">int</span> *pred);</span><br></pre></td></tr></table></figure>
<p>__match_any_sync()返回输入的掩码mask中指定的threads中，value值相同的线程的线程的掩码。对mask进一步筛选，条件是value值相同。<br>Returns mask of threads that have same value of value in mask.</p>
<p>__match_all_sync()，若输入的掩码mask中指定的所有threads都具有相同的value，则返回掩码、pred指向true，否则返回0、pred指向false。</p>
<p><strong>match_any_sync() returns the bit mask of the threads that have the same value ptr, partitioning the incoming threads into groups whose members have the same ptr value.<br>返回</strong>activemask()返回的mask指定的threads中，ptr值相同的threads所构成的掩码。也就是对此时active的线程根据ptr的值进行进一步分类。<br>Returns mask if all threads in mask have the same value for value; otherwise 0 is returned. Predicate pred is set to true if all threads in mask have the same value of value; otherwise the predicate is set to false.</p>
<p>Each group elects a leader thread (line 4), which performs the atomicAdd() (line 7) for the whole group.<br>分类得到的每组线程选取一个leader（首个mask中bit位为1的线程），并使用这个线程执行atomicAdd()，加的数量为mask中bit为1的数量，正好对应此时经过两次筛选剩下的线程数量。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__​ <span class="type">int</span> __ffs ( <span class="type">int</span>  x )</span><br></pre></td></tr></table></figure><br>Find the position of the least significant bit set to 1 in a 32-bit integer.<br>Bit Operations: 在一个32bit整型中找到置为1的最低有效位。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__​ <span class="type">int</span> __popc ( <span class="type">unsigned</span> <span class="type">int</span>  x )</span><br></pre></td></tr></table></figure><br>Count the number of bits that are set to 1 in a 32-bit integer.<br>Bit Operations: 返回一个32bit整型中为1的bit位数量。</p>
<p>Every thread gets the old value from the leader (line 8) returned by the atomicAdd().<br>warp中其他线程通过__shfl_sync()读取leader线程中的res变量。</p>
<p>Line 9 computes and returns the old value the current thread would get from atomicInc() if it were to call the function instead of atomicAggInc.<br>最终函数返回若线程调用atomicInc()而不是atomicAggInc()而应该得到的返回值。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>())                        <span class="comment">//得到当前线程对应bit置1的mask</span></span><br><span class="line">((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>)                  <span class="comment">//得到当前线程的所有低位置1的mask</span></span><br><span class="line">mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>)           <span class="comment">//保留当前线程所有低位线程所对应的mask</span></span><br><span class="line">__popc(mask &amp; ((<span class="number">1</span> &lt;&lt; <span class="built_in">lane_id</span>()) – <span class="number">1</span>))   <span class="comment">//计数</span></span><br></pre></td></tr></table></figure><br>也就是对当前warp中的线程由低到高做一个单调递增的累加。如果低位一个线程为1，那么高位所有返回值+1。</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/54055195/activemask-vs-ballot-sync"><strong>activemask() vs </strong>ballot_sync()</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__ballot_sync(<span class="type">unsigned</span> mask, predicate):</span><br></pre></td></tr></table></figure>
<p>Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if and only if <strong>predicate evaluates to non-zero for the Nth thread of the warp and the Nth thread is active</strong>.<br>是一个shuffle operation，隐含一个对mask指定的线程的同步作用，没指定的不约束，行为未定义（可能执行了shuffle也可能没执行shuffle）。返回一个mask，如果输入mask中指定的线程对应predicate为true，那么对应bit置1，否则为0。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__activemask():</span><br></pre></td></tr></table></figure>
<p>Returns a 32-bit integer mask of <strong>all currently active threads in the calling warp</strong>. The Nth bit is set if <strong>the Nth lane in the warp is active when __activemask() is called</strong>. Inactive threads are represented by 0 bits in the returned mask. Threads which have exited the program are always marked as inactive. Note that threads that are convergent at an __activemask() call are not guaranteed to be convergent at subsequent instructions unless those instructions are synchronizing warp-builtin functions.<br>仅是一个查询功能，没有任何操作，不对线程进行任何限制，仅返回当前warp中active（convergent）的线程对应的mask。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-shfl-sync">PTX ISA: 9.7.8.6. Data Movement and Conversion Instructions: shfl.sync</a></p>
<p>As an additional clarification on the usage of the mask parameter, note the usage statements in the PTX guide. In particular, the mask parameter is not intended to be an exclusion method. If you wish threads to be excluded from the shuffle operation, you must use conditional code to do that. This is important in light of the following statement from the PTX guide:<br>mask仅指出哪些线程必须要进行同步、参加shuffle operation，并不会将没指定的线程排除在外。如果需要将某些线程排除在外，需要认为使用conditional code，制造divergence。</p>
<blockquote>
<p>The behavior of shfl.sync is undefined if the executing thread is not in the membermask.</p>
</blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> mask = __match_any_sync(__activemask(), (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>)ptr);</span><br></pre></td></tr></table></figure>
<p>回看这条指令，<strong>activemask()指出当前同步的线程有哪些，以这些线程的mask为参数传给</strong>match_any_sync()，此函数进一步指出这些线程中，ptr指向的值相等的有哪些，并以这些线程为mask继续向下执行。</p>
<p>this statement is saying “tell me which threads are converged” (i.e. the <strong>activemask() request), and then “use (at least) those threads to perform the </strong>match_all operation. This is perfectly legal and will use whatever threads happen to be converged at that point. As that listing 9 example continues, the mask computed in the above step is used in the only other warp-cooperative primitive:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = __shfl_sync(mask, res, leader); </span><br></pre></td></tr></table></figure><br><strong>shfl_sync()函数又带了一个对mask中指定线程的同步，就是说mask所指定的线程在</strong>match_any_sync()与__shfl_sync()之间的操作可以基本认为是同步的。</p>
<h2 id="Implicit-Warp-Synchronous-Programming-is-Unsafe"><a href="#Implicit-Warp-Synchronous-Programming-is-Unsafe" class="headerlink" title="Implicit Warp-Synchronous Programming is Unsafe"></a>Implicit Warp-Synchronous Programming is Unsafe</h2><p>CUDA toolkits prior to version 9.0 provided a (now legacy) version of warp-level primitives. Compared with the CUDA 9 primitives, the legacy primitives do not accept a mask argument.<br>早于CUDA 9.0的版本的warp-level primitives版本没有mask参数（不需要warp同步）。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __any(<span class="type">int</span> predicate)</span><br><span class="line"><span class="type">int</span> __any_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate)</span><br></pre></td></tr></table></figure><br>The mask argument, as explained previously, specifies the set of threads in a warp that must participate in the primitives. The new primitives perform intra-warp thread-level synchronization if the threads specified by the mask are not already synchronized during execution.<br>CUDA 9.0引入的新版本提供了warp内级别的线程同步，由mask指定的线程必须同步，其余线程不做要求。</p>
<p>The legacy warp-level primitives do not allow programmers to specify the required threads and do not perform synchronization. Therefore, the threads that must participate in the warp-level operation are not explicitly expressed by the CUDA program. The correctness of such a program depends on implicit warp-synchronous behavior, which may change from one hardware architecture to another, from one CUDA toolkit release to another (due to changes in <strong>compiler optimizations</strong>, for example), or even from one run-time execution to another. Such implicit warp-synchronous programming is unsafe and may not work correctly.<br>原先的warp-level基元函数在使用时不要求指定必须参与的线程，也不进行线程同步。因此，CUDA程序没有显式指定哪些线程必须参与warp-level操作，这种程序的正确性有赖于隐式warp同步。这种程序随着架构、CUDA版本、运行时的不同可能具有不同行为。正确性无法得到保证。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming all 32 threads in a warp execute line 1 together.</span></span><br><span class="line"><span class="built_in">assert</span>(__ballot(<span class="number">1</span>) == FULL_MASK);</span><br><span class="line"><span class="type">int</span> result;</span><br><span class="line"><span class="keyword">if</span> (thread_id % <span class="number">2</span>) &#123;</span><br><span class="line">    result = <span class="built_in">foo</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">    result = <span class="built_in">bar</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">unsigned</span> ballot_result = __ballot(result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">assert</span><span class="params">( <span class="type">int</span> expression )</span></span>;</span><br></pre></td></tr></table></figure>
<p>Evaluates an expression and, when the result is false, prints a diagnostic message and aborts the program.<br>若表达式为false，输出诊断消息并终止程序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> __ballot_sync(<span class="type">int</span> predicate);</span><br></pre></td></tr></table></figure>
<p><strong>ballot()是</strong>ballot_sync()的早期版本。接受一个int predicate参数。<br>return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp.<br>返回一个整形，若第N个线程predicate为真，则整型对应bit置为1，否则为0。<br><strong>ballot(1) == FULL_MASK，毫无作用，输入参数为1，每个线程都为1。<br>foo, bar这里理解为随意两个函数，张三李四之类的<br>假设warp中的32个线程同时执行了第一次</strong>ballot()，而随后的if else导致了线程的divergence，而不同分支中函数执行时间未必一致，因此在执行到第二次<strong>ballot()时，warp的re-convergence不能保证。因此ballot_result中可能不会包含warp中所有线程的结果，而仅包含当前线程调用</strong>ballot()时convergent threads的结果。<br>在第二次<strong>ballot()之前使用</strong>syncwarp()同步整个warp也不保证正确性，这也是隐式同步implicit warp-synchronous programming，其隐含一个条件：一旦同步之后，warp中所有线程保持同步直到遇到下一次thread-divergent branch。这个虽然看似正确，但是并没有得到CUDA的官方认可。最安全的办法就是将<strong>ballot()替换为</strong>ballot_sync()，并把所有线程都标记在mask参数中。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp();</span><br><span class="line"><span class="type">unsigned</span> ballot_result = __ballot(result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> ballot_result = __ballot_sync(FULL_MASK, result);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>   v = <span class="built_in">foo</span>();</span><br><span class="line"><span class="number">2</span>   <span class="keyword">if</span> (threadIdx.x % <span class="number">2</span>) &#123;</span><br><span class="line"><span class="number">3</span>       __syncwarp();</span><br><span class="line"><span class="number">4</span>       v = __shfl(<span class="number">0</span>);       <span class="comment">// L3 will get undefined result because lane 0 </span></span><br><span class="line"><span class="number">5</span>       __syncwarp();        <span class="comment">// is not active when L3 is executed. L3 and L6</span></span><br><span class="line"><span class="number">6</span>   &#125; <span class="keyword">else</span> &#123;                 <span class="comment">// will execute divergently.</span></span><br><span class="line"><span class="number">7</span>       __syncwarp();</span><br><span class="line"><span class="number">8</span>       v = __shfl(<span class="number">0</span>);</span><br><span class="line"><span class="number">9</span>       __syncwarp();</span><br><span class="line"><span class="number">10</span>  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__syncwarp(); </span><br><span class="line">v = __shfl(<span class="number">0</span>); </span><br><span class="line">__syncwarp(); </span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shfl_sync(FULL_MASK, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>二者并不等同。</p>
<p><strong>Reason 1:</strong><br>The <strong>syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed **a </strong>syncwarp() (with the same mask)<strong> before resuming execution. It also provides a </strong>memory fence<strong> to allow threads to communicate via memory before and after calling the primitive.<br>线程首先会同步(convergent)在line 3 &amp; 7。接下来由于if else分支的关系，threads in a warp become divergent，因此line 4执行的时候，lane 0由于不在此分支上，因此有可能是inactive的（被屏蔽了），对lane 0使用<strong>shfl()结果undefined。改用</strong>shfl_sync(FULL_MASK, 0)就会解决这个问题，此函数会强制mask中指定的所有threads converge，然后执行对应shuffle operation。
</strong>Reason 2:<em>*<br>__syncwarp()仅能保证在执行此函数时，warp中所有threads converge，并不能保证在执行完这条代码之后，threads还能保持convergent，虽然大多数情况是convergent的，但没有得到官方认可is not guaranteed。属于implicit warp-synchronous programming。Thread convergence is guaranteed only within explicitly synchronous warp-level primitives like </em>_sync functions. 隐式同步is unsafe program，因此CUDA 9.0之后舍弃了原有的shuffle函数。</p>
<h2 id="Update-Legacy-Warp-Level-Programming"><a href="#Update-Legacy-Warp-Level-Programming" class="headerlink" title="Update Legacy Warp-Level Programming"></a>Update Legacy Warp-Level Programming</h2><p>将原来的程序更新为使用新shuffle函数（the sync version of the primitives）的程序。<br>Any form of implicit warp-synchronous programming, such as communicating between threads of a warp without synchronization.<br>也可以选择use Cooperative Groups restructure your code。其提供了一个更高级别的编组抽象，可以任意组织线程并同步组内线程，such as multi-block synchronization。</p>
<p>使用warp-level primitives最困难的部份就是确定函数所使用的mask（参与shuffle operations的threads）。Here are some suggestions:</p>
<blockquote>
<ol>
<li><p>Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.<br>不要简单的使用FULL_MASK当mask参数传入primitives，如果warp中有threads执行不到会造成死锁hang。</p>
</li>
<li><p>Don’t just use <strong>activemask() as the mask value. </strong>activemask() tells you what threads happen to be convergent when the function is called, which can be different from what you want to be in the collective operation.<br>也不要简单的使用<strong>activemask()的返回值当作mask，</strong>activemask()仅仅返回当前碰巧convergent的threads，与你希望的threads可能会有出入。</p>
</li>
<li><p>Do analyze the program logic and understand the membership requirements. Compute the mask ahead based on your program logic.<br>分析程序逻辑，理解需求，在程序逻辑分支之前先行计算mask。</p>
</li>
<li><p>If your program does opportunistic warp-synchronous programming, use “detective” functions such as <strong>activemask() and </strong>match_any_sync() to find the right mask.<br>如果要在程序中使用opportunistic warp-synchronous programming（不做同步，直接查询当前active threads，再筛选出满足条件的threads，有哪些用哪些），使用一些具有查询检测功能的函数，例如<strong>activemask() and </strong>match_any_sync()，来返回满足要求的mask。</p>
</li>
<li><p>Use <strong>syncwarp() to separate operations with intra-warp dependences. Do not assume lock-step execution.<br>不要假定线程同步implicit synchronous，使用</strong>syncwarp()分离具有intra-warp dependences的操作。warp内前后依赖，指后续操作依赖前面操作的结果。</p>
</li>
</ol>
</blockquote>
<p>If your existing CUDA program gives a different result on Volta architecture GPUs, and you suspect the difference is caused by Volta’s new independent thread scheduling which can change warp synchronous behavior, you may want to recompile your program with <strong>nvcc options -arch=compute_60 -code=sm_70</strong>. Such compiled programs opt-in to Pascal’s thread scheduling.<br>Caused by implicit warp-synchronous programming.</p>
<p>Volta’s new independent thread scheduling:<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/inside-volta/">Inside Volta: The World’s Most Advanced Data Center GPU</a></p>
<p>Volta independent thread scheduling enables interleaved execution of statements from divergent branches. This enables execution of fine-grain parallel algorithms where threads within a warp may synchronize and communicate.</p>
<center><img src="/2022/02/10/cuda-1/Volta_independent_thread_scheduling.png" width="100%" height="100%"><font color="#708090" size="2">Volta_independent_thread_scheduling.</font></center>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/09/NVIDIA-intern-paper-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/09/NVIDIA-intern-paper-md/" class="post-title-link" itemprop="url">NVIDIA Intern Paper 1 Smoke Simulation Method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-09 23:02:28" itemprop="dateCreated datePublished" datetime="2022-02-09T23:02:28+08:00">2022-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-11 15:11:38" itemprop="dateModified" datetime="2022-02-11T15:11:38+08:00">2022-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation/" itemprop="url" rel="index"><span itemprop="name">Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation/1-Smoke-Simulation-Method/" itemprop="url" rel="index"><span itemprop="name">1 Smoke Simulation Method</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation"><a href="#Cuda-Performance-Optimization-of-a-Multigrid-Poisson-Solver-for-Smoke-Simulation" class="headerlink" title="Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation"></a>Cuda Performance Optimization of a Multigrid Poisson Solver for Smoke Simulation</h1><h2 id="1-Smoke-Simulation-Method"><a href="#1-Smoke-Simulation-Method" class="headerlink" title="1. Smoke Simulation Method"></a>1. Smoke Simulation Method</h2><h3 id="1-1-Operator-Splitting-Method"><a href="#1-1-Operator-Splitting-Method" class="headerlink" title="1.1. Operator-Splitting Method"></a>1.1. Operator-Splitting Method</h3><p><strong>Reference:</strong> Harris M J. Fast fluid dynamics simulation on the GPU[J]. SIGGRAPH Courses, 2005, 220(10.1145): 1198555-1198790.<br>&ensp;&ensp;The smoke simulation solves an incompressible NS equation expressed as</p>
<script type="math/tex; mode=display">\frac{\partial \mathbf{u} }{\partial t}=-\left( \mathbf{u}\cdot \nabla  \right)\mathbf{u}-\frac{1}{\rho }\nabla p+\nu { {\nabla }^{2} }\mathbf{u}+\mathbf{F} \tag {1.1.1}</script><p>, where <strong>u</strong> is the velocity field and for an incompressible fluid, it satisfies</p>
<script type="math/tex; mode=display">\nabla \cdot \mathbf{u}=0. \tag {1.1.2}</script><p>With the Operator-Splitting Method, the solution of the NS equation is calculated via composition of transformations on the state. In other words, each component of the NS equation is a step that takes a field as input, and produces a new field as output. $\color{red}{[Fast Fluid Dynamics Simulation on the GPU.]}$ Define an operator <strong>S</strong> that is equivalent to the solution of NS equation over a single time step. Then, the operator S can be decomposed into the operators for advection <strong>A</strong>, diffusion <strong>D</strong>, force application <strong>F</strong>, and projection <strong>P</strong>.</p>
<center><img src="/2022/02/09/NVIDIA-intern-paper-md/1.1.1_1.png" width="50%" height="50%"></center>

<p>&ensp;&ensp;After <strong>F</strong>, <strong>D</strong>, and <strong>A</strong> operations, we can get a divergent velocity field <strong>w</strong>. Then we can get the velocity field <strong>u</strong> with zero divergence by adopting the Helmholtz-Hodge Decomposition Theorem.</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{u}+\nabla p \tag {1.1.3}</script><p>This process is called the projection corresponding to <strong>P</strong>. A projection operation is always needed to guarantee the incompressibility of the fluid.<br>&ensp;&ensp;If we apply the divergence operator to both sides of the above equation, we obtain</p>
<script type="math/tex; mode=display">\nabla \cdot \mathbf{u}+{ {\nabla }^{2} }p={ {\nabla }^{2} }p=\nabla \cdot \mathbf{w}. \tag {1.1.4}</script><p>Note that for an incompressible fluid, we have $\nabla \cdot \mathbf{u}=0$. Then, it becomes a Poisson equation for the pressure of the fluid, sometimes called the Poisson-pressure equation. With this equation, we can get the pressure p, and then use <strong>w</strong> and p to compute the new divergence-free velocity field <strong>u</strong> by</p>
<script type="math/tex; mode=display">\mathbf{w}=\mathbf{u}+\nabla p. \tag {1.1.5}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/1970/01/01/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Jiang Shao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="邵大宝的学习Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/1970/01/01/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 1970-01-01 08:00:00" itemprop="dateCreated datePublished" datetime="1970-01-01T08:00:00+08:00">1970-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-09 19:43:51" itemprop="dateModified" datetime="2022-02-09T19:43:51+08:00">2022-02-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Jiang Shao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiang Shao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
